{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-19T17:09:33.154166Z",
     "start_time": "2021-01-19T17:09:10.181750Z"
    },
    "provenance": [
     {
      "end_time": "2021-01-18T13:36:04.691Z",
      "execution_time": "2.39s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta | awk -F , 'NF == 10' | sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv",
      "start_time": "2021-01-18T13:36:02.305Z"
     },
     {
      "end_time": "2021-01-18T13:42:44.853Z",
      "execution_time": "4.73s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta > ../../B.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' ../../B.tmp\n!awk -F , 'NF == 10' <../../B.tmp > B.csv",
      "start_time": "2021-01-18T13:42:40.125Z"
     },
     {
      "end_time": "2021-01-18T13:43:00.650Z",
      "execution_time": "4.72s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta > ../../B.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' ../../B.tmp\n!awk -F , 'NF == 10' <../../B.tmp > B.csv",
      "start_time": "2021-01-18T13:42:55.929Z"
     },
     {
      "end_time": "2021-01-18T13:44:45.857Z",
      "execution_time": "4.81s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta > ../../B.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' ../../B.tmp\n!awk -F , 'NF == 10' <../../B.tmp > B.csv",
      "start_time": "2021-01-18T13:44:41.044Z"
     },
     {
      "end_time": "2021-01-18T13:45:31.424Z",
      "execution_time": "4.80s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta > ../../B.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' ../../B.tmp\n!awk -F , 'NF == 10' <../../B.tmp > ../../B.csv",
      "start_time": "2021-01-18T13:45:26.624Z"
     },
     {
      "end_time": "2021-01-18T17:25:37.289Z",
      "execution_time": "4.75s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta > ../../B.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' ../../B.tmp\n!awk -F , 'NF == 10' <../../B.tmp > ../../B.csv",
      "start_time": "2021-01-18T17:25:32.543Z"
     },
     {
      "end_time": "2021-01-18T17:28:03.314Z",
      "execution_time": "4.90s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta > ../../B.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' ../../B.tmp\n!awk -F , 'NF == 10' <../../B.tmp > ../../B.csv",
      "start_time": "2021-01-18T17:27:58.414Z"
     },
     {
      "end_time": "2021-01-18T17:49:27.331Z",
      "execution_time": "22.8s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../A.fasta > ../../A.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' ../../A.tmp\n!awk -F , 'NF == 10' <../../A.tmp > ../../A.csv",
      "start_time": "2021-01-18T17:49:04.569Z"
     },
     {
      "end_time": "2021-01-19T09:28:47.152Z",
      "execution_time": "23.0s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../A.fasta > ../../A.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g' ../../A.tmp\n!awk -F , 'NF == 10' <../../A.tmp > ../../A.csv",
      "start_time": "2021-01-19T09:28:24.177Z"
     },
     {
      "end_time": "2021-01-19T13:16:16.639Z",
      "execution_time": "23.4s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../A.fasta > ../../A.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g' ../../A.tmp\n!awk -F , 'NF == 10' <../../A.tmp > ../../A.csv",
      "start_time": "2021-01-19T13:15:53.242Z"
     },
     {
      "end_time": "2021-01-19T15:05:13.031Z",
      "execution_time": "14.6s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta > ../../B.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g' ../../B.tmp\n!awk -F , 'NF == 10' <../../A.tmp > ../../B.csv",
      "start_time": "2021-01-19T15:04:58.408Z"
     },
     {
      "end_time": "2021-01-19T15:14:16.747Z",
      "execution_time": "4.68s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../B.fasta > ../../B.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g' ../../B.tmp\n!awk -F , 'NF == 10' <../../B.tmp > ../../B.csv",
      "start_time": "2021-01-19T15:14:12.063Z"
     },
     {
      "end_time": "2021-01-19T16:29:10.353Z",
      "execution_time": "24.3s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../A.fasta > ../../A.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g' ../../A.tmp\n!awk -F , 'NF == 10' <../../A.tmp > ../../A.csv",
      "start_time": "2021-01-19T16:28:46.053Z"
     },
     {
      "end_time": "2021-01-19T17:09:33.154Z",
      "execution_time": "23.0s",
      "outputs": [],
      "source": "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../A.fasta > ../../A.tmp \n#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g' ../../A.tmp\n!awk -F , 'NF == 10' <../../A.tmp > ../../A.csv",
      "start_time": "2021-01-19T17:09:10.181Z"
     }
    ]
   },
   "outputs": [],
   "source": [
    "!awk '/^[>;]/ { if (seq) { print seq }; seq=\"\"; print } /^[^>;]/ { seq = seq $0 } END { print seq }' ../../A.fasta > ../../A.tmp \n",
    "#!sed 's/ /_/g;N;s/\\n/,/g;s/|/,/g;s/>//g' > ../../B.csv\n",
    "!sed -i 's/ /_/g;N;s/\\n/,/g;s/|/,/g' ../../A.tmp\n",
    "!awk -F , 'NF == 10' <../../A.tmp > ../../A.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-23T09:44:37.759068Z",
     "start_time": "2021-01-23T09:44:35.836130Z"
    },
    "provenance": [
     {
      "end_time": "Unknown",
      "execution_time": "Unknown",
      "outputs": [],
      "source": "import functools as ft\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport difflib\nimport scipy.spatial.distance as ssd",
      "start_time": "Unknown"
     },
     {
      "end_time": "2021-01-15T08:57:18.616Z",
      "execution_time": "1.90s",
      "outputs": [],
      "source": "import functools as ft\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport difflib\nimport scipy.spatial.distance as ssd",
      "start_time": "2021-01-15T08:57:16.717Z"
     },
     {
      "end_time": "2021-01-15T08:58:42.143Z",
      "execution_time": "1.56s",
      "outputs": [],
      "source": "import functools as ft\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport difflib\nimport scipy.spatial.distance as ssd",
      "start_time": "2021-01-15T08:58:40.580Z"
     },
     {
      "end_time": "2021-01-15T09:00:21.985Z",
      "execution_time": "6ms",
      "outputs": [],
      "source": "import functools as ft\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport difflib\nimport scipy.spatial.distance as ssd",
      "start_time": "2021-01-15T09:00:21.979Z"
     },
     {
      "end_time": "2021-01-17T17:13:58.363Z",
      "execution_time": "1.83s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time ",
      "start_time": "2021-01-17T17:13:56.528Z"
     },
     {
      "end_time": "2021-01-17T17:15:09.350Z",
      "execution_time": "1.61s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time ",
      "start_time": "2021-01-17T17:15:07.741Z"
     },
     {
      "end_time": "2021-01-17T17:17:20.813Z",
      "execution_time": "1.53s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time ",
      "start_time": "2021-01-17T17:17:19.284Z"
     },
     {
      "end_time": "2021-01-17T17:24:00.228Z",
      "execution_time": "1.54s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time ",
      "start_time": "2021-01-17T17:23:58.684Z"
     },
     {
      "end_time": "2021-01-17T17:49:45.894Z",
      "execution_time": "6ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd",
      "start_time": "2021-01-17T17:49:45.888Z"
     },
     {
      "end_time": "2021-01-17T18:16:39.749Z",
      "execution_time": "10ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-17T18:16:39.739Z"
     },
     {
      "end_time": "2021-01-17T18:21:52.651Z",
      "execution_time": "5ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-17T18:21:52.646Z"
     },
     {
      "end_time": "2021-01-17T18:36:36.150Z",
      "execution_time": "1.55s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-17T18:36:34.601Z"
     },
     {
      "end_time": "2021-01-18T08:35:24.238Z",
      "execution_time": "1.89s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T08:35:22.348Z"
     },
     {
      "end_time": "2021-01-18T11:19:58.961Z",
      "execution_time": "6ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T11:19:58.955Z"
     },
     {
      "end_time": "2021-01-18T11:54:06.310Z",
      "execution_time": "6ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T11:54:06.304Z"
     },
     {
      "end_time": "2021-01-18T11:54:37.447Z",
      "execution_time": "1.53s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T11:54:35.913Z"
     },
     {
      "end_time": "2021-01-18T13:36:06.899Z",
      "execution_time": "6ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T13:36:06.893Z"
     },
     {
      "end_time": "2021-01-18T13:46:04.796Z",
      "execution_time": "1.62s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T13:46:03.173Z"
     },
     {
      "end_time": "2021-01-18T13:47:30.771Z",
      "execution_time": "1.54s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T13:47:29.227Z"
     },
     {
      "end_time": "2021-01-18T16:54:39.565Z",
      "execution_time": "4ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T16:54:39.561Z"
     },
     {
      "end_time": "2021-01-18T16:55:00.382Z",
      "execution_time": "1.59s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T16:54:58.789Z"
     },
     {
      "end_time": "2021-01-18T17:25:41.393Z",
      "execution_time": "1.55s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T17:25:39.841Z"
     },
     {
      "end_time": "2021-01-18T17:28:05.696Z",
      "execution_time": "1.54s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T17:28:04.151Z"
     },
     {
      "end_time": "2021-01-18T17:49:31.597Z",
      "execution_time": "1.54s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-18T17:49:30.054Z"
     },
     {
      "end_time": "2021-01-19T13:16:21.520Z",
      "execution_time": "2.12s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-19T13:16:19.403Z"
     },
     {
      "end_time": "2021-01-19T15:05:14.618Z",
      "execution_time": "7ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-19T15:05:14.611Z"
     },
     {
      "end_time": "2021-01-19T15:14:20.650Z",
      "execution_time": "1.76s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-19T15:14:18.890Z"
     },
     {
      "end_time": "2021-01-19T16:29:10.359Z",
      "execution_time": "4ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-19T16:29:10.355Z"
     },
     {
      "end_time": "2021-01-22T16:11:20.787Z",
      "execution_time": "1.58s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\n#from memory_profiler import memory_usage",
      "start_time": "2021-01-22T16:11:19.210Z"
     },
     {
      "end_time": "2021-01-23T09:44:37.759Z",
      "execution_time": "1.92s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\n#from memory_profiler import memory_usage",
      "start_time": "2021-01-23T09:44:35.836Z"
     }
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import collections as co\n",
    "import itertools as it\n",
    "import umap\n",
    "import hdbscan\n",
    "import time \n",
    "import scipy.spatial.distance as ssd\n",
    "#from memory_profiler import memory_usage\n",
    "from plotnine.data import *\n",
    "from plotnine import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-22T16:11:20.931079Z",
     "start_time": "2021-01-22T16:11:20.789280Z"
    },
    "provenance": [
     {
      "end_time": "Unknown",
      "execution_time": "Unknown",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.subtype = np.empty((self.row, 1, ),dtype = \"object\")\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        self.subtype = np.empty((self.row, 1, ), dtype = \"object\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.index[line] = info\n            self.matrix[line] = vector\n            self.subtype[line] = subtype\n\n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    def get_index(self):\n        \n        return(self.index)\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_subtype(self):\n        \n        return(self.subtype)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "Unknown"
     },
     {
      "end_time": "2021-01-15T08:57:19.841Z",
      "execution_time": "187ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.subtype = np.empty((self.row, 1, ),dtype = \"object\")\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        self.subtype = np.empty((self.row, 1, ), dtype = \"object\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.index[line] = info\n            self.matrix[line] = vector\n            self.subtype[line] = subtype\n\n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    def get_index(self):\n        \n        return(self.index)\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_subtype(self):\n        \n        return(self.subtype)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-15T08:57:19.654Z"
     },
     {
      "end_time": "2021-01-15T09:00:23.526Z",
      "execution_time": "185ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.subtype = np.empty((self.row, 1, ),dtype = \"object\")\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        self.subtype = np.empty((self.row, 1, ), dtype = \"object\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.index[line] = info\n            self.matrix[line] = vector\n            self.subtype[line] = subtype\n\n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    def get_index(self):\n        \n        return(self.index)\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_subtype(self):\n        \n        return(self.subtype)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-15T09:00:23.341Z"
     },
     {
      "end_time": "2021-01-17T17:13:59.315Z",
      "execution_time": "147ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:13:59.168Z"
     },
     {
      "end_time": "2021-01-17T17:15:10.931Z",
      "execution_time": "149ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:15:10.782Z"
     },
     {
      "end_time": "2021-01-17T17:17:21.455Z",
      "execution_time": "161ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:17:21.294Z"
     },
     {
      "end_time": "2021-01-17T17:19:17.615Z",
      "execution_time": "160ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n         \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            print(read)\n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:19:17.455Z"
     },
     {
      "end_time": "2021-01-17T17:20:46.568Z",
      "execution_time": "160ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n         \n        for line, read in infile.itertuples(index=True, name=None):\n\n            print('hello')\n            \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            print(read)\n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:20:46.408Z"
     },
     {
      "end_time": "2021-01-17T17:22:59.275Z",
      "execution_time": "159ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            print(read)\n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:22:59.116Z"
     },
     {
      "end_time": "2021-01-17T17:24:01.067Z",
      "execution_time": "146ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:24:00.921Z"
     },
     {
      "end_time": "2021-01-17T18:21:53.884Z",
      "execution_time": "148ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T18:21:53.736Z"
     },
     {
      "end_time": "2021-01-17T18:36:37.171Z",
      "execution_time": "148ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T18:36:37.023Z"
     },
     {
      "end_time": "2021-01-18T11:20:00.605Z",
      "execution_time": "142ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T11:20:00.463Z"
     },
     {
      "end_time": "2021-01-18T11:54:07.641Z",
      "execution_time": "157ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T11:54:07.484Z"
     },
     {
      "end_time": "2021-01-18T11:54:38.062Z",
      "execution_time": "138ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T11:54:37.924Z"
     },
     {
      "end_time": "2021-01-18T13:36:11.351Z",
      "execution_time": "153ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T13:36:11.198Z"
     },
     {
      "end_time": "2021-01-18T13:46:04.935Z",
      "execution_time": "138ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T13:46:04.797Z"
     },
     {
      "end_time": "2021-01-18T13:47:31.501Z",
      "execution_time": "143ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T13:47:31.358Z"
     },
     {
      "end_time": "2021-01-18T16:55:01.264Z",
      "execution_time": "140ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T16:55:01.124Z"
     },
     {
      "end_time": "2021-01-18T17:25:43.096Z",
      "execution_time": "145ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T17:25:42.951Z"
     },
     {
      "end_time": "2021-01-18T17:28:05.842Z",
      "execution_time": "145ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T17:28:05.697Z"
     },
     {
      "end_time": "2021-01-18T17:49:34.379Z",
      "execution_time": "142ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-18T17:49:34.237Z"
     },
     {
      "end_time": "2021-01-19T13:16:22.595Z",
      "execution_time": "142ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-19T13:16:22.453Z"
     },
     {
      "end_time": "2021-01-19T15:05:15.831Z",
      "execution_time": "156ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-19T15:05:15.675Z"
     },
     {
      "end_time": "2021-01-19T15:14:20.792Z",
      "execution_time": "140ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-19T15:14:20.652Z"
     },
     {
      "end_time": "2021-01-19T16:29:10.513Z",
      "execution_time": "152ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-19T16:29:10.361Z"
     },
     {
      "end_time": "2021-01-22T16:11:20.931Z",
      "execution_time": "142ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-22T16:11:20.789Z"
     }
    ]
   },
   "outputs": [],
   "source": [
    "class vectorizer(object):\n",
    "    \n",
    "    def __init__(self, k = 7, convert = 0):\n",
    "    \n",
    "        self.k = k\n",
    "        self.convert = convert\n",
    "        self.exist = co.defaultdict(int) \n",
    "        self.keys = list(self.exist.keys())\n",
    "        self.col = len(self.keys)\n",
    "        self.row = 0\n",
    "        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n",
    "        self.amino = co.defaultdict(str, {\n",
    "            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n",
    "            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n",
    "            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n",
    "            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n",
    "            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n",
    "            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n",
    "            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n",
    "            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n",
    "            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n",
    "        })\n",
    "                \n",
    "    def translate(self, read):\n",
    "    \n",
    "        chain = ''\n",
    "\n",
    "        for i in range(len(read) - 2):\n",
    "            trip = read[i:i+3]\n",
    "            chain += self.amino[trip]\n",
    "\n",
    "        return(chain)\n",
    "    \n",
    "    \n",
    "    def adjust_to_data(self, infile):\n",
    "    \n",
    "        self.row = infile.shape[0]\n",
    "            \n",
    "        for line, read in infile.itertuples(index=True, name=None):\n",
    "\n",
    "            if self.convert == 1:\n",
    "                seq = self.translate(read)\n",
    "                del read\n",
    "\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                for i in range(num):\n",
    "                    kmer = seq[i:i+self.k]\n",
    "                    self.exist[kmer] = 0\n",
    "\n",
    "            else:\n",
    "                seq = read\n",
    "                del read\n",
    "\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                if re.match('^[ACGT]*$', seq): \n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        self.exist[kmer] = 0\n",
    "                else:\n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        if re.match('^[ACGT]*$', kmer): \n",
    "                            self.exist[kmer] = 0\n",
    "            \n",
    "        self.keys = list(self.exist.keys())\n",
    "        self.col = len(self.keys)\n",
    "        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n",
    "        \n",
    "        del seq\n",
    "    \n",
    "    def calculate_frequence(self, infile):\n",
    "        \n",
    "        for line, read in infile.itertuples(index=True, name=None): \n",
    "                 \n",
    "            if self.convert == 1:\n",
    "                seq = self.translate(read)\n",
    "                del read\n",
    "\n",
    "                counts = self.exist.copy()\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                for i in range(num):\n",
    "                    kmer = seq[i:i+self.k]\n",
    "                    counts[kmer] += 1\n",
    "\n",
    "            else:\n",
    "                seq = read\n",
    "                del read\n",
    "\n",
    "                counts = self.exist.copy()\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                if re.match('^[ACGT]*$', seq): \n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        counts[kmer] += 1\n",
    "                else:\n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        if re.match('^[ACGT]*$', kmer): \n",
    "                            counts[kmer] += 1\n",
    "\n",
    "            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n",
    "\n",
    "            self.matrix[line] = vector\n",
    "            \n",
    "            counts.clear()\n",
    "            del vector\n",
    "            del seq\n",
    "            del counts\n",
    "    \n",
    "    \n",
    "    def get_keys(self):\n",
    "        \n",
    "        return(self.keys)\n",
    "    \n",
    "    \n",
    "    def get_matrix(self):\n",
    "        \n",
    "        return(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-22T18:38:15.383988Z",
     "start_time": "2021-01-22T18:38:15.131809Z"
    },
    "provenance": [
     {
      "end_time": "Unknown",
      "execution_time": "Unknown",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A.csv'   \n    setfile = 'Input/settings.csv'\n    worldfile = 'Input/cities.csv'\n    outpath = 'Output/'\n    #outfile = 'output.csv'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    print(\"Finished.\")\n\n    start_table = time.perf_counter()\n\n    print(\"Creating SQL tables.\", end = ' ')\n\n    tables = extractor()\n    tables.fill_dicts(worldfile)\n    tables.input_sequences(infile, 8)\n    \n    intab = tables.get_dataframe([0, 5, 13], ['accession', 'subtype', 'genome'], ['accession'])\n    intab.reset_index(level=['accession'], inplace=True)\n    genomes = tables.get_dataframe([0, 13], ['accession', 'genome'], ['accession'])\n    header = tables.get_dataframe([0, 1, 2], ['accession', 'strain', 'segment'], ['accession'])\n    strains = tables.get_dataframe([1, 6, 7, 8, 9, 10, 11], ['strain', 'species', 'city', 'subcountry', 'country', 'year', 'host'], ['strain'])\n    \n    print(\"Finished.\")\n    \n    stop_table = time.perf_counter()\n    \n    print(f\"Table creation done in {stop_table - start_table:0.4f} seconds.\")\n    \n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(intab)\n    freq_nt.calculate_frequence(intab)\n\n    matrix_nt = freq_nt.get_matrix()\n    index_nt = freq_nt.get_index()   \n    subtype_nt = freq_nt.get_subtype()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(intab)\n    freq_aa.calculate_frequence(intab)\n\n    matrix_aa = freq_aa.get_matrix()\n    index_aa = freq_aa.get_index()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix_aa_ind = pd.DataFrame(matrix_aa_red, index = index_aa)\n    matrix_nt_ind = pd.DataFrame(matrix_nt_red, index = index_nt)\n\n    matrix = pd.concat([matrix_nt_ind, matrix_aa_ind], axis=1, copy = False, ignore_index = True) #falsches Ergebnis? checken ob ignore_index = Fehler\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), index = index_nt, columns = ['cluster', 'centroid'])\n    subtype = pd.DataFrame(subtype_nt, index = index_nt, columns = ['subtype'])\n    clusters = pd.concat([blank, subtype], axis=1, copy = False)\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    #clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    informations = pd.concat([header, clusters], axis=1, copy = False)\n    informations.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'informations.csv', index_label='accession', index=True, header=True, sep=',')\n    strains.to_csv(outpath + 'strains.csv', index_label='strain', index=True, header=True, sep=',')\n    genomes.to_csv(outpath + 'genomes.csv', index_label='accession', index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(f\"Overall running time {(stop_clust - start_clust)+(stop_table - start_table):0.4f} seconds.\")",
      "start_time": "Unknown"
     },
     {
      "end_time": "2021-01-15T08:57:20.373Z",
      "execution_time": "218ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    worldfile = 'Input/cities.csv'\n    outpath = 'Output/'\n    #outfile = 'output.csv'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    print(\"Finished.\")\n\n    start_table = time.perf_counter()\n\n    print(\"Creating SQL tables.\", end = ' ')\n\n    tables = extractor()\n    tables.fill_dicts(worldfile)\n    tables.input_sequences(infile, 8)\n    \n    intab = tables.get_dataframe([0, 5, 13], ['accession', 'subtype', 'genome'], ['accession'])\n    intab.reset_index(level=['accession'], inplace=True)\n    genomes = tables.get_dataframe([0, 13], ['accession', 'genome'], ['accession'])\n    header = tables.get_dataframe([0, 1, 2], ['accession', 'strain', 'segment'], ['accession'])\n    strains = tables.get_dataframe([1, 6, 7, 8, 9, 10, 11], ['strain', 'species', 'city', 'subcountry', 'country', 'year', 'host'], ['strain'])\n    \n    print(\"Finished.\")\n    \n    stop_table = time.perf_counter()\n    \n    print(f\"Table creation done in {stop_table - start_table:0.4f} seconds.\")\n    \n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(intab)\n    freq_nt.calculate_frequence(intab)\n\n    matrix_nt = freq_nt.get_matrix()\n    index_nt = freq_nt.get_index()   \n    subtype_nt = freq_nt.get_subtype()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(intab)\n    freq_aa.calculate_frequence(intab)\n\n    matrix_aa = freq_aa.get_matrix()\n    index_aa = freq_aa.get_index()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix_aa_ind = pd.DataFrame(matrix_aa_red, index = index_aa)\n    matrix_nt_ind = pd.DataFrame(matrix_nt_red, index = index_nt)\n\n    matrix = pd.concat([matrix_nt_ind, matrix_aa_ind], axis=1, copy = False, ignore_index = True) #falsches Ergebnis? checken ob ignore_index = Fehler\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), index = index_nt, columns = ['cluster', 'centroid'])\n    subtype = pd.DataFrame(subtype_nt, index = index_nt, columns = ['subtype'])\n    clusters = pd.concat([blank, subtype], axis=1, copy = False)\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    #clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    informations = pd.concat([header, clusters], axis=1, copy = False)\n    informations.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'informations.csv', index_label='accession', index=True, header=True, sep=',')\n    strains.to_csv(outpath + 'strains.csv', index_label='strain', index=True, header=True, sep=',')\n    genomes.to_csv(outpath + 'genomes.csv', index_label='accession', index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(f\"Overall running time {(stop_clust - start_clust)+(stop_table - start_table):0.4f} seconds.\")",
      "start_time": "2021-01-15T08:57:20.155Z"
     },
     {
      "end_time": "2021-01-15T09:00:24.040Z",
      "execution_time": "218ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A_HA.csv'   \n    setfile = 'Input/settings.csv'\n    worldfile = 'Input/cities.csv'\n    outpath = 'Output/'\n    #outfile = 'output.csv'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    print(\"Finished.\")\n\n    start_table = time.perf_counter()\n\n    print(\"Creating SQL tables.\", end = ' ')\n\n    tables = extractor()\n    tables.fill_dicts(worldfile)\n    tables.input_sequences(infile, 8)\n    \n    intab = tables.get_dataframe([0, 5, 13], ['accession', 'subtype', 'genome'], ['accession'])\n    intab.reset_index(level=['accession'], inplace=True)\n    genomes = tables.get_dataframe([0, 13], ['accession', 'genome'], ['accession'])\n    header = tables.get_dataframe([0, 1, 2], ['accession', 'strain', 'segment'], ['accession'])\n    strains = tables.get_dataframe([1, 6, 7, 8, 9, 10, 11], ['strain', 'species', 'city', 'subcountry', 'country', 'year', 'host'], ['strain'])\n    \n    print(\"Finished.\")\n    \n    stop_table = time.perf_counter()\n    \n    print(f\"Table creation done in {stop_table - start_table:0.4f} seconds.\")\n    \n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(intab)\n    freq_nt.calculate_frequence(intab)\n\n    matrix_nt = freq_nt.get_matrix()\n    index_nt = freq_nt.get_index()   \n    subtype_nt = freq_nt.get_subtype()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(intab)\n    freq_aa.calculate_frequence(intab)\n\n    matrix_aa = freq_aa.get_matrix()\n    index_aa = freq_aa.get_index()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix_aa_ind = pd.DataFrame(matrix_aa_red, index = index_aa)\n    matrix_nt_ind = pd.DataFrame(matrix_nt_red, index = index_nt)\n\n    matrix = pd.concat([matrix_nt_ind, matrix_aa_ind], axis=1, copy = False, ignore_index = True) #falsches Ergebnis? checken ob ignore_index = Fehler\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), index = index_nt, columns = ['cluster', 'centroid'])\n    subtype = pd.DataFrame(subtype_nt, index = index_nt, columns = ['subtype'])\n    clusters = pd.concat([blank, subtype], axis=1, copy = False)\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    #clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    informations = pd.concat([header, clusters], axis=1, copy = False)\n    informations.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'informations.csv', index_label='accession', index=True, header=True, sep=',')\n    strains.to_csv(outpath + 'strains.csv', index_label='strain', index=True, header=True, sep=',')\n    genomes.to_csv(outpath + 'genomes.csv', index_label='accession', index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(f\"Overall running time {(stop_clust - start_clust)+(stop_table - start_table):0.4f} seconds.\")",
      "start_time": "2021-01-15T09:00:23.822Z"
     },
     {
      "end_time": "2021-01-17T17:14:00.204Z",
      "execution_time": "164ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv('Input/A.csv', sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:14:00.040Z"
     },
     {
      "end_time": "2021-01-17T17:15:11.679Z",
      "execution_time": "151ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv('Input/A.csv', sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:15:11.528Z"
     },
     {
      "end_time": "2021-01-17T17:17:22.333Z",
      "execution_time": "155ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Finished.\")\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:17:22.178Z"
     },
     {
      "end_time": "2021-01-17T17:19:18.008Z",
      "execution_time": "153ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Finished.\")\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:19:17.855Z"
     },
     {
      "end_time": "2021-01-17T17:20:46.978Z",
      "execution_time": "159ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Finished.\")\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:20:46.819Z"
     },
     {
      "end_time": "2021-01-17T17:21:26.183Z",
      "execution_time": "154ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Finished.\")\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    print(sequence)\n    \n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:21:26.029Z"
     },
     {
      "end_time": "2021-01-17T17:21:48.669Z",
      "execution_time": "295ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. Empty DataFrame\nColumns: [genome]\nIndex: []\n"
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-13-1abdee311c75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-8-4ba4d11f7e41>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = 'Input/B_HA.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nparameter = settings.loc[4].to_list()\n\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsubset = upload.query('segment == 4').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"Finished.\")\n\nstart_clust = time.perf_counter()\n\nprint(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\nprint(sequence)\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\nnum = clusters['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[clusters.cluster == i]\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n\ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\nclusters.update(centroids)\nclusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nprint(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"Mean of inner cluster distance mean {overall_mean/num}\")\nprint(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:21:48.374Z"
     },
     {
      "end_time": "2021-01-17T17:24:02.194Z",
      "execution_time": "163ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 1').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"Finished.\")\n\n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    print(sequence)\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:24:02.031Z"
     },
     {
      "end_time": "2021-01-17T17:31:35.044Z",
      "execution_time": "2m 22s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. "
       },
       {
        "ename": "NameError",
        "evalue": "name 'ssd' is not defined",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-6-9e87fee015d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0minner_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0maccessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mNameError\u001b[0m: name 'ssd' is not defined"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = 'Input/B_HA.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nparameter = settings.loc[4].to_list()\n\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsubset = upload.query('segment == 1').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"Finished.\")\n\nstart_clust = time.perf_counter()\n\nprint(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\nnum = clusters['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[clusters.cluster == i]\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n\ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\nclusters.update(centroids)\nclusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nprint(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"Mean of inner cluster distance mean {overall_mean/num}\")\nprint(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:29:13.330Z"
     },
     {
      "end_time": "2021-01-17T17:51:51.327Z",
      "execution_time": "153ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 1').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"Finished.\")\n\n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:51:51.174Z"
     },
     {
      "end_time": "2021-01-17T18:21:54.850Z",
      "execution_time": "236ms",
      "outputs": [
       {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (<ipython-input-20-40480b8c0d08>, line 17)",
        "output_type": "error",
        "traceback": [
         "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-40480b8c0d08>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    start_clust = time.perf_counter()\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
        ]
       }
      ],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    memory = memory_usage(f)\n    infile = 'Input/A_HA_sample.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    print(f\"Starting calculations for segment {segment}.\"\n    \n    start_clust = time.perf_counter()\n    \n    segment = 4\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    print(\"Maximum memory used: %s\" % max(mem_usage))\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:21:54.614Z"
     },
     {
      "end_time": "2021-01-17T18:22:09.168Z",
      "execution_time": "171ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    memory = memory_usage(f)\n    infile = 'Input/A_HA_sample.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    print(f\"Starting calculations for segment {segment}.\")\n    \n    start_clust = time.perf_counter()\n    \n    segment = 4\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    print(\"Maximum memory used: %s\" % max(mem_usage))\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:22:08.997Z"
     },
     {
      "end_time": "2021-01-17T18:23:59.617Z",
      "execution_time": "166ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A_HA_sample.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    print(f\"Starting calculations for segment {segment}.\")\n    \n    start_clust = time.perf_counter()\n    \n    segment = 4\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:23:59.451Z"
     },
     {
      "end_time": "2021-01-17T18:24:19.343Z",
      "execution_time": "162ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A_HA_sample.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    segment = 4\n    print(f\"Starting calculations for segment {segment}.\")\n    \n    start_clust = time.perf_counter()\n    \n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:24:19.181Z"
     },
     {
      "end_time": "2021-01-17T18:36:38.700Z",
      "execution_time": "158ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    segment = 4\n    print(f\"Starting calculations for segment {segment}:\")\n    \n    start_clust = time.perf_counter()\n    \n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:36:38.542Z"
     },
     {
      "end_time": "2021-01-18T11:20:11.735Z",
      "execution_time": "172ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    \n    print(\"Finished.\")\n    \n    for segment in segments:\n        \n        print(f\" Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n\n        parameter = settings.loc[segment].to_list()\n        upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\" Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\" Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\" Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\" Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].update(centroids)\n        clusters[segment].sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        print(f\" Clustering done in {stop_clust - start_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\" {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\" Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\" {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\" {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-18T11:20:11.563Z"
     },
     {
      "end_time": "2021-01-18T11:22:44.058Z",
      "execution_time": "163ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    \n    print(\"Finished.\")\n    \n    for segment in segments:\n        \n        print(f\" Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n\n        parameter = settings.loc[segment].to_list()\n        upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\" Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\" Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\" Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\" Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].update(centroids)\n        clusters[segment].sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        print(f\" Clustering done in {stop_clust - start_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\" {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\" Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\" {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\" {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-18T11:22:43.895Z"
     },
     {
      "end_time": "2021-01-18T11:52:57.627Z",
      "execution_time": "173ms",
      "outputs": [
       {
        "ename": "SyntaxError",
        "evalue": "EOL while scanning string literal (<ipython-input-29-13dc04f3ad3e>, line 164)",
        "output_type": "error",
        "traceback": [
         "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-13dc04f3ad3e>\"\u001b[0;36m, line \u001b[0;32m164\u001b[0m\n\u001b[0;31m    print(f\"Overall execution time {time:0.2f} seconds.)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\ntime = 0\n\nprint(\"Finished.\")\n\nfor segment in segments:\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\" Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\" Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\" Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\" Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters[segment].update(centroids)\n    clusters[segment].sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    time = time + exec_clust\n\n    print(f\" Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\" {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\" Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\" {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\" {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n\nprint(f\"Overall execution time {time:0.2f} seconds.)",
      "start_time": "2021-01-18T11:52:57.454Z"
     },
     {
      "end_time": "2021-01-18T11:53:06.258Z",
      "execution_time": "908ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n"
       },
       {
        "ename": "AttributeError",
        "evalue": "'int' object has no attribute 'perf_counter'",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-30-8d6bf1788c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting calculations for segment {segment}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mstart_clust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mparameter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'perf_counter'"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\ntime = 0\n\nprint(\"Finished.\")\n\nfor segment in segments:\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\" Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\" Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\" Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\" Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters[segment].update(centroids)\n    clusters[segment].sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    time = time + exec_clust\n\n    print(f\" Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\" {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\" Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\" {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\" {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n\nprint(f\"Overall execution time {time:0.2f} seconds.\")",
      "start_time": "2021-01-18T11:53:05.350Z"
     },
     {
      "end_time": "2021-01-18T11:53:53.051Z",
      "execution_time": "922ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n"
       },
       {
        "ename": "AttributeError",
        "evalue": "'int' object has no attribute 'perf_counter'",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-31-18f6b86a9892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting calculations for segment {segment}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mstart_clust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mparameter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'perf_counter'"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\n\nprint(\"Finished.\")\n\nfor segment in segments:\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\" Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\" Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\" Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\" Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters[segment].update(centroids)\n    clusters[segment].sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    exec_time = exec_time + exec_clust\n\n    print(f\" Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\" {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\" Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\" {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\" {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T11:53:52.129Z"
     },
     {
      "end_time": "2021-01-18T12:13:59.892Z",
      "execution_time": "19m 21s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 160.10 seconds.\n 10382 sequences, 6 unclustered, 239 cluster.\n Mean of inner cluster distance mean 0.0001108158\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nStarting calculations for segment 2:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 153.61 seconds.\n 10406 sequences, 12 unclustered, 263 cluster.\n Mean of inner cluster distance mean 0.0000472150\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nStarting calculations for segment 3:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 136.31 seconds.\n 10397 sequences, 7 unclustered, 235 cluster.\n Mean of inner cluster distance mean 0.0001054200\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nStarting calculations for segment 4:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 147.91 seconds.\n 10512 sequences, 11 unclustered, 244 cluster.\n Mean of inner cluster distance mean 0.0000740418\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nStarting calculations for segment 5:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 147.11 seconds.\n 10390 sequences, 18 unclustered, 251 cluster.\n Mean of inner cluster distance mean 0.0001167966\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nStarting calculations for segment 6:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 145.60 seconds.\n 10496 sequences, 20 unclustered, 256 cluster.\n Mean of inner cluster distance mean 0.0001162096\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nStarting calculations for segment 7:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 134.39 seconds.\n 10402 sequences, 37 unclustered, 245 cluster.\n Mean of inner cluster distance mean 0.0001648957\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nStarting calculations for segment 8:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 135.29 seconds.\n 10405 sequences, 26 unclustered, 242 cluster.\n Mean of inner cluster distance mean 0.0001719148\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nOverall execution time 1160.31 seconds.\n"
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\n\nprint(\"Finished.\")\n\nfor segment in segments:\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\" Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\" Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\" Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\" Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters[segment].update(centroids)\n    clusters[segment].sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    exec_time = exec_time + exec_clust\n\n    print(f\" Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\" {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\" Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\" {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\" {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T11:54:38.646Z"
     },
     {
      "end_time": "2021-01-18T13:36:12.677Z",
      "execution_time": "210ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n\n    print(\"Finished.\")\n\n    for segment in segments:\n\n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\" Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\" Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\" Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\" Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].update(centroids)\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\" Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\" {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\" Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\" {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\" {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T13:36:12.467Z"
     },
     {
      "end_time": "2021-01-18T13:46:06.432Z",
      "execution_time": "187ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n\n    print(\"Finished.\")\n\n    for segment in segments:\n\n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\" Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\" Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\" Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\" Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\" Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].update(centroids)\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\" Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\" {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\" Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\" {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\" {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T13:46:06.245Z"
     },
     {
      "end_time": "2021-01-18T13:47:32.330Z",
      "execution_time": "188ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n\n    print(\"Finished.\")\n\n    for segment in segments:\n\n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].update(centroids)\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T13:47:32.142Z"
     },
     {
      "end_time": "2021-01-18T15:04:38.229Z",
      "execution_time": "2m 37s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. "
       },
       {
        "ename": "AttributeError",
        "evalue": "'list' object has no attribute 'max'",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-98-265fc29e9bae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_clust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mincrement\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'max'"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\nfor segment in segments:\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n    parameter = settings.loc[segment].to_list()\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"- Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"- Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n    new_clust = clusters[segment].query('cluster != -1')['cluster'] + increment\n\n    clusters[segment].update(centroids)\n    clusters[segment].update(new_clust)\n    increment += clusters['cluster'].max()\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    exec_time = exec_time + exec_clust\n\n    print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T15:02:01.481Z"
     },
     {
      "end_time": "2021-01-18T15:35:31.886Z",
      "execution_time": "19m 13s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 153.94 seconds.\n- 10381 sequences, 11 unclustered, 235 cluster.\n- Mean of inner cluster distance mean 0.0001484015\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 153.70 seconds.\n- 10405 sequences, 11 unclustered, 259 cluster.\n- Mean of inner cluster distance mean 0.0000459151\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 3:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 135.62 seconds.\n- 10396 sequences, 16 unclustered, 231 cluster.\n- Mean of inner cluster distance mean 0.0000558589\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 4:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 147.79 seconds.\n- 10511 sequences, 13 unclustered, 245 cluster.\n- Mean of inner cluster distance mean 0.0000552752\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 5:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 145.51 seconds.\n- 10389 sequences, 15 unclustered, 252 cluster.\n- Mean of inner cluster distance mean 0.0000610103\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 146.32 seconds.\n- 10495 sequences, 14 unclustered, 253 cluster.\n- Mean of inner cluster distance mean 0.0000657352\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 7:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 135.91 seconds.\n- 10401 sequences, 29 unclustered, 246 cluster.\n- Mean of inner cluster distance mean 0.0000506483\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 8:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 133.15 seconds.\n- 10403 sequences, 35 unclustered, 244 cluster.\n- Mean of inner cluster distance mean 0.0001635287\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 1151.95 seconds.\n"
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\nfor segment in segments:\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n    parameter = settings.loc[segment].to_list()\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"- Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"- Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n    new_clust = clusters[segment].query('cluster != -1')['cluster'] + increment\n\n    clusters[segment].update(centroids)\n    clusters[segment].update(new_clust)\n    increment += clusters[segment]['cluster'].max()\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    exec_time = exec_time + exec_clust\n\n    print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T15:16:18.712Z"
     },
     {
      "end_time": "2021-01-18T15:49:55.732Z",
      "execution_time": "211ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    for segment in segments:\n\n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n        new_clust = clusters[segment].query('cluster != -1')['cluster'] + increment\n        new_clust['cluster'] = new_clust['cluster'].astype(int) \n\n        clusters[segment].update(centroids)\n        clusters[segment].update(new_clust)\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T15:49:55.521Z"
     },
     {
      "end_time": "2021-01-18T15:59:17.978Z",
      "execution_time": "220ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    for segment in segments:\n\n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n        new_clust = clusters[segment].query('cluster != -1')['cluster'] + int(increment)\n        \n        clusters[segment].update(centroids)\n        clusters[segment].update(new_clust.astype(int))\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T15:59:17.758Z"
     },
     {
      "end_time": "2021-01-18T16:05:55.277Z",
      "execution_time": "2m 45s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 157.49 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. "
       },
       {
        "ename": "KeyboardInterrupt",
        "evalue": "",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-130-37f7e9b03fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmatrix_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-2-b2ad2e68375b>\u001b[0m in \u001b[0;36mcalculate_frequence\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m    110\u001b[0m                             \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkmer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\nfor segment in segments:\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n    parameter = settings.loc[segment].to_list()\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"- Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"- Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n    new_clust = clusters[segment].query('cluster != -1')['cluster'] + increment\n\n    clusters[segment].update(centroids)\n    clusters[segment].update(new_clust.astype(int))\n    increment += num - 1\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    exec_time = exec_time + exec_clust\n\n    print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T16:03:09.932Z"
     },
     {
      "end_time": "2021-01-18T16:17:13.992Z",
      "execution_time": "3m 38s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 151.69 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "ename": "KeyboardInterrupt",
        "evalue": "",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-150-c015a141ea6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     ).fit_transform(matrix_nt)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmatrix_nt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2012\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m         \"\"\"\n\u001b[0;32m-> 2014\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m                 \u001b[0muse_pynndescent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1801\u001b[0;31m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1802\u001b[0m             )\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36mnearest_neighbors\u001b[0;34m(X, n_neighbors, metric, metric_kwds, angular, random_state, low_memory, use_pynndescent, verbose)\u001b[0m\n\u001b[1;32m    395\u001b[0m                     \u001b[0mleaf_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleaf_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                     \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m                 )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\nfor segment in segments:\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n    parameter = settings.loc[segment].to_list()\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"- Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"- Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n    clusters[segment]['cluster'] = clusters[segment]['cluster'].astype(int)\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n    new_clust = clusters[segment].query('cluster != -1')['cluster'] + increment\n\n    clusters[segment].update(centroids)\n    clusters[segment].update(new_clust)\n    increment += num - 1\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    exec_time = exec_time + exec_clust\n\n    print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T16:13:36.489Z"
     },
     {
      "end_time": "2021-01-18T16:18:11.439Z",
      "execution_time": "56ms",
      "outputs": [
       {
        "ename": "IndentationError",
        "evalue": "unexpected indent (<ipython-input-153-e6a068b364f1>, line 22)",
        "output_type": "error",
        "traceback": [
         "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-153-e6a068b364f1>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    print(f\"Starting calculations for segment {segment}:\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\n#for segment in segments:\nsegment = 1\n\n    print(f\"Starting calculations for segment {segment}:\")\n\n    start_clust = time.perf_counter()\n    parameter = settings.loc[segment].to_list()\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"- Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"- Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n    clusters[segment]['cluster'] = clusters[segment]['cluster'].astype(int)\n\n    num = clusters[segment]['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[segment].query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n    new_clust = clusters[segment].query('cluster != -1')['cluster'] + increment\n\n    clusters[segment].update(centroids)\n    clusters[segment].update(new_clust)\n    increment += num - 1\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    exec_clust = stop_clust - start_clust\n    exec_time = exec_time + exec_clust\n\n    print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T16:18:11.383Z"
     },
     {
      "end_time": "2021-01-18T16:21:01.844Z",
      "execution_time": "2m 35s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 153.77 seconds.\n- 10381 sequences, 11 unclustered, 235 cluster.\n- Mean of inner cluster distance mean 0.0001484015\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 153.77 seconds.\n"
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\n#for segment in segments:\nsegment = 1\n\nprint(f\"Starting calculations for segment {segment}:\")\n\nstart_clust = time.perf_counter()\nparameter = settings.loc[segment].to_list()\nsubset = upload.query('segment == @segment').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"- Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"- Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\nclusters[segment]['cluster'] = clusters[segment]['cluster'].astype(int)\n\nnum = clusters[segment]['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[segment].query('cluster == @i')\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n\ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\nnew_clust = clusters[segment].query('cluster != -1')['cluster'] + increment\n\nclusters[segment].update(centroids)\nclusters[segment].update(new_clust)\nincrement += num - 1\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nexec_clust = stop_clust - start_clust\nexec_time = exec_time + exec_clust\n\nprint(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\nprint(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\nprint(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T16:18:27.111Z"
     },
     {
      "end_time": "2021-01-18T16:30:56.813Z",
      "execution_time": "2m 33s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. "
       },
       {
        "ename": "NameError",
        "evalue": "name 'clusers' is not defined",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-175-cc69eb22596b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mblank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusterlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusterlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'centroid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccession\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accession'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mNameError\u001b[0m: name 'clusers' is not defined"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\n#for segment in segments:\nsegment = 1\n\nprint(f\"Starting calculations for segment {segment}:\")\n\nstart_clust = time.perf_counter()\nparameter = settings.loc[segment].to_list()\nsubset = upload.query('segment == @segment').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"- Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"- Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\nprint(clusers[segment])\n\nnum = clusters[segment]['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[segment].query('cluster == @i')\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n\nprint(clusers[segment])\n    \ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\nnew_clust = pd.DataFrame(clusters[segment].query('cluster != -1')['cluster'] + increment)\n\nprint(clusers[segment])\n\nclusters[segment].update(centroids)\nclusters[segment].update(new_clust)\nincrement += num - 1\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nexec_clust = stop_clust - start_clust\nexec_time = exec_time + exec_clust\n\nprint(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\nprint(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\nprint(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T16:28:23.536Z"
     },
     {
      "end_time": "2021-01-18T16:34:45.593Z",
      "execution_time": "2m 37s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction.            cluster centroid subtype\naccession                          \nLC033391       216    false      NA\nKT854634        58    false      NA\nKX615827        65    false      NA\nCY218618       217    false      NA\nMN637988        89    false      NA\n...            ...      ...     ...\nCY018763       214    false      NA\nCY018659       207    false      NA\nCY018843       206    false      NA\nCY018771       206    false      NA\nCY019537       206    false      NA\n\n[10381 rows x 3 columns]\n           cluster centroid subtype\naccession                          \nLC033391       216    false      NA\nKT854634        58    false      NA\nKX615827        65    false      NA\nCY218618       217    false      NA\nMN637988        89    false      NA\n...            ...      ...     ...\nCY018763       214    false      NA\nCY018659       207    false      NA\nCY018843       206    false      NA\nCY018771       206    false      NA\nCY019537       206    false      NA\n\n[10381 rows x 3 columns]\n           cluster centroid subtype\naccession                          \nLC033391       216    false      NA\nKT854634        58    false      NA\nKX615827        65    false      NA\nCY218618       217    false      NA\nMN637988        89    false      NA\n...            ...      ...     ...\nCY018763       214    false      NA\nCY018659       207    false      NA\nCY018843       206    false      NA\nCY018771       206    false      NA\nCY019537       206    false      NA\n\n[10381 rows x 3 columns]\n           cluster centroid subtype\naccession                          \nLC033391     216.0    false      NA\nKT854634      58.0    false      NA\nKX615827      65.0    false      NA\nCY218618     217.0    false      NA\nMN637988      89.0    false      NA\n...            ...      ...     ...\nCY018763     214.0    false      NA\nCY018659     207.0    false      NA\nCY018843     206.0    false      NA\nCY018771     206.0    false      NA\nCY019537     206.0    false      NA\n\n[10381 rows x 3 columns]\nFinished.\n- Clustering done in 156.00 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 156.00 seconds.\n"
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\n#for segment in segments:\nsegment = 1\n\nprint(f\"Starting calculations for segment {segment}:\")\n\nstart_clust = time.perf_counter()\nparameter = settings.loc[segment].to_list()\nsubset = upload.query('segment == @segment').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"- Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"- Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\nprint(clusters[segment])\n\nnum = clusters[segment]['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[segment].query('cluster == @i')\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n\nprint(clusters[segment])\n    \ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\nnew_clust = pd.DataFrame(clusters[segment].query('cluster != -1')['cluster'] + increment).astype(int)\n\nprint(clusters[segment])\n\nclusters[segment].update(centroids)\nclusters[segment].update(new_clust)\nincrement += num - 1\n\nprint(clusters[segment])\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nexec_clust = stop_clust - start_clust\nexec_time = exec_time + exec_clust\n\nprint(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\nprint(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\nprint(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T16:32:08.552Z"
     },
     {
      "end_time": "2021-01-18T16:52:45.940Z",
      "execution_time": "2m 33s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 152.43 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 152.43 seconds.\n"
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 0\n\nprint(\"Finished.\")\n\n#for segment in segments:\nsegment = 1\n\nprint(f\"Starting calculations for segment {segment}:\")\n\nstart_clust = time.perf_counter()\nparameter = settings.loc[segment].to_list()\nsubset = upload.query('segment == @segment').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"- Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"- Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\nnum = clusters[segment]['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[segment].query('cluster == @i')\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n    \ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\nclusters[segment].loc[clusters[segment]['cluster'] == -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] == -1, ['cluster']] + increment\nclusters[segment].update(centroids)\n\nincrement += num - 1\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nexec_clust = stop_clust - start_clust\nexec_time = exec_time + exec_clust\n\nprint(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\nprint(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\nprint(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T16:50:12.530Z"
     },
     {
      "end_time": "2021-01-18T16:55:02.137Z",
      "execution_time": "202ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    for segment in segments:\n\n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] == -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] == -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T16:55:01.935Z"
     },
     {
      "end_time": "2021-01-18T17:20:49.622Z",
      "execution_time": "2m 32s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 151.31 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 151.31 seconds.\n"
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = '../../B.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsegments = settings.index.values.tolist()\nclusters = co.defaultdict(list)\nexec_time = 0\nincrement = 100\n\nprint(\"Finished.\")\n\n#for segment in segments:\nsegment = 1\n\nprint(f\"Starting calculations for segment {segment}:\")\n\nstart_clust = time.perf_counter()\nparameter = settings.loc[segment].to_list()\nsubset = upload.query('segment == @segment').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"- Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"- Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"- Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\nnum = clusters[segment]['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[segment].query('cluster == @i')\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n\ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\nclusters[segment].loc[clusters[segment]['cluster'] == -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] == -1, ['cluster']] + increment\nclusters[segment].update(centroids)\n\nincrement += num - 1\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nexec_clust = stop_clust - start_clust\nexec_time = exec_time + exec_clust\n\nprint(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\nprint(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\nprint(\"Finished.\")\n\nresult = pd.concat(clusters)\nresult.index.set_names([\"segment\", \"accession\"], inplace=True)\nresult.reset_index(level = \"segment\", inplace=True)  \nresult.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\nresult.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\nprint(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T17:18:17.348Z"
     },
     {
      "end_time": "2021-01-18T17:25:44.802Z",
      "execution_time": "204ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 100\n\n    print(\"Finished.\")\n\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T17:25:44.598Z"
     },
     {
      "end_time": "2021-01-18T17:28:06.098Z",
      "execution_time": "204ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T17:28:05.894Z"
     },
     {
      "end_time": "2021-01-18T17:49:35.324Z",
      "execution_time": "203ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-18T17:49:35.121Z"
     },
     {
      "end_time": "2021-01-19T13:16:24.825Z",
      "execution_time": "215ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [1]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['true']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-19T13:16:24.610Z"
     },
     {
      "end_time": "2021-01-19T15:05:19.357Z",
      "execution_time": "213ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    #segments = [1]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-19T15:05:19.144Z"
     },
     {
      "end_time": "2021-01-19T15:14:22.080Z",
      "execution_time": "219ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    #segments = [1]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-19T15:14:21.861Z"
     },
     {
      "end_time": "2021-01-19T16:29:10.726Z",
      "execution_time": "212ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [6]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-19T16:29:10.514Z"
     },
     {
      "end_time": "2021-01-19T16:47:06.229Z",
      "execution_time": "220ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../B.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [6]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-19T16:47:06.009Z"
     },
     {
      "end_time": "2021-01-19T16:50:10.279Z",
      "execution_time": "222ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [6]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = parameter[1].item(),\n            min_dist = parameter[2].item(),\n            n_components = parameter[3].item(),\n            random_state = parameter[4].item(),\n            metric = parameter[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = parameter[7].item(),\n            min_dist = parameter[8].item(),\n            n_components = parameter[9].item(),\n            random_state = parameter[10].item(),\n            metric = parameter[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n            cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n            alpha = parameter[15].item(), #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = parameter[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-19T16:50:10.057Z"
     },
     {
      "end_time": "2021-01-22T16:11:12.753Z",
      "execution_time": "220ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [6]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        setting = [para if type(para) == str else para.item() for para in parameter]\n        \n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = setting[0], convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = setting[1],\n            min_dist = setting[2],\n            n_components = setting[3],\n            random_state = setting[4],\n            metric = setting[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = setting[6], convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = setting[7],\n            min_dist = setting[8],\n            n_components = setting[9],\n            random_state = setting[10],\n            metric = setting[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = setting[12], #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = setting[13], #minimum size that can become a cluster\n            cluster_selection_epsilon = setting[14], #don't seperate clusters with a distance less than value\n            alpha = setting[15], #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = setting[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic))-1)} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster_wo_align.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-22T16:11:12.533Z"
     },
     {
      "end_time": "2021-01-22T16:11:21.160Z",
      "execution_time": "228ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [6]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        setting = [para if type(para) == str else para.item() for para in parameter]\n        \n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n        freq_nt = vectorizer(k = setting[0], convert = 0)\n        freq_nt.adjust_to_data(sequence)\n        freq_nt.calculate_frequence(sequence)\n\n        matrix_nt = freq_nt.get_matrix()\n        keys_nt = freq_nt.get_keys()\n\n        del freq_nt\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_nt_red = umap.UMAP(\n            n_neighbors = setting[1],\n            min_dist = setting[2],\n            n_components = setting[3],\n            random_state = setting[4],\n            metric = setting[5],\n        ).fit_transform(matrix_nt)\n\n        del matrix_nt\n\n        print(\"Finished.\")\n\n        print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n        freq_aa = vectorizer(k = setting[6], convert = 1)\n        freq_aa.adjust_to_data(sequence)\n        freq_aa.calculate_frequence(sequence)\n\n        matrix_aa = freq_aa.get_matrix()\n        keys_aa = freq_aa.get_keys()\n\n        del freq_aa\n\n        print(\"Finished.\")\n\n        print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n        matrix_aa_red = umap.UMAP(\n            n_neighbors = setting[7],\n            min_dist = setting[8],\n            n_components = setting[9],\n            random_state = setting[10],\n            metric = setting[11],\n        ).fit_transform(matrix_aa)\n\n        del matrix_aa\n\n        print(\"Finished.\")\n\n        matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = setting[12], #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = setting[13], #minimum size that can become a cluster\n            cluster_selection_epsilon = setting[14], #don't seperate clusters with a distance less than value\n            alpha = setting[15], #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = setting[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic))-1)} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster_wo_align.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-22T16:11:20.932Z"
     },
     {
      "end_time": "2021-01-22T17:41:49.531Z",
      "execution_time": "159ms",
      "outputs": [
       {
        "ename": "SyntaxError",
        "evalue": "EOL while scanning string literal (<ipython-input-6-efe013816b96>, line 107)",
        "output_type": "error",
        "traceback": [
         "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-efe013816b96>\"\u001b[0;36m, line \u001b[0;32m107\u001b[0m\n\u001b[0;31m    print(\"Exit. Wrong clustering mode.\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
        ]
       }
      ],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    mode = 0 #checkup 0 = nt, 1 = aa, 2 = both\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [6]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        setting = [para if type(para) == str else para.item() for para in parameter]\n        \n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        if mode == 0 or mode == 2:\n        \n            print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n            freq_nt = vectorizer(k = setting[0], convert = 0)\n            freq_nt.adjust_to_data(sequence)\n            freq_nt.calculate_frequence(sequence)\n\n            matrix_nt = freq_nt.get_matrix()\n            keys_nt = freq_nt.get_keys()\n\n            del freq_nt\n\n            print(\"Finished.\")\n\n            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n            matrix_nt_red = umap.UMAP(\n                n_neighbors = setting[1],\n                min_dist = setting[2],\n                n_components = setting[3],\n                random_state = setting[4],\n                metric = setting[5],\n            ).fit_transform(matrix_nt)\n\n            del matrix_nt\n\n            print(\"Finished.\")\n\n        if mode == 1 or mode == 2:\n        \n            print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n            freq_aa = vectorizer(k = setting[6], convert = 1)\n            freq_aa.adjust_to_data(sequence)\n            freq_aa.calculate_frequence(sequence)\n\n            matrix_aa = freq_aa.get_matrix()\n            keys_aa = freq_aa.get_keys()\n\n            del freq_aa\n\n            print(\"Finished.\")\n\n            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n            matrix_aa_red = umap.UMAP(\n                n_neighbors = setting[7],\n                min_dist = setting[8],\n                n_components = setting[9],\n                random_state = setting[10],\n                metric = setting[11],\n            ).fit_transform(matrix_aa)\n\n            del matrix_aa\n\n            print(\"Finished.\")\n\n        if mode == 0:\n        \n            matrix = pd.concat([accession, pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        elif mode == 1:\n            \n            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n            \n        elif mode == 2:\n            \n            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n            \n        else:\n            print(\"Exit. Wrong clustering mode.\n        \n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = setting[12], #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = setting[13], #minimum size that can become a cluster\n            cluster_selection_epsilon = setting[14], #don't seperate clusters with a distance less than value\n            alpha = setting[15], #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = setting[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic))-1)} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster_wo_align.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-22T17:41:49.372Z"
     },
     {
      "end_time": "2021-01-22T17:42:50.780Z",
      "execution_time": "253ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    mode = 0 #checkup 0 = nt, 1 = aa, 2 = both\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [6]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        setting = [para if type(para) == str else para.item() for para in parameter]\n        \n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        if mode == 0 or mode == 2:\n        \n            print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n            freq_nt = vectorizer(k = setting[0], convert = 0)\n            freq_nt.adjust_to_data(sequence)\n            freq_nt.calculate_frequence(sequence)\n\n            matrix_nt = freq_nt.get_matrix()\n            keys_nt = freq_nt.get_keys()\n\n            del freq_nt\n\n            print(\"Finished.\")\n\n            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n            matrix_nt_red = umap.UMAP(\n                n_neighbors = setting[1],\n                min_dist = setting[2],\n                n_components = setting[3],\n                random_state = setting[4],\n                metric = setting[5],\n            ).fit_transform(matrix_nt)\n\n            del matrix_nt\n\n            print(\"Finished.\")\n\n        if mode == 1 or mode == 2:\n        \n            print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n            freq_aa = vectorizer(k = setting[6], convert = 1)\n            freq_aa.adjust_to_data(sequence)\n            freq_aa.calculate_frequence(sequence)\n\n            matrix_aa = freq_aa.get_matrix()\n            keys_aa = freq_aa.get_keys()\n\n            del freq_aa\n\n            print(\"Finished.\")\n\n            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n            matrix_aa_red = umap.UMAP(\n                n_neighbors = setting[7],\n                min_dist = setting[8],\n                n_components = setting[9],\n                random_state = setting[10],\n                metric = setting[11],\n            ).fit_transform(matrix_aa)\n\n            del matrix_aa\n\n            print(\"Finished.\")\n\n        if mode == 0:\n        \n            matrix = pd.concat([accession, pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        elif mode == 1:\n            \n            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n            \n        elif mode == 2:\n            \n            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n            \n        else:\n            exit(\"Wrong clustering mode.\")\n        \n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = setting[12], #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = setting[13], #minimum size that can become a cluster\n            cluster_selection_epsilon = setting[14], #don't seperate clusters with a distance less than value\n            alpha = setting[15], #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = setting[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic))-1)} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster_wo_align.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-22T17:42:50.527Z"
     },
     {
      "end_time": "2021-01-22T18:18:56.532Z",
      "execution_time": "250ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    mode = 0\n    \n    #checkup for mode setting formation etc.\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    segments = [4]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        setting = [para if type(para) == str else para.item() for para in parameter]\n        \n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        if mode == 0 or mode == 2:\n        \n            print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n            freq_nt = vectorizer(k = setting[0], convert = 0)\n            freq_nt.adjust_to_data(sequence)\n            freq_nt.calculate_frequence(sequence)\n\n            matrix_nt = freq_nt.get_matrix()\n            keys_nt = freq_nt.get_keys()\n\n            del freq_nt\n\n            print(\"Finished.\")\n\n            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n            matrix_nt_red = umap.UMAP(\n                n_neighbors = setting[1],\n                min_dist = setting[2],\n                n_components = setting[3],\n                random_state = setting[4],\n                metric = setting[5],\n            ).fit_transform(matrix_nt)\n\n            del matrix_nt\n\n            print(\"Finished.\")\n\n        if mode == 1 or mode == 2:\n        \n            print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n            freq_aa = vectorizer(k = setting[6], convert = 1)\n            freq_aa.adjust_to_data(sequence)\n            freq_aa.calculate_frequence(sequence)\n\n            matrix_aa = freq_aa.get_matrix()\n            keys_aa = freq_aa.get_keys()\n\n            del freq_aa\n\n            print(\"Finished.\")\n\n            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n            matrix_aa_red = umap.UMAP(\n                n_neighbors = setting[7],\n                min_dist = setting[8],\n                n_components = setting[9],\n                random_state = setting[10],\n                metric = setting[11],\n            ).fit_transform(matrix_aa)\n\n            del matrix_aa\n\n            print(\"Finished.\")\n\n        if mode == 0:\n        \n            matrix = pd.concat([accession, pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        elif mode == 1:\n            \n            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n            \n        elif mode == 2:\n            \n            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n            \n        else:\n            exit(\"Wrong clustering mode.\")\n        \n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = setting[12], #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = setting[13], #minimum size that can become a cluster\n            cluster_selection_epsilon = setting[14], #don't seperate clusters with a distance less than value\n            alpha = setting[15], #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = setting[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic))-1)} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster_wo_align.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-22T18:18:56.282Z"
     },
     {
      "end_time": "2021-01-22T18:38:15.383Z",
      "execution_time": "252ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = '../../A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    mode = 0\n    \n    #checkup for mode setting formation etc.\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    upload.query('curation == \"Pass\"', inplace = True)\n    segments = settings.index.values.tolist()\n    clusters = co.defaultdict(list)\n    exec_time = 0\n    increment = 0\n\n    print(\"Finished.\")\n\n    #segments = [4]\n    for segment in segments:\n    \n        print(f\"Starting calculations for segment {segment}:\")\n\n        start_clust = time.perf_counter()\n        parameter = settings.loc[segment].to_list()\n        setting = [para if type(para) == str else para.item() for para in parameter]\n        \n        subset = upload.query('segment == @segment').reset_index()\n\n        sequence = subset[['genome']].copy()\n        accession = subset[['accession']].copy()\n        subtype = subset[['subtype']].copy()\n\n        if mode == 0 or mode == 2:\n        \n            print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n\n            freq_nt = vectorizer(k = setting[0], convert = 0)\n            freq_nt.adjust_to_data(sequence)\n            freq_nt.calculate_frequence(sequence)\n\n            matrix_nt = freq_nt.get_matrix()\n            keys_nt = freq_nt.get_keys()\n\n            del freq_nt\n\n            print(\"Finished.\")\n\n            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n            matrix_nt_red = umap.UMAP(\n                n_neighbors = setting[1],\n                min_dist = setting[2],\n                n_components = setting[3],\n                random_state = setting[4],\n                metric = setting[5],\n            ).fit_transform(matrix_nt)\n\n            del matrix_nt\n\n            print(\"Finished.\")\n\n        if mode == 1 or mode == 2:\n        \n            print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n\n            freq_aa = vectorizer(k = setting[6], convert = 1)\n            freq_aa.adjust_to_data(sequence)\n            freq_aa.calculate_frequence(sequence)\n\n            matrix_aa = freq_aa.get_matrix()\n            keys_aa = freq_aa.get_keys()\n\n            del freq_aa\n\n            print(\"Finished.\")\n\n            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n\n            matrix_aa_red = umap.UMAP(\n                n_neighbors = setting[7],\n                min_dist = setting[8],\n                n_components = setting[9],\n                random_state = setting[10],\n                metric = setting[11],\n            ).fit_transform(matrix_aa)\n\n            del matrix_aa\n\n            print(\"Finished.\")\n\n        if mode == 0:\n        \n            matrix = pd.concat([accession, pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n        elif mode == 1:\n            \n            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n            \n        elif mode == 2:\n            \n            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n            \n        else:\n            exit(\"Wrong clustering mode.\")\n        \n        print(\"- Running HDBscan for clustering.\", end = ' ')\n\n        matrix_clust = hdbscan.HDBSCAN(\n            min_samples = setting[12], #larger the value the more conservative the clustering (more points will be declared as noise)\n            min_cluster_size = setting[13], #minimum size that can become a cluster\n            cluster_selection_epsilon = setting[14], #don't seperate clusters with a distance less than value\n            alpha = setting[15], #don't mess with this\n        ).fit(matrix)\n\n        print(\"Finished.\")\n\n        print(\"- Centroid extraction.\", end = ' ')\n\n        clusterlabel = matrix_clust.labels_\n\n        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n        num = clusters[segment]['cluster'].max()+1\n        values = ['True']*num\n        accessions = []\n        exclude = []\n        include = []\n        overall_mean=0\n        subs = co.defaultdict(list)\n\n        for i in range(num):\n\n            query = clusters[segment].query('cluster == @i')\n            match = query.index.values.tolist()\n            sub = matrix.filter(items = match, axis=0)\n            dist = ssd.cdist(sub, sub, metric = setting[16])\n            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n            accessions.append(inner_mean.idxmin())\n            overall_mean = overall_mean + inner_mean.mean()\n\n            for sub in query['subtype'].tolist():\n                if re.match('^[H][0-9]+N[0-9]+$', sub): \n                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n                else:\n                    subs['X'].append('X0')\n                    subs['X'].append('X0')\n\n            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n                exclude.append(2)\n                if 'X' not in subs.keys():\n                    include.append(2)\n            elif len(set(subs['H'])) == 1:\n                exclude.append(1)\n                if 'X' not in subs.keys():\n                    include.append(1)\n            elif len(set(subs['N'])) == 1:\n                exclude.append(0)\n                if 'X' not in subs.keys():\n                    include.append(0)\n\n            subs.clear()\n\n        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n        clusters[segment].update(centroids)\n\n        increment += num - 1\n\n        print(\"Finished.\")\n\n        stop_clust = time.perf_counter()\n        exec_clust = stop_clust - start_clust\n        exec_time = exec_time + exec_clust\n\n        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n        diagnostic = co.Counter(clusterlabel)\n        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic))-1)} cluster.\")\n        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n        print(\"Finished.\")\n\n    result = pd.concat(clusters)\n    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n    result.reset_index(level = \"segment\", inplace=True)  \n    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n    result.to_csv(outpath + 'cluster_wo_align.csv', index=True, header=True, sep=',', mode='w')\n    print(f\"Overall execution time {exec_time:0.2f} seconds.\")",
      "start_time": "2021-01-22T18:38:15.131Z"
     }
    ]
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    print(\"Read input and settings file.\", end = ' ')\n",
    "\n",
    "    infile = '../../A.csv'   \n",
    "    setfile = 'Input/settings.csv'\n",
    "    outpath = 'Output/'\n",
    "    mode = 0\n",
    "    \n",
    "    #checkup for mode setting formation etc.\n",
    "    \n",
    "    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n",
    "    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n",
    "    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n",
    "    upload.query('curation == \"Pass\"', inplace = True)\n",
    "    segments = settings.index.values.tolist()\n",
    "    clusters = co.defaultdict(list)\n",
    "    exec_time = 0\n",
    "    increment = 0\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    for segment in segments:\n",
    "    \n",
    "        print(f\"Starting calculations for segment {segment}:\")\n",
    "\n",
    "        start_clust = time.perf_counter()\n",
    "        parameter = settings.loc[segment].to_list()\n",
    "        setting = [para if type(para) == str else para.item() for para in parameter]\n",
    "        \n",
    "        subset = upload.query('segment == @segment').reset_index()\n",
    "\n",
    "        sequence = subset[['genome']].copy()\n",
    "        accession = subset[['accession']].copy()\n",
    "        subtype = subset[['subtype']].copy()\n",
    "\n",
    "        if mode == 0 or mode == 2:\n",
    "        \n",
    "            print(\"- Nucleotide k-mer frequency calculation.\", end = ' ')\n",
    "\n",
    "            freq_nt = vectorizer(k = setting[0], convert = 0)\n",
    "            freq_nt.adjust_to_data(sequence)\n",
    "            freq_nt.calculate_frequence(sequence)\n",
    "\n",
    "            matrix_nt = freq_nt.get_matrix()\n",
    "            keys_nt = freq_nt.get_keys()\n",
    "\n",
    "            del freq_nt\n",
    "\n",
    "            print(\"Finished.\")\n",
    "\n",
    "            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n",
    "\n",
    "            matrix_nt_red = umap.UMAP(\n",
    "                n_neighbors = setting[1],\n",
    "                min_dist = setting[2],\n",
    "                n_components = setting[3],\n",
    "                random_state = setting[4],\n",
    "                metric = setting[5],\n",
    "            ).fit_transform(matrix_nt)\n",
    "\n",
    "            del matrix_nt\n",
    "\n",
    "            print(\"Finished.\")\n",
    "\n",
    "        if mode == 1 or mode == 2:\n",
    "        \n",
    "            print(\"- Aminoacid k-mer frequency calculation.\", end = ' ')\n",
    "\n",
    "            freq_aa = vectorizer(k = setting[6], convert = 1)\n",
    "            freq_aa.adjust_to_data(sequence)\n",
    "            freq_aa.calculate_frequence(sequence)\n",
    "\n",
    "            matrix_aa = freq_aa.get_matrix()\n",
    "            keys_aa = freq_aa.get_keys()\n",
    "\n",
    "            del freq_aa\n",
    "\n",
    "            print(\"Finished.\")\n",
    "\n",
    "            print(\"- Running UMAP for dimension reduction.\", end = ' ')\n",
    "\n",
    "            matrix_aa_red = umap.UMAP(\n",
    "                n_neighbors = setting[7],\n",
    "                min_dist = setting[8],\n",
    "                n_components = setting[9],\n",
    "                random_state = setting[10],\n",
    "                metric = setting[11],\n",
    "            ).fit_transform(matrix_aa)\n",
    "\n",
    "            del matrix_aa\n",
    "\n",
    "            print(\"Finished.\")\n",
    "\n",
    "        if mode == 0:\n",
    "        \n",
    "            matrix = pd.concat([accession, pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n",
    "\n",
    "        elif mode == 1:\n",
    "            \n",
    "            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n",
    "            \n",
    "        elif mode == 2:\n",
    "            \n",
    "            matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n",
    "            \n",
    "        else:\n",
    "            exit(\"Wrong clustering mode.\")\n",
    "        \n",
    "        print(\"- Running HDBscan for clustering.\", end = ' ')\n",
    "\n",
    "        matrix_clust = hdbscan.HDBSCAN(\n",
    "            min_samples = setting[12], #larger the value the more conservative the clustering (more points will be declared as noise)\n",
    "            min_cluster_size = setting[13], #minimum size that can become a cluster\n",
    "            cluster_selection_epsilon = setting[14], #don't seperate clusters with a distance less than value\n",
    "            alpha = setting[15], #don't mess with this\n",
    "        ).fit(matrix)\n",
    "\n",
    "        print(\"Finished.\")\n",
    "\n",
    "        print(\"- Centroid extraction.\", end = ' ')\n",
    "\n",
    "        clusterlabel = matrix_clust.labels_\n",
    "\n",
    "        blank = pd.DataFrame(zip(clusterlabel, ['False'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n",
    "        clusters[segment] = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n",
    "\n",
    "        num = clusters[segment]['cluster'].max()+1\n",
    "        values = ['True']*num\n",
    "        accessions = []\n",
    "        exclude = []\n",
    "        include = []\n",
    "        overall_mean=0\n",
    "        subs = co.defaultdict(list)\n",
    "\n",
    "        for i in range(num):\n",
    "\n",
    "            query = clusters[segment].query('cluster == @i')\n",
    "            match = query.index.values.tolist()\n",
    "            sub = matrix.filter(items = match, axis=0)\n",
    "            dist = ssd.cdist(sub, sub, metric = setting[16])\n",
    "            inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n",
    "            accessions.append(inner_mean.idxmin())\n",
    "            overall_mean = overall_mean + inner_mean.mean()\n",
    "\n",
    "            for sub in query['subtype'].tolist():\n",
    "                if re.match('^[H][0-9]+N[0-9]+$', sub): \n",
    "                    subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n",
    "                    subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n",
    "                else:\n",
    "                    subs['X'].append('X0')\n",
    "                    subs['X'].append('X0')\n",
    "\n",
    "            if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n",
    "                exclude.append(2)\n",
    "                if 'X' not in subs.keys():\n",
    "                    include.append(2)\n",
    "            elif len(set(subs['H'])) == 1:\n",
    "                exclude.append(1)\n",
    "                if 'X' not in subs.keys():\n",
    "                    include.append(1)\n",
    "            elif len(set(subs['N'])) == 1:\n",
    "                exclude.append(0)\n",
    "                if 'X' not in subs.keys():\n",
    "                    include.append(0)\n",
    "\n",
    "            subs.clear()\n",
    "\n",
    "        centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n",
    "\n",
    "        clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] = clusters[segment].loc[clusters[segment]['cluster'] != -1, ['cluster']] + increment\n",
    "        clusters[segment].update(centroids)\n",
    "\n",
    "        increment += num - 1\n",
    "\n",
    "        print(\"Finished.\")\n",
    "\n",
    "        stop_clust = time.perf_counter()\n",
    "        exec_clust = stop_clust - start_clust\n",
    "        exec_time = exec_time + exec_clust\n",
    "\n",
    "        print(f\"- Clustering done in {exec_clust:0.2f} seconds.\")\n",
    "        diagnostic = co.Counter(clusterlabel)\n",
    "        print(f\"- {str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic))-1)} cluster.\")\n",
    "        print(f\"- Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n",
    "        print(f\"- {exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n",
    "        print(f\"- {exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n",
    "        print(\"Finished.\")\n",
    "\n",
    "    result = pd.concat(clusters)\n",
    "    result.index.set_names([\"segment\", \"accession\"], inplace=True)\n",
    "    result.reset_index(level = \"segment\", inplace=True)  \n",
    "    result.sort_values(by=['segment', 'cluster', 'subtype'], inplace = True)\n",
    "    result.to_csv(outpath + 'cluster_wo_align.csv', index=True, header=True, sep=',', mode='w')\n",
    "    \n",
    "    density = pd.DataFrame(result.value_counts(subset=['segment', 'cluster']), columns = ['size'])\n",
    "    density.reset_index(level = [\"segment\", \"cluster\"], inplace=True)\n",
    "    \n",
    "    plot = (ggplot(density, aes(x=\"size\", colour = \"factor(segment)\", fill = \"factor(segment)\")) \n",
    "     + labs(\n",
    "        x=\"Cluster Size\",\n",
    "        y=\"Density\",\n",
    "        fill=\"Segment\",\n",
    "        colour=\"Segment\",\n",
    "        title=\"Cluster Size Distribution\",\n",
    "     )\n",
    "     + geom_density(alpha = 0.1)\n",
    "     + scale_x_log10()\n",
    "     + scale_y_continuous()\n",
    "    # + scale_y_log10()\n",
    "    )\n",
    "    plot.save(outpath + 'density.csv')\n",
    "    \n",
    "    print(f\"Overall execution time {exec_time:0.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-22T20:17:27.374305Z",
     "start_time": "2021-01-22T18:38:16.681760Z"
    },
    "provenance": [
     {
      "end_time": "Unknown",
      "execution_time": "Unknown",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nCreating SQL tables. "
       },
       {
        "ename": "IndexError",
        "evalue": "list index out of range",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
         "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-2-6bbd2fe1518b>\", line 72, in process_rows\n    organism = head[4]\nIndexError: list index out of range\n\"\"\"",
         "\nThe above exception was the direct cause of the following exception:\n",
         "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-7-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-6-653664b12606>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworldfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mintab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accession'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'genome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accession'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-2-6bbd2fe1518b>\u001b[0m in \u001b[0;36minput_sequences\u001b[0;34m(self, infile, procs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mIndexError\u001b[0m: list index out of range"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "Unknown"
     },
     {
      "end_time": "2021-01-17T17:17:23.299Z",
      "execution_time": "250ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. "
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-4-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-3-91f2f3134950>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-2-a3ce8d3a10f4>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:17:23.049Z"
     },
     {
      "end_time": "2021-01-17T17:19:18.598Z",
      "execution_time": "144ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. "
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-7-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-6-91f2f3134950>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-5-955e0dcf5285>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:19:18.454Z"
     },
     {
      "end_time": "2021-01-17T17:20:47.706Z",
      "execution_time": "143ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. "
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-10-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-9-91f2f3134950>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-8-4ba4d11f7e41>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:20:47.563Z"
     },
     {
      "end_time": "2021-01-17T17:21:26.655Z",
      "execution_time": "153ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. Empty DataFrame\nColumns: [genome]\nIndex: []\n"
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-12-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-11-57f1a7fa5f04>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-8-4ba4d11f7e41>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:21:26.502Z"
     },
     {
      "end_time": "2021-01-17T17:26:32.796Z",
      "execution_time": "2m 30s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation.                                                   genome\n0      ATGAATATAAATCCTTATTTTCTCTTCATAGATGTGCCCGTACAGG...\n1      ATGAATATAAATCCGTATTTTCTATTCATAGATGTACCTATACAGG...\n2      CTTTAAGATGAATATAAATCCTTATTTTCTCTTCATAGATGTGCCC...\n3      CTTTAAGATGAATATAAATCCGTATTTTCTATTCATAGATGTACCT...\n4      CTTTAAGATGAATATAAATCCTTATTTTCTCTTCATAGATGTGCCC...\n...                                                  ...\n10376  CTTTAAGATGAATATAAATCCTTATTTTCTCTTCATAGATGTGCCC...\n10377  GATGAATATAAATCCTTATTTTCTCTTCATAGATGTACCCATACAG...\n10378  GATGAATATAAATCCTTATTTTCTCTTCATAGATGTACCCATACAG...\n10379  ATGAATATAAATCCTTATTTTCTCTTCATAGATGTACCCATACAGG...\n10380  CTTTAAGATGAATATAAATCCTTATTTTCTCTTCATAGATGTACCC...\n\n[10381 rows x 1 columns]\nFinished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. "
       },
       {
        "ename": "NameError",
        "evalue": "name 'ssd' is not defined",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-4-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-3-a2ac216f60ef>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0minner_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0maccessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mNameError\u001b[0m: name 'ssd' is not defined"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:24:02.779Z"
     },
     {
      "end_time": "2021-01-17T18:11:28.662Z",
      "execution_time": "19m 32s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. Finished.\nClustering done in 1168.1290 seconds.\n55999 sequences, 275 unclustered, 1300 cluster.\nMean of inner cluster distance mean 2.7103107300434113e-05\n848(795) clusters containing matching NA types.\n871(808) clusters containing matching HA types.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:51:56.642Z"
     },
     {
      "end_time": "2021-01-17T18:22:09.675Z",
      "execution_time": "14ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. "
       },
       {
        "ename": "NameError",
        "evalue": "name 'f' is not defined",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-22-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-21-d3e207e7a989>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read input and settings file.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Input/A_HA_sample.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msetfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Input/settings.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T18:22:09.661Z"
     },
     {
      "end_time": "2021-01-17T18:24:00.728Z",
      "execution_time": "171ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\n"
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'segment' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-24-ec55abb246c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maximum memory used: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-23-3b727354fed4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting calculations for segment {segment}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstart_clust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'segment' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    memory = memory_usage(main)\n    #main()\n    print(\"Maximum memory used: %s\" % max(memory))",
      "start_time": "2021-01-17T18:24:00.557Z"
     },
     {
      "end_time": "2021-01-17T18:27:11.585Z",
      "execution_time": "2m 52s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 4.\nNucleotide k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. Finished.\nClustering done in 171.7212 seconds.\n10000 sequences, 0 unclustered, 174 cluster.\nMean of inner cluster distance mean 8.321637921018586e-05\n118(104) clusters containing matching NA types.\n171(121) clusters containing matching HA types.\nMaximum memory used: 3453.1796875\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(\"Maximum memory used: %s\" % max(memory))",
      "start_time": "2021-01-17T18:24:19.692Z"
     },
     {
      "end_time": "2021-01-17T18:56:42.288Z",
      "execution_time": "20m 3s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 4:\nNucleotide k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. Finished.\nClustering done in 1198.7109 seconds.\n56617 sequences, 36 unclustered, 1052 cluster.\nMean of inner cluster distance mean 0.0000974158\n779(692) clusters containing matching NA types.\n1048(787) clusters containing matching HA types.\nMaximum memory used: 16254.1406Mb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory):0.4f}Mb.\")",
      "start_time": "2021-01-17T18:36:39.564Z"
     },
     {
      "end_time": "2021-01-18T11:20:14.555Z",
      "execution_time": "48ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. "
       },
       {
        "ename": "FileNotFoundError",
        "evalue": "[Errno 2] No such file or directory: 'Input/A.csv'",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-26-0bf07cfe1ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-25-3d5d076e5859>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mupload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
         "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
         "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Input/A.csv'"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-18T11:20:14.507Z"
     },
     {
      "end_time": "2021-01-18T11:42:15.970Z",
      "execution_time": "19m 32s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\n Starting calculations for segment 1:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 162.44 seconds.\n 10382 sequences, 6 unclustered, 239 cluster.\n Mean of inner cluster distance mean 0.0001108158\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\n Starting calculations for segment 2:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 154.35 seconds.\n 10406 sequences, 12 unclustered, 263 cluster.\n Mean of inner cluster distance mean 0.0000472150\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\n Starting calculations for segment 3:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 139.47 seconds.\n 10397 sequences, 7 unclustered, 235 cluster.\n Mean of inner cluster distance mean 0.0001054200\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\n Starting calculations for segment 4:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 151.33 seconds.\n 10512 sequences, 11 unclustered, 244 cluster.\n Mean of inner cluster distance mean 0.0000740418\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\n Starting calculations for segment 5:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 145.29 seconds.\n 10390 sequences, 18 unclustered, 251 cluster.\n Mean of inner cluster distance mean 0.0001167966\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\n Starting calculations for segment 6:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 144.38 seconds.\n 10496 sequences, 20 unclustered, 256 cluster.\n Mean of inner cluster distance mean 0.0001162096\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\n Starting calculations for segment 7:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 137.91 seconds.\n 10402 sequences, 37 unclustered, 245 cluster.\n Mean of inner cluster distance mean 0.0001648957\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\n Starting calculations for segment 8:\n Nucleotide k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Aminoacid k-mer frequency calculation. Finished.\n Running UMAP for dimension reduction. Finished.\n Running HDBscan for clustering. Finished.\n Centroid extraction. Finished.\n Clustering done in 135.61 seconds.\n 10405 sequences, 26 unclustered, 242 cluster.\n Mean of inner cluster distance mean 0.0001719148\n 0(0) clusters containing matching NA types.\n 0(0) clusters containing matching HA types.\nMaximum memory used: 1.90 Gb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-18T11:22:44.408Z"
     },
     {
      "end_time": "2021-01-18T14:07:10.322Z",
      "execution_time": "19m 37s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 163.20 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 156.37 seconds.\n- 10405 sequences, 12 unclustered, 263 cluster.\n- Mean of inner cluster distance mean 0.0000434237\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nStarting calculations for segment 3:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 137.21 seconds.\n- 10396 sequences, 16 unclustered, 231 cluster.\n- Mean of inner cluster distance mean 0.0000524843\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nStarting calculations for segment 4:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 149.73 seconds.\n- 10511 sequences, 13 unclustered, 245 cluster.\n- Mean of inner cluster distance mean 0.0000552752\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nStarting calculations for segment 5:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 146.05 seconds.\n- 10389 sequences, 15 unclustered, 252 cluster.\n- Mean of inner cluster distance mean 0.0000610103\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 147.72 seconds.\n- 10495 sequences, 14 unclustered, 253 cluster.\n- Mean of inner cluster distance mean 0.0000657352\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nStarting calculations for segment 7:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 140.08 seconds.\n- 10401 sequences, 29 unclustered, 246 cluster.\n- Mean of inner cluster distance mean 0.0000506483\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nStarting calculations for segment 8:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 136.12 seconds.\n- 10403 sequences, 46 unclustered, 247 cluster.\n- Mean of inner cluster distance mean 0.0001609608\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nOverall execution time 1176.48 seconds.\nMaximum memory used: 1.94 Gb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-18T13:47:32.843Z"
     },
     {
      "end_time": "2021-01-18T15:52:37.477Z",
      "execution_time": "2m 35s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. "
       },
       {
        "ename": "KeyError",
        "evalue": "'cluster'",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
         "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
         "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
         "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
         "\u001b[0;31mKeyError\u001b[0m: 'cluster'",
         "\nThe above exception was the direct cause of the following exception:\n",
         "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-113-0bf07cfe1ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-112-d39057b236fc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'centroid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mnew_clust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cluster != -1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mincrement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mnew_clust\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_clust\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mKeyError\u001b[0m: 'cluster'"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-18T15:50:02.368Z"
     },
     {
      "end_time": "2021-01-18T16:02:07.075Z",
      "execution_time": "2m 47s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 155.62 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "Process MemTimer-3:\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n"
       },
       {
        "ename": "KeyboardInterrupt",
        "evalue": "",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-126-0bf07cfe1ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-125-de25d41a961d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmatrix_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-2-b2ad2e68375b>\u001b[0m in \u001b[0;36mcalculate_frequence\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'^[ACGT]*$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                         \u001b[0mkmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkmer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/memory_profiler.py\", line 225, in run\n    stop = self.pipe.poll(self.interval)\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n    return self._poll(timeout)\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n    r = wait([self], timeout)\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n    ready = selector.select(timeout)\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/selectors.py\", line 376, in select\n    fd_event_list = self._poll.poll(timeout)\nKeyboardInterrupt\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-18T15:59:20.274Z"
     },
     {
      "end_time": "2021-01-18T17:14:23.775Z",
      "execution_time": "19m 21s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 163.33 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 155.19 seconds.\n- 10405 sequences, 12 unclustered, 263 cluster.\n- Mean of inner cluster distance mean 0.0000434237\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 3:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 136.42 seconds.\n- 10396 sequences, 16 unclustered, 231 cluster.\n- Mean of inner cluster distance mean 0.0000524843\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 4:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 146.01 seconds.\n- 10511 sequences, 13 unclustered, 245 cluster.\n- Mean of inner cluster distance mean 0.0000552752\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 5:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 144.96 seconds.\n- 10389 sequences, 15 unclustered, 252 cluster.\n- Mean of inner cluster distance mean 0.0000610103\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 144.69 seconds.\n- 10495 sequences, 14 unclustered, 253 cluster.\n- Mean of inner cluster distance mean 0.0000657352\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 7:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 134.64 seconds.\n- 10401 sequences, 29 unclustered, 246 cluster.\n- Mean of inner cluster distance mean 0.0000506483\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 8:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 134.43 seconds.\n- 10403 sequences, 46 unclustered, 247 cluster.\n- Mean of inner cluster distance mean 0.0001609608\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 1159.66 seconds.\nMaximum memory used: 1.94 Gb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-18T16:55:03.111Z"
     },
     {
      "end_time": "2021-01-18T17:47:16.749Z",
      "execution_time": "19m 10s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 163.14 seconds.\n- 10381 sequences, 13 unclustered, 232 cluster.\n- Mean of inner cluster distance mean 0.0001164629\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 153.85 seconds.\n- 10405 sequences, 12 unclustered, 263 cluster.\n- Mean of inner cluster distance mean 0.0000434237\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 3:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 133.67 seconds.\n- 10396 sequences, 16 unclustered, 231 cluster.\n- Mean of inner cluster distance mean 0.0000524843\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 4:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 145.48 seconds.\n- 10511 sequences, 13 unclustered, 245 cluster.\n- Mean of inner cluster distance mean 0.0000552752\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 5:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 144.89 seconds.\n- 10389 sequences, 15 unclustered, 252 cluster.\n- Mean of inner cluster distance mean 0.0000610103\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 142.81 seconds.\n- 10495 sequences, 14 unclustered, 253 cluster.\n- Mean of inner cluster distance mean 0.0000657352\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 7:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 133.56 seconds.\n- 10401 sequences, 29 unclustered, 246 cluster.\n- Mean of inner cluster distance mean 0.0000506483\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 8:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 131.43 seconds.\n- 10403 sequences, 46 unclustered, 247 cluster.\n- Mean of inner cluster distance mean 0.0001609608\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 1148.83 seconds.\nMaximum memory used: 1.95 Gb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-18T17:28:06.919Z"
     },
     {
      "end_time": "2021-01-18T20:28:46.966Z",
      "execution_time": "2h 39m 11s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1166.79 seconds.\n- 55979 sequences, 55 unclustered, 874 cluster.\n- Mean of inner cluster distance mean 0.0000685445\n- 558(516) clusters containing matching NA types.\n- 572(522) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1294.37 seconds.\n- 55933 sequences, 52 unclustered, 862 cluster.\n- Mean of inner cluster distance mean 0.0000667975\n- 586(549) clusters containing matching NA types.\n- 601(563) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 3:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1344.59 seconds.\n- 56021 sequences, 53 unclustered, 923 cluster.\n- Mean of inner cluster distance mean 0.0001168338\n- 634(584) clusters containing matching NA types.\n- 646(594) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 4:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1150.09 seconds.\n- 56600 sequences, 41 unclustered, 842 cluster.\n- Mean of inner cluster distance mean 0.0000675273\n- 630(556) clusters containing matching NA types.\n- 839(623) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 5:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1131.26 seconds.\n- 56087 sequences, 53 unclustered, 882 cluster.\n- Mean of inner cluster distance mean 0.0000889324\n- 619(574) clusters containing matching NA types.\n- 633(584) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1091.69 seconds.\n- 56529 sequences, 81 unclustered, 909 cluster.\n- Mean of inner cluster distance mean 0.0000839112\n- 903(675) clusters containing matching NA types.\n- 689(618) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 7:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1148.25 seconds.\n- 56081 sequences, 137 unclustered, 889 cluster.\n- Mean of inner cluster distance mean 0.0001458667\n- 621(575) clusters containing matching NA types.\n- 641(594) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 8:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1218.37 seconds.\n- 56174 sequences, 136 unclustered, 912 cluster.\n- Mean of inner cluster distance mean 0.0002149102\n- 630(576) clusters containing matching NA types.\n- 649(591) clusters containing matching HA types.\nFinished.\nOverall execution time 9545.40 seconds.\nMaximum memory used: 19.22 Gb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-18T17:49:36.174Z"
     },
     {
      "end_time": "2021-01-19T13:36:27.134Z",
      "execution_time": "20m 1s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1196.37 seconds.\n- 55436 sequences, 41 unclustered, 881 cluster.\n- Mean of inner cluster distance mean 0.0000639726\n- 558(516) clusters containing matching NA types.\n- 576(526) clusters containing matching HA types.\nFinished.\nOverall execution time 1196.37 seconds.\nMaximum memory used: 13.16 Gb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-19T13:16:26.113Z"
     },
     {
      "end_time": "2021-01-19T15:33:16.596Z",
      "execution_time": "18m 54s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 156.24 seconds.\n- 10326 sequences, 7 unclustered, 236 cluster.\n- Mean of inner cluster distance mean 0.0001498887\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 151.02 seconds.\n- 10360 sequences, 7 unclustered, 258 cluster.\n- Mean of inner cluster distance mean 0.0000465008\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 3:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 131.79 seconds.\n- 10272 sequences, 12 unclustered, 230 cluster.\n- Mean of inner cluster distance mean 0.0000481188\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 4:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. "
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": "/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/umap/spectral.py:253: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\nfailed. This is likely due to too small an eigengap. Consider\nadding some noise or jitter to your data.\n\nFalling back to random initialisation!\n  \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 145.77 seconds.\n- 10482 sequences, 9 unclustered, 245 cluster.\n- Mean of inner cluster distance mean 0.0000517155\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 5:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 141.69 seconds.\n- 10184 sequences, 24 unclustered, 247 cluster.\n- Mean of inner cluster distance mean 0.0000660574\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 142.85 seconds.\n- 10417 sequences, 10 unclustered, 251 cluster.\n- Mean of inner cluster distance mean 0.0000574508\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 7:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 132.16 seconds.\n- 10254 sequences, 31 unclustered, 242 cluster.\n- Mean of inner cluster distance mean 0.0001082707\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 8:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 131.17 seconds.\n- 10325 sequences, 32 unclustered, 240 cluster.\n- Mean of inner cluster distance mean 0.0000626106\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 1132.68 seconds.\nMaximum memory used: 1.94 Gb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-19T15:14:22.866Z"
     },
     {
      "end_time": "2021-01-19T16:45:16.662Z",
      "execution_time": "16m 6s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. "
       },
       {
        "ename": "ValueError",
        "evalue": "Min samples and Min cluster size must be positive integers",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-10-0bf07cfe1ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-9-454cfc183c89>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mcluster_selection_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#don't seperate clusters with a distance less than value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#don't mess with this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         ).fit(matrix)\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    917\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condensed_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_linkage_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m          self._min_spanning_tree) = hdbscan(X, **kwargs)\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mhdbscan\u001b[0;34m(X, min_cluster_size, min_samples, alpha, cluster_selection_epsilon, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin_cluster_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         raise ValueError('Min samples and Min cluster size must be positive'\n\u001b[0m\u001b[1;32m    491\u001b[0m                          ' integers')\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mValueError\u001b[0m: Min samples and Min cluster size must be positive integers"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()\n    #memory = memory_usage(main)\n    #print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-19T16:29:10.728Z"
     },
     {
      "end_time": "2021-01-19T16:49:16.273Z",
      "execution_time": "2m 10s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 128.78 seconds.\n- 10417 sequences, 5 unclustered, 257 cluster.\n- Mean of inner cluster distance mean 0.0000449730\n- 0(0) clusters containing matching NA types.\n- 0(0) clusters containing matching HA types.\nFinished.\nOverall execution time 128.78 seconds.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()\n    #memory = memory_usage(main)\n    #print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-19T16:47:06.624Z"
     },
     {
      "end_time": "2021-01-19T17:06:32.198Z",
      "execution_time": "16m 21s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 977.08 seconds.\n- 55075 sequences, 25 unclustered, 940 cluster.\n- Mean of inner cluster distance mean 0.0000845859\n- 936(709) clusters containing matching NA types.\n- 729(652) clusters containing matching HA types.\nFinished.\nOverall execution time 977.08 seconds.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()\n    #memory = memory_usage(main)\n    #print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-19T16:50:10.872Z"
     },
     {
      "end_time": "2021-01-22T16:29:15.120Z",
      "execution_time": "17m 54s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Aminoacid k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 1069.53 seconds.\n- 55075 sequences, 82 unclustered, 927 cluster.\n- Mean of inner cluster distance mean 0.0000722766\n- 924(685) clusters containing matching NA types.\n- 705(624) clusters containing matching HA types.\nFinished.\nOverall execution time 1069.53 seconds.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()\n    #memory = memory_usage(main)\n    #print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-22T16:11:21.414Z"
     },
     {
      "end_time": "2021-01-22T17:54:25.105Z",
      "execution_time": "11m 34s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 689.94 seconds.\n- 55075 sequences, 9 unclustered, 502 cluster.\n- Mean of inner cluster distance mean 0.0001359075\n- 499(349) clusters containing matching NA types.\n- 367(321) clusters containing matching HA types.\nFinished.\nOverall execution time 689.94 seconds.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()\n    #memory = memory_usage(main)\n    #print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-22T17:42:51.181Z"
     },
     {
      "end_time": "2021-01-22T18:29:56.937Z",
      "execution_time": "10m 57s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 4:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 653.12 seconds.\n- 55281 sequences, 3 unclustered, 440 cluster.\n- Mean of inner cluster distance mean 0.0001725082\n- 308(279) clusters containing matching NA types.\n- 439(316) clusters containing matching HA types.\nFinished.\nOverall execution time 653.12 seconds.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()\n    #memory = memory_usage(main)\n    #print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-22T18:18:59.888Z"
     },
     {
      "end_time": "2021-01-22T20:17:27.374Z",
      "execution_time": "1h 39m 11s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 1:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 687.20 seconds.\n- 55436 sequences, 0 unclustered, 441 cluster.\n- Mean of inner cluster distance mean 0.0001269798\n- 259(236) clusters containing matching NA types.\n- 269(243) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 2:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 744.98 seconds.\n- 55292 sequences, 1 unclustered, 435 cluster.\n- Mean of inner cluster distance mean 0.0001319285\n- 286(264) clusters containing matching NA types.\n- 291(270) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 3:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 779.03 seconds.\n- 55351 sequences, 0 unclustered, 494 cluster.\n- Mean of inner cluster distance mean 0.0000922217\n- 337(311) clusters containing matching NA types.\n- 341(317) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 4:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 651.19 seconds.\n- 55281 sequences, 0 unclustered, 444 cluster.\n- Mean of inner cluster distance mean 0.0001553733\n- 317(287) clusters containing matching NA types.\n- 444(320) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 5:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 749.04 seconds.\n- 55541 sequences, 0 unclustered, 508 cluster.\n- Mean of inner cluster distance mean 0.0001125889\n- 349(319) clusters containing matching NA types.\n- 359(327) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 6:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 684.92 seconds.\n- 55075 sequences, 9 unclustered, 502 cluster.\n- Mean of inner cluster distance mean 0.0001359075\n- 499(349) clusters containing matching NA types.\n- 367(321) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 7:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 795.55 seconds.\n- 55620 sequences, 6 unclustered, 513 cluster.\n- Mean of inner cluster distance mean 0.0001000632\n- 349(319) clusters containing matching NA types.\n- 362(332) clusters containing matching HA types.\nFinished.\nStarting calculations for segment 8:\n- Nucleotide k-mer frequency calculation. Finished.\n- Running UMAP for dimension reduction. Finished.\n- Running HDBscan for clustering. Finished.\n- Centroid extraction. Finished.\n- Clustering done in 853.50 seconds.\n- 55563 sequences, 2 unclustered, 516 cluster.\n- Mean of inner cluster distance mean 0.0000799072\n- 336(299) clusters containing matching NA types.\n- 344(306) clusters containing matching HA types.\nFinished.\nOverall execution time 5945.39 seconds.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()\n    #memory = memory_usage(main)\n    #print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")",
      "start_time": "2021-01-22T18:38:16.681Z"
     }
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read input and settings file. Finished.\n",
      "Starting calculations for segment 1:\n",
      "- Nucleotide k-mer frequency calculation. Finished.\n",
      "- Running UMAP for dimension reduction. Finished.\n",
      "- Running HDBscan for clustering. Finished.\n",
      "- Centroid extraction. Finished.\n",
      "- Clustering done in 687.20 seconds.\n",
      "- 55436 sequences, 0 unclustered, 441 cluster.\n",
      "- Mean of inner cluster distance mean 0.0001269798\n",
      "- 259(236) clusters containing matching NA types.\n",
      "- 269(243) clusters containing matching HA types.\n",
      "Finished.\n",
      "Starting calculations for segment 2:\n",
      "- Nucleotide k-mer frequency calculation. Finished.\n",
      "- Running UMAP for dimension reduction. Finished.\n",
      "- Running HDBscan for clustering. Finished.\n",
      "- Centroid extraction. Finished.\n",
      "- Clustering done in 744.98 seconds.\n",
      "- 55292 sequences, 1 unclustered, 435 cluster.\n",
      "- Mean of inner cluster distance mean 0.0001319285\n",
      "- 286(264) clusters containing matching NA types.\n",
      "- 291(270) clusters containing matching HA types.\n",
      "Finished.\n",
      "Starting calculations for segment 3:\n",
      "- Nucleotide k-mer frequency calculation. Finished.\n",
      "- Running UMAP for dimension reduction. Finished.\n",
      "- Running HDBscan for clustering. Finished.\n",
      "- Centroid extraction. Finished.\n",
      "- Clustering done in 779.03 seconds.\n",
      "- 55351 sequences, 0 unclustered, 494 cluster.\n",
      "- Mean of inner cluster distance mean 0.0000922217\n",
      "- 337(311) clusters containing matching NA types.\n",
      "- 341(317) clusters containing matching HA types.\n",
      "Finished.\n",
      "Starting calculations for segment 4:\n",
      "- Nucleotide k-mer frequency calculation. Finished.\n",
      "- Running UMAP for dimension reduction. Finished.\n",
      "- Running HDBscan for clustering. Finished.\n",
      "- Centroid extraction. Finished.\n",
      "- Clustering done in 651.19 seconds.\n",
      "- 55281 sequences, 0 unclustered, 444 cluster.\n",
      "- Mean of inner cluster distance mean 0.0001553733\n",
      "- 317(287) clusters containing matching NA types.\n",
      "- 444(320) clusters containing matching HA types.\n",
      "Finished.\n",
      "Starting calculations for segment 5:\n",
      "- Nucleotide k-mer frequency calculation. Finished.\n",
      "- Running UMAP for dimension reduction. Finished.\n",
      "- Running HDBscan for clustering. Finished.\n",
      "- Centroid extraction. Finished.\n",
      "- Clustering done in 749.04 seconds.\n",
      "- 55541 sequences, 0 unclustered, 508 cluster.\n",
      "- Mean of inner cluster distance mean 0.0001125889\n",
      "- 349(319) clusters containing matching NA types.\n",
      "- 359(327) clusters containing matching HA types.\n",
      "Finished.\n",
      "Starting calculations for segment 6:\n",
      "- Nucleotide k-mer frequency calculation. Finished.\n",
      "- Running UMAP for dimension reduction. Finished.\n",
      "- Running HDBscan for clustering. Finished.\n",
      "- Centroid extraction. Finished.\n",
      "- Clustering done in 684.92 seconds.\n",
      "- 55075 sequences, 9 unclustered, 502 cluster.\n",
      "- Mean of inner cluster distance mean 0.0001359075\n",
      "- 499(349) clusters containing matching NA types.\n",
      "- 367(321) clusters containing matching HA types.\n",
      "Finished.\n",
      "Starting calculations for segment 7:\n",
      "- Nucleotide k-mer frequency calculation. Finished.\n",
      "- Running UMAP for dimension reduction. Finished.\n",
      "- Running HDBscan for clustering. Finished.\n",
      "- Centroid extraction. Finished.\n",
      "- Clustering done in 795.55 seconds.\n",
      "- 55620 sequences, 6 unclustered, 513 cluster.\n",
      "- Mean of inner cluster distance mean 0.0001000632\n",
      "- 349(319) clusters containing matching NA types.\n",
      "- 362(332) clusters containing matching HA types.\n",
      "Finished.\n",
      "Starting calculations for segment 8:\n",
      "- Nucleotide k-mer frequency calculation. Finished.\n",
      "- Running UMAP for dimension reduction. Finished.\n",
      "- Running HDBscan for clustering. Finished.\n",
      "- Centroid extraction. Finished.\n",
      "- Clustering done in 853.50 seconds.\n",
      "- 55563 sequences, 2 unclustered, 516 cluster.\n",
      "- Mean of inner cluster distance mean 0.0000799072\n",
      "- 336(299) clusters containing matching NA types.\n",
      "- 344(306) clusters containing matching HA types.\n",
      "Finished.\n",
      "Overall execution time 5945.39 seconds.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()\n",
    "    #memory = memory_usage(main)\n",
    "    #print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
