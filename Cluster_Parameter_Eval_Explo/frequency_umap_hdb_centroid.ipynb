{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import collections as co\n",
    "import itertools as it\n",
    "import umap\n",
    "import hdbscan\n",
    "import time \n",
    "import difflib\n",
    "import scipy.spatial.distance as ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vectorizer(object):\n",
    "    \n",
    "    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n",
    "    def __init__(self, k = 7, convert = 0):\n",
    "    \n",
    "        self.k = k\n",
    "        self.convert = convert\n",
    "        self.exist = co.defaultdict(int) \n",
    "        self.keys = list(self.exist.keys())\n",
    "        self.col = len(self.keys)\n",
    "        self.row = 0\n",
    "        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n",
    "        self.subtype = np.empty((self.row, 1, ),dtype = \"object\")\n",
    "        self.index = np.empty(self.row, dtype = \"object\")\n",
    "        self.amino = co.defaultdict(str, {\n",
    "            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n",
    "            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n",
    "            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n",
    "            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n",
    "            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n",
    "            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n",
    "            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n",
    "            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n",
    "            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n",
    "        })\n",
    "                \n",
    "    def translate(self, read):\n",
    "    \n",
    "        chain = ''\n",
    "\n",
    "        for i in range(len(read) - 2):\n",
    "            trip = read[i:i+3]\n",
    "            chain += self.amino[trip]\n",
    "\n",
    "        return(chain)\n",
    "    \n",
    "    \n",
    "    def adjust_to_data(self, infile):\n",
    "    \n",
    "        self.row = sum(1 for l in open(infile))\n",
    "        data = pd.read_csv(infile, chunksize = 10000, sep = ';', na_filter = False, header = None)\n",
    "        \n",
    "        for chunk in data:\n",
    "\n",
    "            for line, info, read in chunk.itertuples(index=True, name=None):\n",
    "                \n",
    "                if self.convert == 1:\n",
    "                    #seq = self.translate(re.sub('[^ACGT]+', '', read))\n",
    "                    seq = self.translate(read)\n",
    "                    del read\n",
    "                    \n",
    "                    num = len(seq) - self.k + 1\n",
    "                    \n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        self.exist[kmer] = 0\n",
    "                    \n",
    "                else:\n",
    "                    #seq = re.sub('[^ACGT]+', '', read)\n",
    "                    seq = read\n",
    "                    del read\n",
    "\n",
    "                    num = len(seq) - self.k + 1\n",
    "\n",
    "                    if re.match('^[ACGT]*$', seq): \n",
    "                        for i in range(num):\n",
    "                            kmer = seq[i:i+self.k]\n",
    "                            self.exist[kmer] = 0\n",
    "                    else:\n",
    "                        for i in range(num):\n",
    "                            kmer = seq[i:i+self.k]\n",
    "                            if re.match('^[ACGT]*$', kmer): \n",
    "                                self.exist[kmer] = 0\n",
    "            \n",
    "        self.keys = list(self.exist.keys())\n",
    "        self.col = len(self.keys)\n",
    "        self.index = np.empty(self.row, dtype = \"object\")\n",
    "        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n",
    "        self.subtype = np.empty((self.row, 1, ), dtype = \"object\")\n",
    "        \n",
    "        del seq\n",
    "    \n",
    "    def calculate_frequence(self, infile):\n",
    "        \n",
    "        data = pd.read_csv(infile, chunksize = 10000, sep = ';', na_filter = False, header = None)\n",
    "        \n",
    "        for chunk in data:\n",
    "\n",
    "            for line, info, read in chunk.itertuples(index=True, name=None):      \n",
    "                \n",
    "                if self.convert == 1:\n",
    "                    #seq = self.translate(re.sub('[^ACGT]+', '', read))\n",
    "                    seq = self.translate(read)\n",
    "                    del read\n",
    "                \n",
    "                    counts = self.exist.copy()\n",
    "                    num = len(seq) - self.k + 1\n",
    "\n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        counts[kmer] += 1\n",
    "                            \n",
    "                else:\n",
    "                    #seq = re.sub('[^ACGT]+', '', read)\n",
    "                    seq = read\n",
    "                    del read\n",
    "                \n",
    "                    counts = self.exist.copy()\n",
    "                    num = len(seq) - self.k + 1\n",
    "\n",
    "                    if re.match('^[ACGT]*$', seq): \n",
    "                        for i in range(num):\n",
    "                            kmer = seq[i:i+self.k]\n",
    "                            counts[kmer] += 1\n",
    "                    else:\n",
    "                        for i in range(num):\n",
    "                            kmer = seq[i:i+self.k]\n",
    "                            if re.match('^[ACGT]*$', kmer): \n",
    "                                counts[kmer] += 1\n",
    "\n",
    "                vector = np.array(list(counts.values()), dtype = \"float32\")/num\n",
    "                \n",
    "                self.index[line] = info.split('|')[0][1:]\n",
    "                self.matrix[line] = vector\n",
    "                self.subtype[line] = info.split('|')[5]\n",
    "                \n",
    "                counts.clear()\n",
    "                del vector\n",
    "                del seq\n",
    "                del counts\n",
    "    \n",
    "    def get_index(self):\n",
    "        \n",
    "        return(self.index)\n",
    "    \n",
    "    \n",
    "    def get_keys(self):\n",
    "        \n",
    "        return(self.keys)\n",
    "    \n",
    "    \n",
    "    def get_subtype(self):\n",
    "        \n",
    "        return(self.subtype)\n",
    "    \n",
    "    def get_matrix(self):\n",
    "        \n",
    "        return(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extractor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.cities = co.defaultdict(list)\n",
    "        self.subcountries = co.defaultdict(list)\n",
    "        self.countries = co.defaultdict(list)\n",
    "        \n",
    "        self.cities_names = []\n",
    "        self.subcountries_names = []\n",
    "        self.countries_names = []\n",
    "        \n",
    "        self.matrix_genome = np.empty((0, 0, ), dtype=object)\n",
    "        self.matrix_strain = np.empty((0, 0, ), dtype=object)\n",
    "        self.matrix_head = np.empty((0, 0, ), dtype=object)\n",
    "        \n",
    "        self.index_accession = np.empty(0, dtype=object)\n",
    "        self.index_strain = np.empty(0, dtype=object)\n",
    "  \n",
    "\n",
    "    def init_matrix(self, infile):\n",
    "        \n",
    "        lines = sum(1 for l in open(infile))\n",
    "        \n",
    "        self.matrix_genome = np.empty((lines, 1, ), dtype=object)\n",
    "        self.matrix_strain = np.empty((lines, 6, ), dtype=object)\n",
    "        self.matrix_head = np.empty((lines, 2, ), dtype=object)\n",
    "        \n",
    "        self.index_accession = np.empty(lines, dtype=object)\n",
    "        self.index_strain = np.empty(lines, dtype=object)\n",
    "    \n",
    "    \n",
    "    def fill_dicts(self, worldfile):\n",
    "        \n",
    "        data = pd.read_csv(worldfile, chunksize = 10000, sep = ',', na_filter = False)\n",
    "        \n",
    "        for split in data:\n",
    "\n",
    "            for city, country, subcountry, geonameid in split.itertuples(index=False, name=None):\n",
    "\n",
    "                self.cities[city].append([city, subcountry, country])\n",
    "                self.subcountries[subcountry]. append(['null', subcountry, country])\n",
    "                self.countries[country].append(['null', 'null', country])\n",
    "\n",
    "        self.cities_names = list(self.cities.keys())\n",
    "        self.subcountries_names = list(self.subcountries.keys())\n",
    "        self.countries_names = list(self.countries.keys())\n",
    "\n",
    "        \n",
    "    def destination(self, entry):\n",
    "\n",
    "        match_subcountry = difflib.get_close_matches(entry, self.subcountries_names, 1, 0.9)\n",
    "        if not match_subcountry:\n",
    "            match_city = difflib.get_close_matches(entry, self.cities_names, 1, 0.9)\n",
    "            if not match_city:\n",
    "                match_country = difflib.get_close_matches(entry, self.countries_names, 1, 0.9)\n",
    "                if not match_country:\n",
    "                    result = ['null', 'null', 'null']\n",
    "                else:\n",
    "                    match = match_country[0]\n",
    "                    result = self.countries[match]\n",
    "            else:\n",
    "                match = match_city[0]\n",
    "                result = self.cities[match]\n",
    "        else:\n",
    "            match = match_subcountry[0]\n",
    "            result = self.subcountries[match]\n",
    "\n",
    "        if any(isinstance(i, list) for i in result):\n",
    "            output = result[0]\n",
    "        else:\n",
    "            output = result\n",
    "\n",
    "        return(output)\n",
    "    \n",
    "    \n",
    "    def process_rows(self, chunk):\n",
    "\n",
    "        line = chunk[0]\n",
    "        info = chunk[1]\n",
    "        read = chunk[2]\n",
    "\n",
    "        head = info.split('|')\n",
    "\n",
    "        accession = head[0][1:]\n",
    "        strain = head[1]\n",
    "        segment = head[2]\n",
    "        organism = head[4]\n",
    "        subtype = head[5]\n",
    "        if subtype == 'NA' or subtype == 'nan':\n",
    "            subtype = 'null'\n",
    "        host = head[7]\n",
    "\n",
    "        info = strain.split('/')\n",
    "\n",
    "        spec = info[0]\n",
    "        del info[0]\n",
    "        year = info[-1]\n",
    "        del info[-1]\n",
    "\n",
    "        if year.isdecimal():\n",
    "            if len(year) == 2:\n",
    "                year = '19'+year \n",
    "        else:\n",
    "            year = 'null'\n",
    "\n",
    "        if not info:\n",
    "            pos = ['null', 'null', 'null']\n",
    "        else:\n",
    "            for i in info:\n",
    "                pos = self.destination(i)\n",
    "                if not all([item == 'null' for item in pos]):\n",
    "                    break\n",
    "\n",
    "        values = [accession, read, spec, pos[0], pos[1], pos[2], year, host, strain, segment]\n",
    "\n",
    "        return(line, values)\n",
    "\n",
    "    \n",
    "    def input_sequences(self, infile, procs):\n",
    "        \n",
    "        data = pd.read_csv(infile, chunksize = 10000, sep = ';', na_filter = False, header = None)\n",
    "        \n",
    "        #dt = np.dtype([('R','u1'), ('G','u1'), ('B','u1'), ('A','u1')])\n",
    "        \n",
    "        for chunk in data:\n",
    "            \n",
    "            with mp.Pool(procs) as pool:\n",
    "                \n",
    "                for i, values in pool.map(self.process_rows, chunk.itertuples(index=True, name=None)):\n",
    "            \n",
    "                    self.matrix_genome[i] = np.array([values[1]])\n",
    "                    self.matrix_strain[i] = np.array([values[2], values[3], values[4], values[5], values[6], values[7]])\n",
    "                    self.matrix_head[i] = np.array([values[8], values[9]])\n",
    "                    self.index_accession[i] = values[0]\n",
    "                    self.index_strain[i] = values[8]\n",
    "            \n",
    "        \n",
    "    def get_genomes(self):\n",
    "        \n",
    "        genomes = pd.DataFrame(self.matrix_genome, index = self.index_accession, columns = ['genome'])\n",
    "        \n",
    "        return(genomes)\n",
    "    \n",
    "    def get_strains(self):\n",
    "        \n",
    "        strains = pd.DataFrame(self.matrix_strain, index = self.index_strain, columns = ['species', 'city', 'subcountry', 'country', 'year', 'host']).drop_duplicates()\n",
    "        \n",
    "        return(strains)\n",
    "    \n",
    "    def get_header(self):\n",
    "        \n",
    "        header = pd.DataFrame(self.matrix_head, self.index_accession, columns = ['strain', 'segment'])\n",
    "        \n",
    "        return(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    print(\"Welcome. Read input and settings file.\", end = ' ')\n",
    "    #error checkup?\n",
    "\n",
    "    infile = '../../../Desktop/Masterthesis_V5/A_HA.csv'   \n",
    "    outfile = 'output.csv'\n",
    "    setfile = 'settings.csv'\n",
    "    #worldfile = 'cities.csv'\n",
    "    \n",
    "    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n",
    "    parameter = settings.loc[4].to_list()\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n",
    "\n",
    "    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n",
    "    freq_nt.adjust_to_data(infile)\n",
    "    freq_nt.calculate_frequence(infile)\n",
    "\n",
    "    matrix_nt = freq_nt.get_matrix()\n",
    "    index_nt = freq_nt.get_index()   \n",
    "    subtype_nt = freq_nt.get_subtype()\n",
    "    keys_nt = freq_nt.get_keys()\n",
    "\n",
    "    del freq_nt\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    print(\"Running UMAP for dimension reduction.\", end = ' ')\n",
    "\n",
    "    matrix_nt_red = umap.UMAP(\n",
    "        n_neighbors = parameter[1].item(),\n",
    "        min_dist = parameter[2].item(),\n",
    "        n_components = parameter[3].item(),\n",
    "        random_state = parameter[4].item(),\n",
    "        metric = parameter[5],\n",
    "    ).fit_transform(matrix_nt)\n",
    "\n",
    "    del matrix_nt\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n",
    "\n",
    "    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n",
    "    freq_aa.adjust_to_data(infile)\n",
    "    freq_aa.calculate_frequence(infile)\n",
    "\n",
    "    matrix_aa = freq_aa.get_matrix()\n",
    "    index_aa = freq_aa.get_index()\n",
    "    keys_aa = freq_aa.get_keys()\n",
    "\n",
    "    del freq_aa\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    print(\"Running UMAP for dimension reduction.\", end = ' ')\n",
    "\n",
    "    matrix_aa_red = umap.UMAP(\n",
    "        n_neighbors = parameter[7].item(),\n",
    "        min_dist = parameter[8].item(),\n",
    "        n_components = parameter[9].item(),\n",
    "        random_state = parameter[10].item(),\n",
    "        metric = parameter[11],\n",
    "    ).fit_transform(matrix_aa)\n",
    "\n",
    "    del matrix_aa\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    matrix_aa_ind = pd.DataFrame(matrix_aa_red, index = index_aa)\n",
    "    matrix_nt_ind = pd.DataFrame(matrix_nt_red, index = index_nt)\n",
    "\n",
    "    matrix = pd.concat([matrix_nt_ind, matrix_aa_ind], axis=1, copy = False, ignore_index = True) #falsches Ergebnis? checken ob ignore_index = Fehler\n",
    "\n",
    "    print(\"Running HDBscan for clustering.\", end = ' ')\n",
    "\n",
    "    matrix_clust = hdbscan.HDBSCAN(\n",
    "        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n",
    "        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n",
    "        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n",
    "        alpha = parameter[15].item(), #don't mess with this\n",
    "    ).fit(matrix)\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    print(\"Centroid extraction.\", end = ' ')\n",
    "\n",
    "    clusterlabel = matrix_clust.labels_\n",
    "\n",
    "    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), index = index_nt, columns = ['cluster', 'centroid'])\n",
    "    subtype = pd.DataFrame(subtype_nt, index = index_nt, columns = ['subtype'])\n",
    "    clusters = pd.concat([blank, subtype], axis=1, copy = False)\n",
    "\n",
    "    num = clusters['cluster'].max()+1\n",
    "    values = ['true']*num\n",
    "    accessions = []\n",
    "    exclude = []\n",
    "    include = []\n",
    "\n",
    "    for i in range(num):\n",
    "\n",
    "        query = clusters[clusters.cluster == i]\n",
    "        match = query.index.values.tolist()\n",
    "        sub = matrix.filter(items = match, axis=0)\n",
    "        dist = ssd.cdist(sub, sub, metric = parameter[16])\n",
    "        accessions.append(pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean().idxmin())\n",
    "        subs = co.defaultdict(list) \n",
    "\n",
    "        for sub in query['subtype'].tolist():\n",
    "            if re.match('^[H][0-9]+N[0-9]+$', sub): \n",
    "                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n",
    "                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n",
    "            else: #mixed types sind nicht zwingend falsch, schwierig zu bewerten ob sie beachtet werden oder nicht\n",
    "                subs['X'].append('X0')\n",
    "                subs['X'].append('X0')\n",
    "\n",
    "        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n",
    "            exclude.append(2)\n",
    "            if 'X' not in subs.keys():\n",
    "                include.append(2)\n",
    "        elif len(set(subs['H'])) == 1:\n",
    "            exclude.append(1)\n",
    "            if 'X' not in subs.keys():\n",
    "                include.append(1)\n",
    "        elif len(set(subs['N'])) == 1:\n",
    "            exclude.append(0)\n",
    "            if 'X' not in subs.keys():\n",
    "                include.append(0)\n",
    "\n",
    "    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n",
    "\n",
    "    clusters.update(centroids)\n",
    "    #clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n",
    "    \n",
    "    stop = time.perf_counter()\n",
    "\n",
    "    print(\"Finished.\")\n",
    "    print(f\"Clustering done in {stop - start:0.4f} seconds.\")\n",
    "    diagnostic = co.Counter(clusterlabel)\n",
    "    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n",
    "    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n",
    "    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n",
    "    \n",
    "    tables = extractor()\n",
    "    tables.fill_dicts(worldfile)\n",
    "    tables.init_matrix(infile)\n",
    "    tables.input_sequences(infile, 8)\n",
    "    \n",
    "    header = tables.get_header()\n",
    "    strains = tables.get_strains()\n",
    "    genomes = tables.get_genomes()\n",
    "    informations = pd.concat([header, clusters], axis=1, copy = False)\n",
    "    \n",
    "    #informations.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n",
    "    #strains.to_csv(outfile, index=True, header=True, sep=',')\n",
    "    #genomes.to_csv(outfile, index=True, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome. Read input and settings file. Finished.\n",
      "Nucleotide k-mer frequency calculation. Finished.\n",
      "Running UMAP for dimension reduction. Finished.\n",
      "Aminoacid k-mer frequency calculation. Finished.\n",
      "Running UMAP for dimension reduction. Finished.\n",
      "Running HDBscan for clustering. Finished.\n",
      "Centroid extraction. Finished.\n",
      "Clustering done in 1120.5723 seconds.\n",
      "56600 sequences, 35 unclustered, 831 cluster.\n",
      "630(556) clusters containing matching NA types.\n",
      "827(618) clusters containing matching HA types.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
