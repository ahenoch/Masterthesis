{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-17T18:36:36.150419Z",
     "start_time": "2021-01-17T18:36:34.601356Z"
    },
    "provenance": [
     {
      "end_time": "Unknown",
      "execution_time": "Unknown",
      "outputs": [],
      "source": "import functools as ft\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport difflib\nimport scipy.spatial.distance as ssd",
      "start_time": "Unknown"
     },
     {
      "end_time": "2021-01-15T08:57:18.616Z",
      "execution_time": "1.90s",
      "outputs": [],
      "source": "import functools as ft\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport difflib\nimport scipy.spatial.distance as ssd",
      "start_time": "2021-01-15T08:57:16.717Z"
     },
     {
      "end_time": "2021-01-15T08:58:42.143Z",
      "execution_time": "1.56s",
      "outputs": [],
      "source": "import functools as ft\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport difflib\nimport scipy.spatial.distance as ssd",
      "start_time": "2021-01-15T08:58:40.580Z"
     },
     {
      "end_time": "2021-01-15T09:00:21.985Z",
      "execution_time": "6ms",
      "outputs": [],
      "source": "import functools as ft\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport difflib\nimport scipy.spatial.distance as ssd",
      "start_time": "2021-01-15T09:00:21.979Z"
     },
     {
      "end_time": "2021-01-17T17:13:58.363Z",
      "execution_time": "1.83s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time ",
      "start_time": "2021-01-17T17:13:56.528Z"
     },
     {
      "end_time": "2021-01-17T17:15:09.350Z",
      "execution_time": "1.61s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time ",
      "start_time": "2021-01-17T17:15:07.741Z"
     },
     {
      "end_time": "2021-01-17T17:17:20.813Z",
      "execution_time": "1.53s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time ",
      "start_time": "2021-01-17T17:17:19.284Z"
     },
     {
      "end_time": "2021-01-17T17:24:00.228Z",
      "execution_time": "1.54s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time ",
      "start_time": "2021-01-17T17:23:58.684Z"
     },
     {
      "end_time": "2021-01-17T17:49:45.894Z",
      "execution_time": "6ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd",
      "start_time": "2021-01-17T17:49:45.888Z"
     },
     {
      "end_time": "2021-01-17T18:16:39.749Z",
      "execution_time": "10ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-17T18:16:39.739Z"
     },
     {
      "end_time": "2021-01-17T18:21:52.651Z",
      "execution_time": "5ms",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-17T18:21:52.646Z"
     },
     {
      "end_time": "2021-01-17T18:36:36.150Z",
      "execution_time": "1.55s",
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport sys\nimport re\nimport csv\nimport collections as co\nimport itertools as it\nimport umap\nimport hdbscan\nimport time \nimport scipy.spatial.distance as ssd\nfrom memory_profiler import memory_usage",
      "start_time": "2021-01-17T18:36:34.601Z"
     }
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import collections as co\n",
    "import itertools as it\n",
    "import umap\n",
    "import hdbscan\n",
    "import time \n",
    "import scipy.spatial.distance as ssd\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-17T18:36:37.171666Z",
     "start_time": "2021-01-17T18:36:37.023101Z"
    },
    "provenance": [
     {
      "end_time": "Unknown",
      "execution_time": "Unknown",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.subtype = np.empty((self.row, 1, ),dtype = \"object\")\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        self.subtype = np.empty((self.row, 1, ), dtype = \"object\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.index[line] = info\n            self.matrix[line] = vector\n            self.subtype[line] = subtype\n\n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    def get_index(self):\n        \n        return(self.index)\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_subtype(self):\n        \n        return(self.subtype)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "Unknown"
     },
     {
      "end_time": "2021-01-15T08:57:19.841Z",
      "execution_time": "187ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.subtype = np.empty((self.row, 1, ),dtype = \"object\")\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        self.subtype = np.empty((self.row, 1, ), dtype = \"object\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.index[line] = info\n            self.matrix[line] = vector\n            self.subtype[line] = subtype\n\n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    def get_index(self):\n        \n        return(self.index)\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_subtype(self):\n        \n        return(self.subtype)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-15T08:57:19.654Z"
     },
     {
      "end_time": "2021-01-15T09:00:23.526Z",
      "execution_time": "185ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.subtype = np.empty((self.row, 1, ),dtype = \"object\")\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.index = np.empty(self.row, dtype = \"object\")\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        self.subtype = np.empty((self.row, 1, ), dtype = \"object\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, info, subtype, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.index[line] = info\n            self.matrix[line] = vector\n            self.subtype[line] = subtype\n\n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    def get_index(self):\n        \n        return(self.index)\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_subtype(self):\n        \n        return(self.subtype)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-15T09:00:23.341Z"
     },
     {
      "end_time": "2021-01-17T17:13:59.315Z",
      "execution_time": "147ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:13:59.168Z"
     },
     {
      "end_time": "2021-01-17T17:15:10.931Z",
      "execution_time": "149ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:15:10.782Z"
     },
     {
      "end_time": "2021-01-17T17:17:21.455Z",
      "execution_time": "161ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n        #self.row = sum(1 for l in open(infile))\n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:17:21.294Z"
     },
     {
      "end_time": "2021-01-17T17:19:17.615Z",
      "execution_time": "160ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n         \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            print(read)\n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:19:17.455Z"
     },
     {
      "end_time": "2021-01-17T17:20:46.568Z",
      "execution_time": "160ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n         \n        for line, read in infile.itertuples(index=True, name=None):\n\n            print('hello')\n            \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            print(read)\n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:20:46.408Z"
     },
     {
      "end_time": "2021-01-17T17:22:59.275Z",
      "execution_time": "159ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                \n            print(read)\n                \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:22:59.116Z"
     },
     {
      "end_time": "2021-01-17T17:24:01.067Z",
      "execution_time": "146ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T17:24:00.921Z"
     },
     {
      "end_time": "2021-01-17T18:21:53.884Z",
      "execution_time": "148ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T18:21:53.736Z"
     },
     {
      "end_time": "2021-01-17T18:36:37.171Z",
      "execution_time": "148ms",
      "outputs": [],
      "source": "class vectorizer(object):\n    \n    #def __init__(self, procs = 8, k = 7, convert = 0, row = 0, index = [], exist = co.defaultdict(int)):\n    def __init__(self, k = 7, convert = 0):\n    \n        self.k = k\n        self.convert = convert\n        self.exist = co.defaultdict(int) \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.row = 0\n        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n        self.amino = co.defaultdict(str, {\n            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n        })\n                \n    def translate(self, read):\n    \n        chain = ''\n\n        for i in range(len(read) - 2):\n            trip = read[i:i+3]\n            chain += self.amino[trip]\n\n        return(chain)\n    \n    \n    def adjust_to_data(self, infile):\n    \n        self.row = infile.shape[0]\n            \n        for line, read in infile.itertuples(index=True, name=None):\n\n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    self.exist[kmer] = 0\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        self.exist[kmer] = 0\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            self.exist[kmer] = 0\n            \n        self.keys = list(self.exist.keys())\n        self.col = len(self.keys)\n        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n        \n        del seq\n    \n    def calculate_frequence(self, infile):\n        \n        #data = pd.read_csv(infile, chunksize = 10000, sep = ',', na_filter = False, header = None)\n        #for chunk in data:\n            #for line, info, read in chunk.itertuples(index=True, name=None):\n        \n        for line, read in infile.itertuples(index=True, name=None): \n                 \n            if self.convert == 1:\n                #seq = self.translate(re.sub('[^ACGT]+', '', read))\n                seq = self.translate(read)\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                for i in range(num):\n                    kmer = seq[i:i+self.k]\n                    counts[kmer] += 1\n\n            else:\n                #seq = re.sub('[^ACGT]+', '', read)\n                seq = read\n                del read\n\n                counts = self.exist.copy()\n                num = len(seq) - self.k + 1\n\n                if re.match('^[ACGT]*$', seq): \n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        counts[kmer] += 1\n                else:\n                    for i in range(num):\n                        kmer = seq[i:i+self.k]\n                        if re.match('^[ACGT]*$', kmer): \n                            counts[kmer] += 1\n\n            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n\n            self.matrix[line] = vector\n            \n            counts.clear()\n            del vector\n            del seq\n            del counts\n    \n    \n    def get_keys(self):\n        \n        return(self.keys)\n    \n    \n    def get_matrix(self):\n        \n        return(self.matrix)",
      "start_time": "2021-01-17T18:36:37.023Z"
     }
    ]
   },
   "outputs": [],
   "source": [
    "class vectorizer(object):\n",
    "    \n",
    "    def __init__(self, k = 7, convert = 0):\n",
    "    \n",
    "        self.k = k\n",
    "        self.convert = convert\n",
    "        self.exist = co.defaultdict(int) \n",
    "        self.keys = list(self.exist.keys())\n",
    "        self.col = len(self.keys)\n",
    "        self.row = 0\n",
    "        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n",
    "        self.amino = co.defaultdict(str, {\n",
    "            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n",
    "            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n",
    "            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n",
    "            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n",
    "            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n",
    "            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n",
    "            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n",
    "            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n",
    "            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n",
    "        })\n",
    "                \n",
    "    def translate(self, read):\n",
    "    \n",
    "        chain = ''\n",
    "\n",
    "        for i in range(len(read) - 2):\n",
    "            trip = read[i:i+3]\n",
    "            chain += self.amino[trip]\n",
    "\n",
    "        return(chain)\n",
    "    \n",
    "    \n",
    "    def adjust_to_data(self, infile):\n",
    "    \n",
    "        self.row = infile.shape[0]\n",
    "            \n",
    "        for line, read in infile.itertuples(index=True, name=None):\n",
    "\n",
    "            if self.convert == 1:\n",
    "                seq = self.translate(read)\n",
    "                del read\n",
    "\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                for i in range(num):\n",
    "                    kmer = seq[i:i+self.k]\n",
    "                    self.exist[kmer] = 0\n",
    "\n",
    "            else:\n",
    "                seq = read\n",
    "                del read\n",
    "\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                if re.match('^[ACGT]*$', seq): \n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        self.exist[kmer] = 0\n",
    "                else:\n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        if re.match('^[ACGT]*$', kmer): \n",
    "                            self.exist[kmer] = 0\n",
    "            \n",
    "        self.keys = list(self.exist.keys())\n",
    "        self.col = len(self.keys)\n",
    "        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n",
    "        \n",
    "        del seq\n",
    "    \n",
    "    def calculate_frequence(self, infile):\n",
    "        \n",
    "        for line, read in infile.itertuples(index=True, name=None): \n",
    "                 \n",
    "            if self.convert == 1:\n",
    "                seq = self.translate(read)\n",
    "                del read\n",
    "\n",
    "                counts = self.exist.copy()\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                for i in range(num):\n",
    "                    kmer = seq[i:i+self.k]\n",
    "                    counts[kmer] += 1\n",
    "\n",
    "            else:\n",
    "                seq = read\n",
    "                del read\n",
    "\n",
    "                counts = self.exist.copy()\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                if re.match('^[ACGT]*$', seq): \n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        counts[kmer] += 1\n",
    "                else:\n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        if re.match('^[ACGT]*$', kmer): \n",
    "                            counts[kmer] += 1\n",
    "\n",
    "            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n",
    "\n",
    "            self.matrix[line] = vector\n",
    "            \n",
    "            counts.clear()\n",
    "            del vector\n",
    "            del seq\n",
    "            del counts\n",
    "    \n",
    "    \n",
    "    def get_keys(self):\n",
    "        \n",
    "        return(self.keys)\n",
    "    \n",
    "    \n",
    "    def get_matrix(self):\n",
    "        \n",
    "        return(self.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-17T18:36:38.700708Z",
     "start_time": "2021-01-17T18:36:38.542478Z"
    },
    "provenance": [
     {
      "end_time": "Unknown",
      "execution_time": "Unknown",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A.csv'   \n    setfile = 'Input/settings.csv'\n    worldfile = 'Input/cities.csv'\n    outpath = 'Output/'\n    #outfile = 'output.csv'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    print(\"Finished.\")\n\n    start_table = time.perf_counter()\n\n    print(\"Creating SQL tables.\", end = ' ')\n\n    tables = extractor()\n    tables.fill_dicts(worldfile)\n    tables.input_sequences(infile, 8)\n    \n    intab = tables.get_dataframe([0, 5, 13], ['accession', 'subtype', 'genome'], ['accession'])\n    intab.reset_index(level=['accession'], inplace=True)\n    genomes = tables.get_dataframe([0, 13], ['accession', 'genome'], ['accession'])\n    header = tables.get_dataframe([0, 1, 2], ['accession', 'strain', 'segment'], ['accession'])\n    strains = tables.get_dataframe([1, 6, 7, 8, 9, 10, 11], ['strain', 'species', 'city', 'subcountry', 'country', 'year', 'host'], ['strain'])\n    \n    print(\"Finished.\")\n    \n    stop_table = time.perf_counter()\n    \n    print(f\"Table creation done in {stop_table - start_table:0.4f} seconds.\")\n    \n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(intab)\n    freq_nt.calculate_frequence(intab)\n\n    matrix_nt = freq_nt.get_matrix()\n    index_nt = freq_nt.get_index()   \n    subtype_nt = freq_nt.get_subtype()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(intab)\n    freq_aa.calculate_frequence(intab)\n\n    matrix_aa = freq_aa.get_matrix()\n    index_aa = freq_aa.get_index()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix_aa_ind = pd.DataFrame(matrix_aa_red, index = index_aa)\n    matrix_nt_ind = pd.DataFrame(matrix_nt_red, index = index_nt)\n\n    matrix = pd.concat([matrix_nt_ind, matrix_aa_ind], axis=1, copy = False, ignore_index = True) #falsches Ergebnis? checken ob ignore_index = Fehler\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), index = index_nt, columns = ['cluster', 'centroid'])\n    subtype = pd.DataFrame(subtype_nt, index = index_nt, columns = ['subtype'])\n    clusters = pd.concat([blank, subtype], axis=1, copy = False)\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    #clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    informations = pd.concat([header, clusters], axis=1, copy = False)\n    informations.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'informations.csv', index_label='accession', index=True, header=True, sep=',')\n    strains.to_csv(outpath + 'strains.csv', index_label='strain', index=True, header=True, sep=',')\n    genomes.to_csv(outpath + 'genomes.csv', index_label='accession', index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(f\"Overall running time {(stop_clust - start_clust)+(stop_table - start_table):0.4f} seconds.\")",
      "start_time": "Unknown"
     },
     {
      "end_time": "2021-01-15T08:57:20.373Z",
      "execution_time": "218ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    worldfile = 'Input/cities.csv'\n    outpath = 'Output/'\n    #outfile = 'output.csv'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    print(\"Finished.\")\n\n    start_table = time.perf_counter()\n\n    print(\"Creating SQL tables.\", end = ' ')\n\n    tables = extractor()\n    tables.fill_dicts(worldfile)\n    tables.input_sequences(infile, 8)\n    \n    intab = tables.get_dataframe([0, 5, 13], ['accession', 'subtype', 'genome'], ['accession'])\n    intab.reset_index(level=['accession'], inplace=True)\n    genomes = tables.get_dataframe([0, 13], ['accession', 'genome'], ['accession'])\n    header = tables.get_dataframe([0, 1, 2], ['accession', 'strain', 'segment'], ['accession'])\n    strains = tables.get_dataframe([1, 6, 7, 8, 9, 10, 11], ['strain', 'species', 'city', 'subcountry', 'country', 'year', 'host'], ['strain'])\n    \n    print(\"Finished.\")\n    \n    stop_table = time.perf_counter()\n    \n    print(f\"Table creation done in {stop_table - start_table:0.4f} seconds.\")\n    \n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(intab)\n    freq_nt.calculate_frequence(intab)\n\n    matrix_nt = freq_nt.get_matrix()\n    index_nt = freq_nt.get_index()   \n    subtype_nt = freq_nt.get_subtype()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(intab)\n    freq_aa.calculate_frequence(intab)\n\n    matrix_aa = freq_aa.get_matrix()\n    index_aa = freq_aa.get_index()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix_aa_ind = pd.DataFrame(matrix_aa_red, index = index_aa)\n    matrix_nt_ind = pd.DataFrame(matrix_nt_red, index = index_nt)\n\n    matrix = pd.concat([matrix_nt_ind, matrix_aa_ind], axis=1, copy = False, ignore_index = True) #falsches Ergebnis? checken ob ignore_index = Fehler\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), index = index_nt, columns = ['cluster', 'centroid'])\n    subtype = pd.DataFrame(subtype_nt, index = index_nt, columns = ['subtype'])\n    clusters = pd.concat([blank, subtype], axis=1, copy = False)\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    #clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    informations = pd.concat([header, clusters], axis=1, copy = False)\n    informations.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'informations.csv', index_label='accession', index=True, header=True, sep=',')\n    strains.to_csv(outpath + 'strains.csv', index_label='strain', index=True, header=True, sep=',')\n    genomes.to_csv(outpath + 'genomes.csv', index_label='accession', index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(f\"Overall running time {(stop_clust - start_clust)+(stop_table - start_table):0.4f} seconds.\")",
      "start_time": "2021-01-15T08:57:20.155Z"
     },
     {
      "end_time": "2021-01-15T09:00:24.040Z",
      "execution_time": "218ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A_HA.csv'   \n    setfile = 'Input/settings.csv'\n    worldfile = 'Input/cities.csv'\n    outpath = 'Output/'\n    #outfile = 'output.csv'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    print(\"Finished.\")\n\n    start_table = time.perf_counter()\n\n    print(\"Creating SQL tables.\", end = ' ')\n\n    tables = extractor()\n    tables.fill_dicts(worldfile)\n    tables.input_sequences(infile, 8)\n    \n    intab = tables.get_dataframe([0, 5, 13], ['accession', 'subtype', 'genome'], ['accession'])\n    intab.reset_index(level=['accession'], inplace=True)\n    genomes = tables.get_dataframe([0, 13], ['accession', 'genome'], ['accession'])\n    header = tables.get_dataframe([0, 1, 2], ['accession', 'strain', 'segment'], ['accession'])\n    strains = tables.get_dataframe([1, 6, 7, 8, 9, 10, 11], ['strain', 'species', 'city', 'subcountry', 'country', 'year', 'host'], ['strain'])\n    \n    print(\"Finished.\")\n    \n    stop_table = time.perf_counter()\n    \n    print(f\"Table creation done in {stop_table - start_table:0.4f} seconds.\")\n    \n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(intab)\n    freq_nt.calculate_frequence(intab)\n\n    matrix_nt = freq_nt.get_matrix()\n    index_nt = freq_nt.get_index()   \n    subtype_nt = freq_nt.get_subtype()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(intab)\n    freq_aa.calculate_frequence(intab)\n\n    matrix_aa = freq_aa.get_matrix()\n    index_aa = freq_aa.get_index()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix_aa_ind = pd.DataFrame(matrix_aa_red, index = index_aa)\n    matrix_nt_ind = pd.DataFrame(matrix_nt_red, index = index_nt)\n\n    matrix = pd.concat([matrix_nt_ind, matrix_aa_ind], axis=1, copy = False, ignore_index = True) #falsches Ergebnis? checken ob ignore_index = Fehler\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), index = index_nt, columns = ['cluster', 'centroid'])\n    subtype = pd.DataFrame(subtype_nt, index = index_nt, columns = ['subtype'])\n    clusters = pd.concat([blank, subtype], axis=1, copy = False)\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    #clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    informations = pd.concat([header, clusters], axis=1, copy = False)\n    informations.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'informations.csv', index_label='accession', index=True, header=True, sep=',')\n    strains.to_csv(outpath + 'strains.csv', index_label='strain', index=True, header=True, sep=',')\n    genomes.to_csv(outpath + 'genomes.csv', index_label='accession', index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")\n    print(f\"Overall running time {(stop_clust - start_clust)+(stop_table - start_table):0.4f} seconds.\")",
      "start_time": "2021-01-15T09:00:23.822Z"
     },
     {
      "end_time": "2021-01-17T17:14:00.204Z",
      "execution_time": "164ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv('Input/A.csv', sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:14:00.040Z"
     },
     {
      "end_time": "2021-01-17T17:15:11.679Z",
      "execution_time": "151ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv('Input/A.csv', sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:15:11.528Z"
     },
     {
      "end_time": "2021-01-17T17:17:22.333Z",
      "execution_time": "155ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Finished.\")\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:17:22.178Z"
     },
     {
      "end_time": "2021-01-17T17:19:18.008Z",
      "execution_time": "153ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Finished.\")\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:19:17.855Z"
     },
     {
      "end_time": "2021-01-17T17:20:46.978Z",
      "execution_time": "159ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Finished.\")\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:20:46.819Z"
     },
     {
      "end_time": "2021-01-17T17:21:26.183Z",
      "execution_time": "154ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n    \n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n    \n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 4').reset_index()\n    \n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Finished.\")\n    \n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    print(sequence)\n    \n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n    \n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n    \n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n                \n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n    \n    print(\"Finished.\")\n    \n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:21:26.029Z"
     },
     {
      "end_time": "2021-01-17T17:21:48.669Z",
      "execution_time": "295ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. Empty DataFrame\nColumns: [genome]\nIndex: []\n"
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-13-1abdee311c75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-8-4ba4d11f7e41>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = 'Input/B_HA.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nparameter = settings.loc[4].to_list()\n\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsubset = upload.query('segment == 4').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"Finished.\")\n\nstart_clust = time.perf_counter()\n\nprint(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\nprint(sequence)\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\nnum = clusters['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[clusters.cluster == i]\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n\ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\nclusters.update(centroids)\nclusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nprint(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"Mean of inner cluster distance mean {overall_mean/num}\")\nprint(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:21:48.374Z"
     },
     {
      "end_time": "2021-01-17T17:24:02.194Z",
      "execution_time": "163ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/B_HA.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 1').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"Finished.\")\n\n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    print(sequence)\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outfile, index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:24:02.031Z"
     },
     {
      "end_time": "2021-01-17T17:31:35.044Z",
      "execution_time": "2m 22s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. "
       },
       {
        "ename": "NameError",
        "evalue": "name 'ssd' is not defined",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-6-9e87fee015d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0minner_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0maccessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mNameError\u001b[0m: name 'ssd' is not defined"
        ]
       }
      ],
      "source": "#def main():\n\nprint(\"Read input and settings file.\", end = ' ')\n\ninfile = 'Input/B_HA.csv'   \nsetfile = 'Input/settings.csv'\noutpath = 'Output/'\n\nsettings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\nparameter = settings.loc[4].to_list()\n\nupload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\nupload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\nsubset = upload.query('segment == 1').reset_index()\n\nsequence = subset[['genome']].copy()\naccession = subset[['accession']].copy()\nsubtype = subset[['subtype']].copy()\n\nprint(\"Finished.\")\n\nstart_clust = time.perf_counter()\n\nprint(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\nfreq_nt = vectorizer(k = parameter[0].item(), convert = 0)\nfreq_nt.adjust_to_data(sequence)\nfreq_nt.calculate_frequence(sequence)\n\nmatrix_nt = freq_nt.get_matrix()\nkeys_nt = freq_nt.get_keys()\n\ndel freq_nt\n\nprint(\"Finished.\")\n\nprint(\"Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_nt_red = umap.UMAP(\n    n_neighbors = parameter[1].item(),\n    min_dist = parameter[2].item(),\n    n_components = parameter[3].item(),\n    random_state = parameter[4].item(),\n    metric = parameter[5],\n).fit_transform(matrix_nt)\n\ndel matrix_nt\n\nprint(\"Finished.\")\n\nprint(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\nfreq_aa = vectorizer(k = parameter[6].item(), convert = 1)\nfreq_aa.adjust_to_data(sequence)\nfreq_aa.calculate_frequence(sequence)\n\nmatrix_aa = freq_aa.get_matrix()\nkeys_aa = freq_aa.get_keys()\n\ndel freq_aa\n\nprint(\"Finished.\")\n\nprint(\"Running UMAP for dimension reduction.\", end = ' ')\n\nmatrix_aa_red = umap.UMAP(\n    n_neighbors = parameter[7].item(),\n    min_dist = parameter[8].item(),\n    n_components = parameter[9].item(),\n    random_state = parameter[10].item(),\n    metric = parameter[11],\n).fit_transform(matrix_aa)\n\ndel matrix_aa\n\nprint(\"Finished.\")\n\nmatrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\nprint(\"Running HDBscan for clustering.\", end = ' ')\n\nmatrix_clust = hdbscan.HDBSCAN(\n    min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n    min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n    cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n    alpha = parameter[15].item(), #don't mess with this\n).fit(matrix)\n\nprint(\"Finished.\")\n\nprint(\"Centroid extraction.\", end = ' ')\n\nclusterlabel = matrix_clust.labels_\n\nblank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\nclusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\nnum = clusters['cluster'].max()+1\nvalues = ['true']*num\naccessions = []\nexclude = []\ninclude = []\noverall_mean=0\nsubs = co.defaultdict(list)\n\nfor i in range(num):\n\n    query = clusters[clusters.cluster == i]\n    match = query.index.values.tolist()\n    sub = matrix.filter(items = match, axis=0)\n    dist = ssd.cdist(sub, sub, metric = parameter[16])\n    inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n    accessions.append(inner_mean.idxmin())\n    overall_mean = overall_mean + inner_mean.mean()\n\n    for sub in query['subtype'].tolist():\n        if re.match('^[H][0-9]+N[0-9]+$', sub): \n            subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n            subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n        else:\n            subs['X'].append('X0')\n            subs['X'].append('X0')\n\n    if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n        exclude.append(2)\n        if 'X' not in subs.keys():\n            include.append(2)\n    elif len(set(subs['H'])) == 1:\n        exclude.append(1)\n        if 'X' not in subs.keys():\n            include.append(1)\n    elif len(set(subs['N'])) == 1:\n        exclude.append(0)\n        if 'X' not in subs.keys():\n            include.append(0)\n\n    subs.clear()\n\ncentroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\nclusters.update(centroids)\nclusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\nprint(\"Finished.\")\n\nstop_clust = time.perf_counter()\nprint(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\ndiagnostic = co.Counter(clusterlabel)\nprint(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\nprint(f\"Mean of inner cluster distance mean {overall_mean/num}\")\nprint(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\nprint(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:29:13.330Z"
     },
     {
      "end_time": "2021-01-17T17:51:51.327Z",
      "execution_time": "153ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    parameter = settings.loc[4].to_list()\n\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == 1').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n\n    print(\"Finished.\")\n\n    start_clust = time.perf_counter()\n\n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters[clusters.cluster == i]\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T17:51:51.174Z"
     },
     {
      "end_time": "2021-01-17T18:21:54.850Z",
      "execution_time": "236ms",
      "outputs": [
       {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (<ipython-input-20-40480b8c0d08>, line 17)",
        "output_type": "error",
        "traceback": [
         "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-40480b8c0d08>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    start_clust = time.perf_counter()\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
        ]
       }
      ],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    memory = memory_usage(f)\n    infile = 'Input/A_HA_sample.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    print(f\"Starting calculations for segment {segment}.\"\n    \n    start_clust = time.perf_counter()\n    \n    segment = 4\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    print(\"Maximum memory used: %s\" % max(mem_usage))\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:21:54.614Z"
     },
     {
      "end_time": "2021-01-17T18:22:09.168Z",
      "execution_time": "171ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    memory = memory_usage(f)\n    infile = 'Input/A_HA_sample.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    print(f\"Starting calculations for segment {segment}.\")\n    \n    start_clust = time.perf_counter()\n    \n    segment = 4\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    print(\"Maximum memory used: %s\" % max(mem_usage))\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:22:08.997Z"
     },
     {
      "end_time": "2021-01-17T18:23:59.617Z",
      "execution_time": "166ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A_HA_sample.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    print(f\"Starting calculations for segment {segment}.\")\n    \n    start_clust = time.perf_counter()\n    \n    segment = 4\n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:23:59.451Z"
     },
     {
      "end_time": "2021-01-17T18:24:19.343Z",
      "execution_time": "162ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A_HA_sample.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    segment = 4\n    print(f\"Starting calculations for segment {segment}.\")\n    \n    start_clust = time.perf_counter()\n    \n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:24:19.181Z"
     },
     {
      "end_time": "2021-01-17T18:36:38.700Z",
      "execution_time": "158ms",
      "outputs": [],
      "source": "def main():\n\n    print(\"Read input and settings file.\", end = ' ')\n\n    infile = 'Input/A.csv'   \n    setfile = 'Input/settings.csv'\n    outpath = 'Output/'\n\n    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n    \n    print(\"Finished.\")\n    \n    segment = 4\n    print(f\"Starting calculations for segment {segment}:\")\n    \n    start_clust = time.perf_counter()\n    \n    parameter = settings.loc[segment].to_list()\n    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n    subset = upload.query('segment == @segment').reset_index()\n\n    sequence = subset[['genome']].copy()\n    accession = subset[['accession']].copy()\n    subtype = subset[['subtype']].copy()\n    \n    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n\n    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n    freq_nt.adjust_to_data(sequence)\n    freq_nt.calculate_frequence(sequence)\n\n    matrix_nt = freq_nt.get_matrix()\n    keys_nt = freq_nt.get_keys()\n\n    del freq_nt\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_nt_red = umap.UMAP(\n        n_neighbors = parameter[1].item(),\n        min_dist = parameter[2].item(),\n        n_components = parameter[3].item(),\n        random_state = parameter[4].item(),\n        metric = parameter[5],\n    ).fit_transform(matrix_nt)\n\n    del matrix_nt\n\n    print(\"Finished.\")\n\n    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n\n    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n    freq_aa.adjust_to_data(sequence)\n    freq_aa.calculate_frequence(sequence)\n\n    matrix_aa = freq_aa.get_matrix()\n    keys_aa = freq_aa.get_keys()\n\n    del freq_aa\n\n    print(\"Finished.\")\n\n    print(\"Running UMAP for dimension reduction.\", end = ' ')\n\n    matrix_aa_red = umap.UMAP(\n        n_neighbors = parameter[7].item(),\n        min_dist = parameter[8].item(),\n        n_components = parameter[9].item(),\n        random_state = parameter[10].item(),\n        metric = parameter[11],\n    ).fit_transform(matrix_aa)\n\n    del matrix_aa\n\n    print(\"Finished.\")\n\n    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n\n    print(\"Running HDBscan for clustering.\", end = ' ')\n\n    matrix_clust = hdbscan.HDBSCAN(\n        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n        alpha = parameter[15].item(), #don't mess with this\n    ).fit(matrix)\n\n    print(\"Finished.\")\n\n    print(\"Centroid extraction.\", end = ' ')\n\n    clusterlabel = matrix_clust.labels_\n\n    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n\n    num = clusters['cluster'].max()+1\n    values = ['true']*num\n    accessions = []\n    exclude = []\n    include = []\n    overall_mean=0\n    subs = co.defaultdict(list)\n\n    for i in range(num):\n\n        query = clusters.query('cluster == @i')\n        match = query.index.values.tolist()\n        sub = matrix.filter(items = match, axis=0)\n        dist = ssd.cdist(sub, sub, metric = parameter[16])\n        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n        accessions.append(inner_mean.idxmin())\n        overall_mean = overall_mean + inner_mean.mean()\n\n        for sub in query['subtype'].tolist():\n            if re.match('^[H][0-9]+N[0-9]+$', sub): \n                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n            else:\n                subs['X'].append('X0')\n                subs['X'].append('X0')\n\n        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n            exclude.append(2)\n            if 'X' not in subs.keys():\n                include.append(2)\n        elif len(set(subs['H'])) == 1:\n            exclude.append(1)\n            if 'X' not in subs.keys():\n                include.append(1)\n        elif len(set(subs['N'])) == 1:\n            exclude.append(0)\n            if 'X' not in subs.keys():\n                include.append(0)\n\n        subs.clear()\n\n    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n\n    clusters.update(centroids)\n    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n\n    print(\"Finished.\")\n\n    stop_clust = time.perf_counter()\n    print(f\"Clustering done in {stop_clust - start_clust:0.4f} seconds.\")\n    diagnostic = co.Counter(clusterlabel)\n    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n    print(f\"Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")",
      "start_time": "2021-01-17T18:36:38.542Z"
     }
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    print(\"Read input and settings file.\", end = ' ')\n",
    "\n",
    "    infile = 'Input/A.csv'   \n",
    "    setfile = 'Input/settings.csv'\n",
    "    outpath = 'Output/'\n",
    "\n",
    "    settings = pd.read_csv(setfile, sep = ',', na_filter = False, index_col = 0)\n",
    "    upload = pd.read_csv(infile, sep = ',', na_filter = False, header = None)\n",
    "    \n",
    "    print(\"Finished.\")\n",
    "    \n",
    "    segment = 4\n",
    "    print(f\"Starting calculations for segment {segment}:\")\n",
    "    \n",
    "    start_clust = time.perf_counter()\n",
    "    \n",
    "    parameter = settings.loc[segment].to_list()\n",
    "    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n",
    "    subset = upload.query('segment == @segment').reset_index()\n",
    "\n",
    "    sequence = subset[['genome']].copy()\n",
    "    accession = subset[['accession']].copy()\n",
    "    subtype = subset[['subtype']].copy()\n",
    "    \n",
    "    print(\"Nucleotide k-mer frequency calculation.\", end = ' ')\n",
    "\n",
    "    freq_nt = vectorizer(k = parameter[0].item(), convert = 0)\n",
    "    freq_nt.adjust_to_data(sequence)\n",
    "    freq_nt.calculate_frequence(sequence)\n",
    "\n",
    "    matrix_nt = freq_nt.get_matrix()\n",
    "    keys_nt = freq_nt.get_keys()\n",
    "\n",
    "    del freq_nt\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    print(\"Running UMAP for dimension reduction.\", end = ' ')\n",
    "\n",
    "    matrix_nt_red = umap.UMAP(\n",
    "        n_neighbors = parameter[1].item(),\n",
    "        min_dist = parameter[2].item(),\n",
    "        n_components = parameter[3].item(),\n",
    "        random_state = parameter[4].item(),\n",
    "        metric = parameter[5],\n",
    "    ).fit_transform(matrix_nt)\n",
    "\n",
    "    del matrix_nt\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    print(\"Aminoacid k-mer frequency calculation.\", end = ' ')\n",
    "\n",
    "    freq_aa = vectorizer(k = parameter[6].item(), convert = 1)\n",
    "    freq_aa.adjust_to_data(sequence)\n",
    "    freq_aa.calculate_frequence(sequence)\n",
    "\n",
    "    matrix_aa = freq_aa.get_matrix()\n",
    "    keys_aa = freq_aa.get_keys()\n",
    "\n",
    "    del freq_aa\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    print(\"Running UMAP for dimension reduction.\", end = ' ')\n",
    "\n",
    "    matrix_aa_red = umap.UMAP(\n",
    "        n_neighbors = parameter[7].item(),\n",
    "        min_dist = parameter[8].item(),\n",
    "        n_components = parameter[9].item(),\n",
    "        random_state = parameter[10].item(),\n",
    "        metric = parameter[11],\n",
    "    ).fit_transform(matrix_aa)\n",
    "\n",
    "    del matrix_aa\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    matrix = pd.concat([accession, pd.DataFrame(matrix_aa_red), pd.DataFrame(matrix_nt_red)], axis=1, copy = False, ignore_index = False).set_index('accession')\n",
    "\n",
    "    print(\"Running HDBscan for clustering.\", end = ' ')\n",
    "\n",
    "    matrix_clust = hdbscan.HDBSCAN(\n",
    "        min_samples = parameter[12].item(), #larger the value the more conservative the clustering (more points will be declared as noise)\n",
    "        min_cluster_size = parameter[13].item(), #minimum size that can become a cluster\n",
    "        cluster_selection_epsilon = parameter[14].item(), #don't seperate clusters with a distance less than value\n",
    "        alpha = parameter[15].item(), #don't mess with this\n",
    "    ).fit(matrix)\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    print(\"Centroid extraction.\", end = ' ')\n",
    "\n",
    "    clusterlabel = matrix_clust.labels_\n",
    "\n",
    "    blank = pd.DataFrame(zip(clusterlabel, ['false'] * len(clusterlabel)), columns = ['cluster', 'centroid'])\n",
    "    clusters = pd.concat([blank, subtype, accession], axis=1, copy = False).set_index('accession')\n",
    "\n",
    "    num = clusters['cluster'].max()+1\n",
    "    values = ['true']*num\n",
    "    accessions = []\n",
    "    exclude = []\n",
    "    include = []\n",
    "    overall_mean=0\n",
    "    subs = co.defaultdict(list)\n",
    "\n",
    "    for i in range(num):\n",
    "\n",
    "        query = clusters.query('cluster == @i')\n",
    "        match = query.index.values.tolist()\n",
    "        sub = matrix.filter(items = match, axis=0)\n",
    "        dist = ssd.cdist(sub, sub, metric = parameter[16])\n",
    "        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n",
    "        accessions.append(inner_mean.idxmin())\n",
    "        overall_mean = overall_mean + inner_mean.mean()\n",
    "\n",
    "        for sub in query['subtype'].tolist():\n",
    "            if re.match('^[H][0-9]+N[0-9]+$', sub): \n",
    "                subs['H'].append(re.search('[H][0-9]+', sub).group(0))\n",
    "                subs['N'].append(re.search('[N][0-9]+', sub).group(0))\n",
    "            else:\n",
    "                subs['X'].append('X0')\n",
    "                subs['X'].append('X0')\n",
    "\n",
    "        if len(set(subs['H'])) == 1 and len(set(subs['N'])) == 1:\n",
    "            exclude.append(2)\n",
    "            if 'X' not in subs.keys():\n",
    "                include.append(2)\n",
    "        elif len(set(subs['H'])) == 1:\n",
    "            exclude.append(1)\n",
    "            if 'X' not in subs.keys():\n",
    "                include.append(1)\n",
    "        elif len(set(subs['N'])) == 1:\n",
    "            exclude.append(0)\n",
    "            if 'X' not in subs.keys():\n",
    "                include.append(0)\n",
    "\n",
    "        subs.clear()\n",
    "\n",
    "    centroids = pd.DataFrame(values, columns=['centroid'], index = accessions)\n",
    "\n",
    "    clusters.update(centroids)\n",
    "    clusters.sort_values(by=['cluster', 'subtype']).to_csv(outpath + 'cluster.csv', index=True, header=True, sep=',')\n",
    "\n",
    "    print(\"Finished.\")\n",
    "\n",
    "    stop_clust = time.perf_counter()\n",
    "    print(f\"Clustering done in {stop_clust - start_clust:0.2f} seconds.\")\n",
    "    diagnostic = co.Counter(clusterlabel)\n",
    "    print(f\"{str(len(clusterlabel))} sequences, {str(diagnostic[-1])} unclustered, {str(len(set(diagnostic)))} cluster.\")\n",
    "    print(f\"Mean of inner cluster distance mean {overall_mean/num:0.10f}\")\n",
    "    print(f\"{exclude.count(0) + exclude.count(2)}({include.count(0) + include.count(2)}) clusters containing matching NA types.\")\n",
    "    print(f\"{exclude.count(1) + exclude.count(2)}({include.count(1) + include.count(2)}) clusters containing matching HA types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecutionTime": {
     "end_time": "2021-01-17T18:56:42.288657Z",
     "start_time": "2021-01-17T18:36:39.564702Z"
    },
    "provenance": [
     {
      "end_time": "Unknown",
      "execution_time": "Unknown",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nCreating SQL tables. "
       },
       {
        "ename": "IndexError",
        "evalue": "list index out of range",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
         "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-2-6bbd2fe1518b>\", line 72, in process_rows\n    organism = head[4]\nIndexError: list index out of range\n\"\"\"",
         "\nThe above exception was the direct cause of the following exception:\n",
         "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-7-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-6-653664b12606>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworldfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mintab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accession'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'genome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accession'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-2-6bbd2fe1518b>\u001b[0m in \u001b[0;36minput_sequences\u001b[0;34m(self, infile, procs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mIndexError\u001b[0m: list index out of range"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "Unknown"
     },
     {
      "end_time": "2021-01-17T17:17:23.299Z",
      "execution_time": "250ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. "
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-4-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-3-91f2f3134950>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-2-a3ce8d3a10f4>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:17:23.049Z"
     },
     {
      "end_time": "2021-01-17T17:19:18.598Z",
      "execution_time": "144ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. "
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-7-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-6-91f2f3134950>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-5-955e0dcf5285>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:19:18.454Z"
     },
     {
      "end_time": "2021-01-17T17:20:47.706Z",
      "execution_time": "143ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. "
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-10-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-9-91f2f3134950>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-8-4ba4d11f7e41>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:20:47.563Z"
     },
     {
      "end_time": "2021-01-17T17:21:26.655Z",
      "execution_time": "153ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. Empty DataFrame\nColumns: [genome]\nIndex: []\n"
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'seq' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-12-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-11-57f1a7fa5f04>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfreq_nt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mfreq_nt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-8-4ba4d11f7e41>\u001b[0m in \u001b[0;36madjust_to_data\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'seq' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:21:26.502Z"
     },
     {
      "end_time": "2021-01-17T17:26:32.796Z",
      "execution_time": "2m 30s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation.                                                   genome\n0      ATGAATATAAATCCTTATTTTCTCTTCATAGATGTGCCCGTACAGG...\n1      ATGAATATAAATCCGTATTTTCTATTCATAGATGTACCTATACAGG...\n2      CTTTAAGATGAATATAAATCCTTATTTTCTCTTCATAGATGTGCCC...\n3      CTTTAAGATGAATATAAATCCGTATTTTCTATTCATAGATGTACCT...\n4      CTTTAAGATGAATATAAATCCTTATTTTCTCTTCATAGATGTGCCC...\n...                                                  ...\n10376  CTTTAAGATGAATATAAATCCTTATTTTCTCTTCATAGATGTGCCC...\n10377  GATGAATATAAATCCTTATTTTCTCTTCATAGATGTACCCATACAG...\n10378  GATGAATATAAATCCTTATTTTCTCTTCATAGATGTACCCATACAG...\n10379  ATGAATATAAATCCTTATTTTCTCTTCATAGATGTACCCATACAGG...\n10380  CTTTAAGATGAATATAAATCCTTATTTTCTCTTCATAGATGTACCC...\n\n[10381 rows x 1 columns]\nFinished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. "
       },
       {
        "ename": "NameError",
        "evalue": "name 'ssd' is not defined",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-4-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-3-a2ac216f60ef>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0minner_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0maccessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mNameError\u001b[0m: name 'ssd' is not defined"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:24:02.779Z"
     },
     {
      "end_time": "2021-01-17T18:11:28.662Z",
      "execution_time": "19m 32s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nNucleotide k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. Finished.\nClustering done in 1168.1290 seconds.\n55999 sequences, 275 unclustered, 1300 cluster.\nMean of inner cluster distance mean 2.7103107300434113e-05\n848(795) clusters containing matching NA types.\n871(808) clusters containing matching HA types.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T17:51:56.642Z"
     },
     {
      "end_time": "2021-01-17T18:22:09.675Z",
      "execution_time": "14ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. "
       },
       {
        "ename": "NameError",
        "evalue": "name 'f' is not defined",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-22-73bda965d2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
         "\u001b[0;32m<ipython-input-21-d3e207e7a989>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read input and settings file.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Input/A_HA_sample.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msetfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Input/settings.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    main()",
      "start_time": "2021-01-17T18:22:09.661Z"
     },
     {
      "end_time": "2021-01-17T18:24:00.728Z",
      "execution_time": "171ms",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\n"
       },
       {
        "ename": "UnboundLocalError",
        "evalue": "local variable 'segment' referenced before assignment",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
         "\u001b[0;32m<ipython-input-24-ec55abb246c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maximum memory used: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m/home/ahenoch/miniconda3/envs/masterthesis/lib/python3.6/site-packages/memory_profiler.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(proc, interval, timeout, timestamps, include_children, multiprocess, max_usage, retval, stream, backend, max_iterations)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Therefore, the whole process hangs indefinitely. Here, we are ensuring that the process gets killed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mreturned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# finish timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;32m<ipython-input-23-3b727354fed4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting calculations for segment {segment}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstart_clust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
         "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'segment' referenced before assignment"
        ]
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    memory = memory_usage(main)\n    #main()\n    print(\"Maximum memory used: %s\" % max(memory))",
      "start_time": "2021-01-17T18:24:00.557Z"
     },
     {
      "end_time": "2021-01-17T18:27:11.585Z",
      "execution_time": "2m 52s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 4.\nNucleotide k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. Finished.\nClustering done in 171.7212 seconds.\n10000 sequences, 0 unclustered, 174 cluster.\nMean of inner cluster distance mean 8.321637921018586e-05\n118(104) clusters containing matching NA types.\n171(121) clusters containing matching HA types.\nMaximum memory used: 3453.1796875\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(\"Maximum memory used: %s\" % max(memory))",
      "start_time": "2021-01-17T18:24:19.692Z"
     },
     {
      "end_time": "2021-01-17T18:56:42.288Z",
      "execution_time": "20m 3s",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Read input and settings file. Finished.\nStarting calculations for segment 4:\nNucleotide k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nAminoacid k-mer frequency calculation. Finished.\nRunning UMAP for dimension reduction. Finished.\nRunning HDBscan for clustering. Finished.\nCentroid extraction. Finished.\nClustering done in 1198.7109 seconds.\n56617 sequences, 36 unclustered, 1052 cluster.\nMean of inner cluster distance mean 0.0000974158\n779(692) clusters containing matching NA types.\n1048(787) clusters containing matching HA types.\nMaximum memory used: 16254.1406Mb.\n"
       }
      ],
      "source": "if __name__ == \"__main__\":\n\n    #main()\n    memory = memory_usage(main)\n    print(f\"Maximum memory used: {max(memory):0.4f}Mb.\")",
      "start_time": "2021-01-17T18:36:39.564Z"
     }
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read input and settings file. Finished.\n",
      "Starting calculations for segment 4:\n",
      "Nucleotide k-mer frequency calculation. Finished.\n",
      "Running UMAP for dimension reduction. Finished.\n",
      "Aminoacid k-mer frequency calculation. Finished.\n",
      "Running UMAP for dimension reduction. Finished.\n",
      "Running HDBscan for clustering. Finished.\n",
      "Centroid extraction. Finished.\n",
      "Clustering done in 1198.7109 seconds.\n",
      "56617 sequences, 36 unclustered, 1052 cluster.\n",
      "Mean of inner cluster distance mean 0.0000974158\n",
      "779(692) clusters containing matching NA types.\n",
      "1048(787) clusters containing matching HA types.\n",
      "Maximum memory used: 16254.1406Mb.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #main()\n",
    "    memory = memory_usage(main)\n",
    "    print(f\"Maximum memory used: {max(memory)/1000:0.2f} Gb.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
