{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hawaiian-pasta",
   "metadata": {},
   "source": [
    "# Masterthesis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-surveillance",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "| Name | Version | Build | Channel |\n",
    "| -------------------- | ------- | ------------------ | ----------- |\n",
    "| hdbscan | 0.8.26 | py38h0b5ebd8_3 | conda-forge |\n",
    "| jedi | 0.17.2 | py38h578d9bd_1 | conda-forge |\n",
    "| jupyter-resource-usage | 0.5.1 | pyhd8ed1ab_0 | conda-forge |\n",
    "| jupyterlab | 3.0.5 | pyhd8ed1ab_0 | conda-forge |\n",
    "| nodejs | 15.3.0 | h25f6087_0 | conda-forge |\n",
    "| pandas | 1.2.1 | py38h51da96c_0 | conda-forge |\n",
    "| progressbar2 | 3.53.1 | pyh9f0ad1d_0 | conda-forge |\n",
    "| umap-learn | 0.4.6 | py38h32f6830_0 | conda-forge |\n",
    "| seaborn | | | |\n",
    "| biopython | | | |\n",
    "| mafft | | | | \n",
    "\n",
    "- Alexander Henoch\n",
    "- conda install jedi=0.17.2 jupyterlab jupyter-resource-usage nodejs umap-learn=0.4.6 hdbscan=0.8.26 pandas progressbar2 seaborn joblib=0.17.0 biopython mafft\n",
    "- jupyter labextension install @jupyterlab/fasta-extension\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-class",
   "metadata": {},
   "source": [
    "## To-Do\n",
    "\n",
    "1. [X] Frequency extraction \n",
    "2. [ ] Clustering UMAP/HDB\n",
    "    1. [X] Parameter exploration\n",
    "    2. [X] Cluster number reduction\n",
    "    3. [ ] Centroid extraction\n",
    "    4. [ ] New Subtype Classification\n",
    "3. [X] Subtype interaction matrix\n",
    "    1. [X] Iteraction counting\n",
    "4. [ ] Alignment\n",
    "    1. [X] Mafft inclusion\n",
    "    2. \n",
    "5. [ ] Circle hunt\n",
    "6. [ ] Mutations\n",
    "7. [ ] Write Thesis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-british",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "1. Virus/Pandemia\n",
    "2. Influenza/Danger\n",
    "3. Segments/Subtypes\n",
    "4. Sequencing/NTs/AAs/Mutations\n",
    "5. Clustering/-methods/Hierarchical/HDB\n",
    "6. K-mer Frequencies/Dimensions/UMAP\n",
    "7. Linkage Trees/define k/Parameter\n",
    "8. ...\n",
    "9. biol. Subtypes/new Subtypes?\n",
    "10. Alignments/Structures\n",
    "11. Thesis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-jacket",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "1. \n",
    "2. UMAP/HDB/hybrid clustering\n",
    "3. Metrices/Scorings/DBCV/Epsilon/treecutting\n",
    "4. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-fluid",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "1. \n",
    "2. DBCV/Epsilon (Plot)\n",
    "3. cluster density (Plot)\n",
    "4. cluster + linkage tree(?)\n",
    "5. subtypes?\n",
    "6. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-taylor",
   "metadata": {},
   "source": [
    "### Package import\n",
    "\n",
    "- numpy, pandas: Dataframes\n",
    "- sys, re, csv, time: Python core functions\n",
    "- collections, itertools: Extended core functions\n",
    "- umap, hdbscan: Clustering\n",
    "- plotnine, matplotlib inline: Plotting \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "challenging-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import collections as co\n",
    "import itertools as it\n",
    "import umap\n",
    "import hdbscan\n",
    "import time \n",
    "import progressbar\n",
    "\n",
    "from Bio.Align.Applications import MafftCommandline\n",
    "from matplotlib import colors \n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import matplotlib.transforms as transforms\n",
    "import scipy.spatial.distance as ssd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-rings",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-darkness",
   "metadata": {},
   "source": [
    "#### K-mer Frequency calculation\n",
    "\n",
    "- my frequency class\n",
    "    - max possible RAM save\n",
    "    - creation of matrix beforehand (adjustment)\n",
    "        - runtime + / RAM -\n",
    "    - no concatenation /copy of data -> more RAM)\n",
    "        - filling existing matrix\n",
    "        - has to be in the exact right size!\n",
    "    - Ns frequency\n",
    "        - ...\n",
    "    - AAs with Ns\n",
    "        - ...\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legal-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "class frequency(object):\n",
    "    \n",
    "    def __init__(self, k = 7, convert = 0):\n",
    "    \n",
    "        self.k = k\n",
    "        self.convert = convert\n",
    "        self.exist = co.defaultdict(int) \n",
    "        self.keys = list(self.exist.keys())\n",
    "        self.col = len(self.keys)\n",
    "        self.row = 0\n",
    "        self.matrix = np.empty((self.row, self.col, ),dtype = \"float32\")\n",
    "        self.amino = co.defaultdict(str, {\n",
    "            'AAA':'K', 'AAC':'N', 'AAG':'K', 'AAT':'N',\n",
    "            'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "            'AGA':'R', 'AGC':'S', 'AGG':'R', 'AGT':'S',\n",
    "            'ATA':'I', 'ATC':'I', 'ATG':'M', 'ATT':'I',\n",
    "            'CAA':'Q', 'CAC':'H', 'CAG':'Q', 'CAT':'H',\n",
    "            'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "            'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "            'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "            'GAA':'E', 'GAC':'D', 'GAG':'E', 'GAT':'D',\n",
    "            'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "            'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "            'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',    \n",
    "            'TAA':'Y', 'TAC':'*', 'TAG':'*', 'TAT':'Y',\n",
    "            'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "            'TGA':'*', 'TGC':'C', 'TGG':'W', 'TGT':'C',\n",
    "            'TTA':'L', 'TTC':'F', 'TTG':'L', 'TTT':'F'\n",
    "        })\n",
    "                \n",
    "    def translate(self, read):\n",
    "    \n",
    "        chain = ''\n",
    "\n",
    "        for i in range(len(read) - 2):\n",
    "            trip = read[i:i+3]\n",
    "            chain += self.amino[trip]\n",
    "\n",
    "        return(chain)\n",
    "    \n",
    "    \n",
    "    def adjust_to_data(self, infile):\n",
    "    \n",
    "        self.row = infile.shape[0]\n",
    "            \n",
    "        for line, read in infile.itertuples(index=True, name=None):\n",
    "\n",
    "            if self.convert == 1:\n",
    "                seq = self.translate(read)\n",
    "                del read\n",
    "\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                for i in range(num):\n",
    "                    kmer = seq[i:i+self.k]\n",
    "                    self.exist[kmer] = 0\n",
    "\n",
    "            else:\n",
    "                seq = read\n",
    "                del read\n",
    "\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                if re.match('^[ACGT]*$', seq): \n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        self.exist[kmer] = 0\n",
    "                else:\n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        if re.match('^[ACGT]*$', kmer): \n",
    "                            self.exist[kmer] = 0\n",
    "            \n",
    "        self.keys = list(self.exist.keys())\n",
    "        self.col = len(self.keys)\n",
    "        self.matrix = np.empty((self.row, self.col, ), dtype=\"float32\")\n",
    "        \n",
    "        del seq\n",
    "    \n",
    "    \n",
    "    def calculate_frequence(self, infile):\n",
    "        \n",
    "        for line, read in infile.itertuples(index=True, name=None): \n",
    "                 \n",
    "            if self.convert == 1:\n",
    "                seq = self.translate(read)\n",
    "                del read\n",
    "\n",
    "                counts = self.exist.copy()\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                for i in range(num):\n",
    "                    kmer = seq[i:i+self.k]\n",
    "                    counts[kmer] += 1\n",
    "\n",
    "            else:\n",
    "                seq = read\n",
    "                del read\n",
    "\n",
    "                counts = self.exist.copy()\n",
    "                num = len(seq) - self.k + 1\n",
    "\n",
    "                if re.match('^[ACGT]*$', seq): \n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        counts[kmer] += 1\n",
    "                else:\n",
    "                    for i in range(num):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        if re.match('^[ACGT]*$', kmer): \n",
    "                            counts[kmer] += 1\n",
    "\n",
    "            vector = np.array(list(counts.values()), dtype = \"float32\")/num\n",
    "\n",
    "            self.matrix[line] = vector\n",
    "            \n",
    "            counts.clear()\n",
    "            del vector\n",
    "            del seq\n",
    "            del counts\n",
    "    \n",
    "    \n",
    "    def get_keys(self):\n",
    "        \n",
    "        return(self.keys)\n",
    "    \n",
    "    \n",
    "    def get_matrix(self):\n",
    "        \n",
    "        return(self.matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-holmes",
   "metadata": {},
   "source": [
    "#### Frequency Vector Dimension reduction\n",
    "\n",
    "- n neighbors:\n",
    "    - difficult\n",
    "    - below 50 = ~400 **very good** clusters\n",
    "    - around 100 = ~250 **good** clusters\n",
    "    - above 200 = ~150 **mediocre** clusters\n",
    "- min. dist:\n",
    "    - less as possible for better seperation in dimension reduction\n",
    "    - 0.1 creates more noise than 0.0\n",
    "- n components:\n",
    "    - the higher the resulting dimension the more time is needed for HDB clustering\n",
    "- random state:\n",
    "    - 42 answer on everything in the universe\n",
    "    - completely **useless** parameter\n",
    "- metric:\n",
    "    - seems to have very little effect on the **euclidean** clustering of HDB\n",
    "- frequency:\n",
    "    - removed frequency of AA chain\n",
    "    - *way better results*\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "olive-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fasta(infile, outfile):\n",
    "\n",
    "    #with some help from https://stackoverflow.com/questions/50856538/how-to-convert-multiline-fasta-files-to-singleline-fasta-files-without-biopython\n",
    "    \n",
    "    with open(infile) as f_input, open(outfile, 'w') as f_output:\n",
    "\n",
    "        block = []\n",
    "\n",
    "        for line in f_input:\n",
    "\n",
    "            if line.startswith('>'):\n",
    "\n",
    "                if block:\n",
    "                    f_output.write(new_line + ',' + \"\".join(block) + '\\n')\n",
    "                    block = []\n",
    "                \n",
    "                new_line = line.strip().replace('|', ',')\n",
    "            else:\n",
    "                block.append(line.strip())\n",
    "\n",
    "        if block:\n",
    "            f_output.write(new_line + ',' + \"\".join(block) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-radiation",
   "metadata": {},
   "source": [
    "#### Convert Fasta to CSV\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quarterly-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(sequence, accession):\n",
    "\n",
    "    freq = frequency(k = 7, convert = 0)\n",
    "    freq.adjust_to_data(sequence)\n",
    "    freq.calculate_frequence(sequence)\n",
    "\n",
    "    matrix = freq.get_matrix()\n",
    "    keys = freq.get_keys()\n",
    "\n",
    "    #Dimension Reduction with UMAP\n",
    "\n",
    "    reduced = umap.UMAP(\n",
    "        n_neighbors = 100,\n",
    "        min_dist = 0.0,\n",
    "        n_components = 20,\n",
    "        random_state = 42,\n",
    "        metric = 'cosine',\n",
    "    ).fit_transform(matrix)\n",
    "\n",
    "    dataframe = pd.concat([accession, pd.DataFrame(reduced)], axis=1, copy = False, ignore_index = False).set_index('accession')\n",
    "    \n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-rally",
   "metadata": {},
   "source": [
    "#### DBCV and Epsilon Cutoff Discovery\n",
    "\n",
    "- **HDB scan don't necessarily create round clusters**\n",
    "    - calculations based on round clusters are useless\n",
    "    - centroid calculation difficult\n",
    "    - inner cluster distances wrong\n",
    "    - silhouette score wrong\n",
    "        - look [here](https://pberba.github.io/stats/2020/01/17/hdbscan/)\n",
    "- DBCV and Epsilon\n",
    "    - Epsilon:\n",
    "        - Hybrid clustering with DBSCAN* and HDBSCAN*\n",
    "        - look [here](https://hdbscan.readthedocs.io/en/latest/how_to_use_epsilon.html)\n",
    "    - DBCV:\n",
    "        - Density Based Cluster Validity (DBCV) score\n",
    "        - [paper](https://www.dbs.ifi.lmu.de/~zimek/publications/SDM2014/DBCV.pdf)\n",
    "        - needs minimum spanning tree (parameter = True)\n",
    "    - loop to increse Epsilon by comparison with the DBCV score\n",
    "    - *seems to be promising*\n",
    "- centroids:\n",
    "    - *maybe by extraction exemplars and connect them single linkage wise*\n",
    "    - *~13000 exemplars and 13000 clusters -> 250 clusters*\n",
    "    - *how to shrink number of exemplars*\n",
    "- removed reinitialisation of lists to save all values\n",
    "- unclustered and cluster number removed and integrated in final clustering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elementary-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(dataframe):\n",
    "    \n",
    "    epsilon = 0.0\n",
    "    dbcv_list = []\n",
    "    #n_cluster_list = []\n",
    "    #unclustered_list = []\n",
    "    epsilon_list = []\n",
    "\n",
    "    clusterer_init = hdbscan.HDBSCAN(\n",
    "        min_samples = 1,\n",
    "        min_cluster_size = 2,\n",
    "        cluster_selection_epsilon = 0.0,\n",
    "        gen_min_span_tree=True,\n",
    "        #metric = 'euclidean',\n",
    "    ).fit(dataframe)\n",
    "\n",
    "    epsilon_max = clusterer_init.single_linkage_tree_.to_pandas()['distance'].max().item()\n",
    "\n",
    "    while epsilon <= epsilon_max:\n",
    "\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_samples = 1,\n",
    "            min_cluster_size = 2,\n",
    "            cluster_selection_epsilon = epsilon,\n",
    "            gen_min_span_tree=True,\n",
    "            #metric = 'euclidean',\n",
    "        ).fit(dataframe)\n",
    "\n",
    "        dbcv_list.append(clusterer.relative_validity_.item())\n",
    "        label = list(clusterer.labels_)\n",
    "        #n_cluster_list.append(len(set(label))-1)\n",
    "        #unclustered_list.append(label.count(-1))\n",
    "        epsilon_list.append(epsilon)\n",
    "\n",
    "        epsilon = round(epsilon + 1.0, 2)\n",
    "\n",
    "    epsilon_best = epsilon_list[dbcv_list.index(max(dbcv_list))]\n",
    "\n",
    "    #print(f\"First rough iteration:\")\n",
    "    #print(f\"Best DBCV value: {max(dbcv_list)}\")\n",
    "    #print(f\"Cluster number: {n_cluster_list[dbcv_list.index(max(dbcv_list))]}\")\n",
    "    #print(f\"Unclustered/Noise: {unclustered_list[dbcv_list.index(max(dbcv_list))]}\")\n",
    "\n",
    "    epsilon = round(epsilon_best - 1.0, 2)\n",
    "\n",
    "    if (epsilon < 0.0):\n",
    "        epsilon = 0.0\n",
    "\n",
    "    epsilon_max = round(epsilon_best + 1.0, 2)\n",
    "    #dbcv_list = []\n",
    "    #n_cluster_list = []\n",
    "    #unclustered_list = []\n",
    "    #epsilon_list = []\n",
    "\n",
    "    #DBCV Exploration (fine)\n",
    "\n",
    "    while epsilon <= epsilon_max:\n",
    "\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_samples = 1,\n",
    "            min_cluster_size = 2,\n",
    "            cluster_selection_epsilon = epsilon,\n",
    "            gen_min_span_tree=True,\n",
    "            #metric = 'euclidean',\n",
    "        ).fit(dataframe)\n",
    "\n",
    "        dbcv_list.append(clusterer.relative_validity_.item())\n",
    "        label = list(clusterer.labels_)\n",
    "        #n_cluster_list.append(len(set(label))-1)\n",
    "        #unclustered_list.append(label.count(-1))\n",
    "        epsilon_list.append(epsilon)\n",
    "\n",
    "        epsilon = round(epsilon + 0.1, 2)\n",
    "\n",
    "    dbcv_best = max(dbcv_list)\n",
    "    epsilon_best = epsilon_list[dbcv_list.index(dbcv_best)]\n",
    "    parameter = pd.DataFrame(zip(epsilon_list, dbcv_list), columns = ['epsilon', 'DBCV']).set_index('epsilon')\n",
    "    \n",
    "    #print(f\"Second fine iteration:\")\n",
    "    #print(f\"Best DBCV value: {max(dbcv_list)}\")\n",
    "    #print(f\"Cluster number: {n_cluster_list[dbcv_list.index(max(dbcv_list))]}\")\n",
    "    #print(f\"Unclustered/Noise: {unclustered_list[dbcv_list.index(max(dbcv_list))]}\")\n",
    "    \n",
    "    return(parameter, epsilon_best, dbcv_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-fleet",
   "metadata": {},
   "source": [
    "#### Clustering using optimized Epsilon\n",
    "\n",
    "- Final clustering\n",
    "    - *is the clustering the best possible?*\n",
    "    - min. samples: the higher the more conservative = more noise = more unclustered\n",
    "    - min. clustersize: value small? some very special influenza strains should be in small clusters\n",
    "        - *maybe increase*\n",
    "    - selection method?\n",
    "        - HDB hybrid pager is a comparison and eom seems to be more matching\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "secure-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(epsilon_best, dataframe, extra, accession):\n",
    "\n",
    "    clusterer_best = hdbscan.HDBSCAN(\n",
    "        min_samples = 1,\n",
    "        min_cluster_size = 2,\n",
    "        cluster_selection_epsilon = epsilon_best,\n",
    "        gen_min_span_tree=True,\n",
    "        cluster_selection_method = 'eom',\n",
    "        #metric = 'euclidean',\n",
    "    ).fit(dataframe)\n",
    "\n",
    "    label = clusterer_best.labels_\n",
    "    label_list = label.tolist()\n",
    "    \n",
    "    n_cluster = len(set(label_list))-1\n",
    "    unclustered = label_list.count(-1)\n",
    "    \n",
    "    cluster = pd.concat([pd.DataFrame(label, columns = ['cluster']), extra, accession], axis=1, copy = False).set_index('accession')\n",
    "    linkage = clusterer_best.single_linkage_tree_.to_pandas()\n",
    "    linkage.set_index('parent', inplace = True)\n",
    "    \n",
    "    #N_unmatch = cluster.groupby('cluster').filter(lambda x: x['N'].nunique() > 1).cluster.nunique()\n",
    "    N_unmatch = cluster.groupby('cluster').filter(lambda x: x['N'].replace('', np.nan).nunique() > 1).cluster.nunique()\n",
    "    H_unmatch = cluster.groupby('cluster').filter(lambda x: x['H'].replace('', np.nan).nunique() > 1).cluster.nunique() \n",
    "    \n",
    "    return(cluster, linkage, n_cluster, unclustered, H_unmatch, N_unmatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-nightlife",
   "metadata": {},
   "source": [
    "#### Centroid extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assured-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(cluster, dataframe):\n",
    "    \n",
    "    centroid = cluster[['cluster']].copy()\n",
    "    centroid.insert(1, 'centroid', 'false')\n",
    "\n",
    "    num = cluster['cluster'].max()+1\n",
    "    values = ['true']*num\n",
    "    accessions = []\n",
    "\n",
    "    for i in range(num):\n",
    "\n",
    "        query = cluster[cluster.cluster == i]\n",
    "        match = query.index.values.tolist()\n",
    "        sub = dataframe.filter(items = match, axis=0)\n",
    "        dist = ssd.cdist(sub, sub, metric = 'cosine')\n",
    "        inner_mean = pd.DataFrame(dist, columns = match, index = match, dtype = 'float32').mean()\n",
    "        accessions.append(inner_mean.idxmin())\n",
    "\n",
    "    centroid.update(pd.DataFrame(values, columns=['centroid'], index = accessions))\n",
    "\n",
    "    return(centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-jenny",
   "metadata": {},
   "source": [
    "#### Finding th Elbow Method parameter for Backup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "following-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elbow(linkage):\n",
    "    \n",
    "    #with some help from https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\n",
    "    \n",
    "    Z = linkage.reset_index().drop(columns=['parent']).to_numpy()\n",
    "\n",
    "    last = Z[-400:, 2]\n",
    "    last_rev = last[::-1]\n",
    "    idxs = np.arange(1, len(last) + 1)\n",
    "\n",
    "    acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "    acceleration_rev = acceleration[::-1]\n",
    "    \n",
    "    elbow = pd.DataFrame({'n_cluster': idxs[1:-1], 'distance': last_rev[1:-1], 'acceleration': acceleration_rev}).set_index('n_cluster')\n",
    "    \n",
    "    return(elbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-corruption",
   "metadata": {},
   "source": [
    "#### Plot Parameter\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "personal-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dbcv(parameter):\n",
    "\n",
    "    sns.set()\n",
    "    graph = sns.FacetGrid(parameter, col ='segment', col_wrap=4, sharex=True, sharey=True, height=4)\n",
    "    graph.map(sns.lineplot, \"epsilon\", \"DBCV\")#.add_legend() \n",
    "    graph.map_dataframe(annotate_dbcv)\n",
    "    graph.set_axis_labels(\"Epsilon\", \"DBCV\")\n",
    "    graph.set_titles(col_template=\"Segment {col_name}\", row_template=\"Segment {row_name}\")\n",
    "    #plt.subplots_adjust(top=0.9)\n",
    "    #graph.fig.suptitle('DBCV by changing Epsilon')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('DBCV_Epsilon.svg')\n",
    "    plt.savefig('DBCV_Epsilon.pdf')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exact-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_dbcv(data, **kws):\n",
    "    \n",
    "    x = data[\"epsilon\"][data[\"DBCV\"].idxmax()].item()\n",
    "    #t = str(n)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    trans = transforms.blended_transform_factory(\n",
    "    ax.transData, ax.transAxes)\n",
    "    \n",
    "    ax.axvline(x = x, alpha=.5, color=\"r\")\n",
    "    ax.text(x, 0.05, f\"{x:0.1f}\", rotation='vertical', ha='right', transform=trans, color = \"r\", alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-blond",
   "metadata": {},
   "source": [
    "#### Plot Density\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "incoming-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(density):\n",
    "\n",
    "    sns.set()\n",
    "    \n",
    "    histo = sns.FacetGrid(density, col ='segment', col_wrap=4, sharex=True, sharey=True, height=4) \n",
    "    histo.map(sns.histplot, \"size\", kde=True, log_scale=True, binwidth=0.1, element=\"step\", fill = True)#.add_legend()\n",
    "    histo.map_dataframe(annotate_density)\n",
    "    histo.set_axis_labels(\"Count\", \"Cluster Size\")\n",
    "    histo.set_titles(col_template=\"Segment {col_name}\", row_template=\"Segment {row_name}\")\n",
    "    #plt.subplots_adjust(top=0.9)\n",
    "    #histo.fig.suptitle('Cluster Size Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Cluster_Distribution.svg')\n",
    "    plt.savefig('Cluster_Distribution.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complete-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_density(data, **kws):\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    dens_list = ax.get_lines()[0].get_data()[1]\n",
    "    \n",
    "    trans = transforms.blended_transform_factory(ax.transData, ax.transAxes)\n",
    "    \n",
    "    max_dens_index = dens_list.argmax()\n",
    "\n",
    "    x = ax.get_lines()[0].get_data()[0][max_dens_index]\n",
    "    \n",
    "    ax.axvline(x = x, alpha=.5, color=\"r\")\n",
    "    ax.text(x, 0.05, f\"{x:0.1f}\", rotation='vertical', ha='right', transform=trans, color = \"r\", alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-subscriber",
   "metadata": {},
   "source": [
    "#### Plot Elbow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "selective-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elbow(elbow):\n",
    "\n",
    "    sns.set()\n",
    "    graph = sns.FacetGrid(elbow, col ='segment', col_wrap=4, sharex=True, sharey=True, height=4)\n",
    "    graph.map(sns.lineplot, \"n_cluster\", \"distance\")#.add_legend() \n",
    "    graph.map(sns.lineplot, \"n_cluster\", \"acceleration\", color=\"r\", alpha = 0.5)#.add_legend() \n",
    "    graph.set_axis_labels(\"Cluster [k]\", \"Distance\")\n",
    "    graph.set_titles(col_template=\"Segment {col_name}\", row_template=\"Segment {row_name}\")\n",
    "    #plt.subplots_adjust(top=0.9)\n",
    "    #graph.fig.suptitle('Clusternumber estimation by Elbow Method')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Cluster_Elbow.svg')\n",
    "    plt.savefig('Cluster_Elbow.pdf')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-abuse",
   "metadata": {},
   "source": [
    "#### Plot Linkage tree\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defensive-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linkage(cluster, linkage, segment, protein):\n",
    "\n",
    "#with some help from https://stackoverflow.com/questions/38153829/custom-cluster-colors-of-scipy-dendrogram-in-python-link-color-func\n",
    "    \n",
    "    num = cluster.query('segment == @segment')['cluster'].max().item()+1\n",
    "\n",
    "    tree_hex = []\n",
    "    if protein == 'H':\n",
    "        tree_col = sns.color_palette('husl', n_colors=18)\n",
    "    elif protein == 'N':\n",
    "        tree_col = sns.color_palette('husl', n_colors=11)\n",
    "    \n",
    "    for col in tree_col:\n",
    "        tree_hex.append(colors.to_hex(col))\n",
    "\n",
    "    tree_cluster = cluster.query('segment == @segment')[protein]\n",
    "\n",
    "    tree_label = tree_cluster.index\n",
    "\n",
    "    mixed_hex = \"#000000\"\n",
    "    tree_dict = {}\n",
    "    \n",
    "    for index, value in tree_cluster.items():\n",
    "        \n",
    "        if re.match(f'^[{protein}][0-9]+$', value): \n",
    "            tree_dict[index] = tree_hex[int(value[1:])]\n",
    "        else:\n",
    "            tree_dict[index] = mixed_hex\n",
    "\n",
    "    tree_Z = linkage.query('segment == @segment').drop(columns=['segment', 'parent'])\n",
    "\n",
    "    tree_np = tree_Z.to_numpy()\n",
    "\n",
    "    dflt_hex = \"#808080\"\n",
    "    tree_link = {}\n",
    "\n",
    "    for i, i12 in enumerate(tree_np[:,:2].astype(int)):\n",
    "\n",
    "        c1, c2 = (tree_link[x] if x > len(tree_np) else tree_dict[tree_label[x]] for x in i12)\n",
    "\n",
    "        #tree_link[i+1+len(tree_np)] = c1 if c1 == c2 else dflt_hex\n",
    "\n",
    "        if c1 == c2:\n",
    "            tree_link[i+1+len(tree_np)] = c1 \n",
    "        elif c1 == mixed_hex:\n",
    "            tree_link[i+1+len(tree_np)] = c2\n",
    "        elif c2 == mixed_hex:\n",
    "            tree_link[i+1+len(tree_np)] = c1\n",
    "        else: \n",
    "            tree_link[i+1+len(tree_np)] = dflt_hex\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(50, 100)) \n",
    "    sns.set()\n",
    "    #plt.title('Hierarchical Clustering Dendrogram of Segment ' + str(segment) + ' colored by ' + str(protein) + ' Protein', fontsize=40)\n",
    "    plt.ylabel('Sample Index', fontsize=30)\n",
    "    plt.xlabel('Distance', fontsize=30)\n",
    "    plt.xticks(fontsize = 20)\n",
    "    plt.style.use('ggplot')\n",
    "    with plt.rc_context({'lines.linewidth': 4}):\n",
    "        dendrogram(Z=tree_np, labels=tree_label, color_threshold=None, leaf_font_size=15, leaf_rotation=0, link_color_func=lambda x: tree_link[x], truncate_mode='lastp', p=num, orientation=\"left\")#, show_contracted=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Linkage_Cluster_' + str(segment) + '.svg')\n",
    "    plt.savefig('Linkage_Cluster_' + str(segment) + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-chicken",
   "metadata": {},
   "source": [
    "#### Main Pipeline\n",
    "\n",
    "- Metric for centroids (cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thorough-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsplit(subtype):\n",
    "    if re.match('^[H][0-9]+N[0-9]+$', subtype): \n",
    "        H = re.search('[H][0-9]+', subtype).group(0)\n",
    "        N = re.search('[N][0-9]+', subtype).group(0)\n",
    "    else:\n",
    "        H = ''#np.nan\n",
    "        N = ''#np.nan\n",
    "    return(H, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "described-alloy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:00:00] |                                  | (ETA:  --:--:--) "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ab6fe49a5d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cluster.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcentroid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'centroid.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drylab5/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drylab5/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drylab5/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drylab5/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drylab5/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drylab5/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[0;32m-> 1357\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drylab5/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cluster.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ab6fe49a5d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mextra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'H'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'strain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mparameter_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbcv_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mcluster_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinkage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munclustered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_unmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_unmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9fe5192a189d>\u001b[0m in \u001b[0;36mget_vectors\u001b[0;34m(sequence, accession)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_frequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-32f4595e237e>\u001b[0m in \u001b[0;36mcalculate_frequence\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m    111\u001b[0m                             \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkmer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    cluster = pd.read_csv('cluster.csv', sep = ',', na_filter = False, header = 0, index_col = 0)\n",
    "    centroid = pd.read_csv('centroid.csv', sep = ',', na_filter = False, header = 0, index_col = 0)\n",
    "    linkage = pd.read_csv('linkage.csv', sep = ',', na_filter = False, header = 0)\n",
    "    parameter = pd.read_csv('parameter.csv', sep = ',', na_filter = False, header = 0)\n",
    "    information = pd.read_csv('information.csv', sep = ',', na_filter = False, header = 0, index_col = 0)\n",
    "    density = pd.read_csv('density.csv', sep = ',', na_filter = False, header = 0)\n",
    "    elbow = pd.read_csv('elbow.csv', sep = ',', na_filter = False, header = 0)\n",
    "\n",
    "except:\n",
    "    \n",
    "    #Convert the Fasta to Csv\n",
    "    convert_fasta('../Data/A.fasta', '../Data/A2.csv')\n",
    "    \n",
    "    #Dict Creation\n",
    "    parameter_dict = co.defaultdict(list)\n",
    "    cluster_dict = co.defaultdict(list)\n",
    "    centroid_dict = co.defaultdict(list)\n",
    "    information_dict = co.defaultdict(list)\n",
    "    linkage_dict = co.defaultdict(list)\n",
    "    elbow_dict = co.defaultdict(list)\n",
    "\n",
    "    #Upload Fasta\n",
    "    upload = pd.read_csv('../Data/A2.csv', sep = ',', na_filter = False, header = None)\n",
    "    upload.columns = ['accession', 'strain', 'segment', 'protein', 'genus', 'subtype', 'date', 'host', 'curation', 'genome']\n",
    "    upload.query('curation == \"Pass\"', inplace = True)\n",
    "    upload[['H', 'N']] = upload.subtype.apply(lambda x: subsplit(x)).tolist()\n",
    "\n",
    "    #Progress Variables\n",
    "    segments = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    widgets = [' [', progressbar.Timer(format = 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('#'),' (', progressbar.ETA(), ') ', ] \n",
    "    bar = progressbar.ProgressBar(max_value = len(segments), widgets = widgets).start() \n",
    "\n",
    "    #Pipeline\n",
    "    #for segment in segments[:1]:\n",
    "    for segment in segments:\n",
    "\n",
    "        subset = upload.query('segment == @segment').reset_index()\n",
    "        sequence = subset[['genome']].copy()\n",
    "        accession = subset[['accession']].copy()\n",
    "        extra = subset[['H', 'N', 'strain']].copy()\n",
    "\n",
    "        dataframe = get_vectors(sequence, accession)\n",
    "        parameter_dict[segment], epsilon_best, dbcv_best = get_epsilon(dataframe)\n",
    "        cluster_dict[segment], linkage_dict[segment], n_cluster, unclustered, H_unmatch, N_unmatch = get_cluster(epsilon_best, dataframe, extra, accession)\n",
    "        centroid_dict[segment] = get_centroids(cluster_dict[segment], dataframe)\n",
    "        information_dict[segment] = [n_cluster, unclustered, H_unmatch, N_unmatch, epsilon_best, dbcv_best]\n",
    "        elbow_dict[segment] = get_elbow(linkage_dict[segment])\n",
    "\n",
    "        bar.update(segment)\n",
    "\n",
    "    bar.finish()    \n",
    "\n",
    "    #Polish the Tables\n",
    "    cluster = pd.concat(cluster_dict)\n",
    "    cluster.index.set_names([\"segment\", \"accession\"], inplace=True)\n",
    "    cluster.reset_index(level = \"segment\", inplace=True)\n",
    "    \n",
    "    centroid = pd.concat(centroid_dict)\n",
    "    centroid.index.set_names([\"segment\", \"accession\"], inplace=True)\n",
    "    centroid.reset_index(level = \"segment\", inplace=True)\n",
    "\n",
    "    linkage = pd.concat(linkage_dict)\n",
    "    linkage.index.set_names([\"segment\", \"parent\"], inplace=True)\n",
    "    linkage.reset_index(level = [\"segment\", \"parent\"], inplace=True)\n",
    "    \n",
    "    parameter = pd.concat(parameter_dict)\n",
    "    parameter.index.set_names([\"segment\", \"epsilon\"], inplace=True)\n",
    "    parameter.reset_index(level = [\"segment\", \"epsilon\"], inplace=True)\n",
    "\n",
    "    information = pd.DataFrame.from_dict(information_dict, orient='index', columns=['n_cluster', 'unclustered', 'H_unmatch', 'N_unmatch', 'epsilon_best', 'dbcv_best'])\n",
    "    information.index.set_names([\"segment\"], inplace=True)\n",
    "\n",
    "    elbow = pd.concat(elbow_dict)\n",
    "    elbow.index.set_names(['segment', 'n_cluster'], inplace=True)\n",
    "    elbow.reset_index(level = [\"segment\", \"n_cluster\"], inplace=True)\n",
    "    \n",
    "    density = pd.DataFrame(cluster.value_counts(subset=['segment', 'cluster']), columns = ['size'])\n",
    "    density.reset_index(level = [\"segment\", \"cluster\"], inplace=True)\n",
    "    \n",
    "    #Export the Tables\n",
    "    ##Result Tables\n",
    "    cluster.to_csv('cluster.csv', index=True, header=True, sep=',', mode='w')\n",
    "    centroid.to_csv('centroid.csv', index=True, header=True, sep=',', mode='w')\n",
    "    information.to_csv('information.csv', index=True, header=True, sep=',', mode='w')\n",
    "    #cluster.to_latex('cluster.tex')\n",
    "    #centroid.to_latex('centroid.tex')\n",
    "    #information.to_latex('information.tex')\n",
    "    \n",
    "    ##Supplement & Plot Tables:\n",
    "    parameter.to_csv('parameter.csv', index=False, header=True, sep=',', mode='w')\n",
    "    elbow.to_csv('elbow.csv', index=False, header=True, sep=',', mode='w')\n",
    "    density.to_csv('density.csv', index=False, header=True, sep=',', mode='w')\n",
    "    linkage.to_csv('linkage.csv', index=False, header=True, sep=',', mode='w')\n",
    "    #linkage.to_latex('linkage.tex')\n",
    "    #parameter.to_latex('parameter.tex')\n",
    "    #elbow.to_latex('elbow.tex')\n",
    "    #density.to_latex('density.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-advocate",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dbcv(parameter)\n",
    "plot_density(density)\n",
    "plot_elbow(elbow)\n",
    "display(information)\n",
    "display(cluster)\n",
    "plot_linkage(cluster, linkage, 4, 'H')\n",
    "plot_linkage(cluster, linkage, 6, 'N')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-treasure",
   "metadata": {},
   "source": [
    "### Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-riding",
   "metadata": {},
   "source": [
    "#### Main Pipeline\n",
    "\n",
    "- Visualize Nucleotide Sequences\n",
    "    - taken from [here](https://github.com/jupyterlab/jupyter-renderers/tree/master/packages/fasta-extension)\n",
    "    - official jupyter extension example code\n",
    "- Merge not working like expacted:\n",
    "    - overlap = 4 (2x2)\n",
    "        - like every possible combination\n",
    "        - with 3 and 3 ist 9 not 6\n",
    "        - which behaivior better?\n",
    "    - Change:\n",
    "        - Intersection and Combination\n",
    "        - one column with intersection:\n",
    "            - 3 and 3 = 3\n",
    "        - one column with combination:\n",
    "            - 3 and 3 = 9\n",
    "    - Based on difficult merge and count combinations in pandas\n",
    "    - third possible solutions reduce straon to one \n",
    "        - one column with same strain:\n",
    "            - 1\n",
    "            - maybe best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta(data):\n",
    "    \n",
    "    bundle = {}\n",
    "    bundle['application/vnd.fasta.fasta'] = data\n",
    "    bundle['text/plain'] = data\n",
    "    display(bundle, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment(dataframe):    \n",
    "\n",
    "    fasta.to_csv('tmp.fasta', header=None, index=True, sep='\\n', mode='w')\n",
    "\n",
    "    mafft_cline = MafftCommandline(input='tmp.fasta', thread=threads)\n",
    "    stdout, stderr = mafft_cline()\n",
    "\n",
    "    os.remove('tmp.fasta') \n",
    "    \n",
    "    return(stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    interaction = pd.read_csv('interaction.csv', sep = ',', na_filter = False, header = 0, index_col = 0)\n",
    "    merge = pd.read_csv('merge.csv', sep = ',', na_filter = False, header = 0)\n",
    "\n",
    "except:\n",
    "\n",
    "    occurence = cluster.reset_index(drop = True).drop(['H', 'N'], axis = 1).drop_duplicates()#[['segment', 'cluster', 'strain']]\n",
    "\n",
    "    merge = (occurence.merge(occurence, on=['strain'], suffixes = [\"_x\", \"_y\"]))# & segment_x != segment_y')\n",
    "    interaction = (occurence.query('segment == 1').drop(['segment'], axis = 1)\n",
    "             .merge(occurence.query('segment == 2').drop(['segment'], axis = 1), on=['strain'], suffixes = [\"_1\", \"_2\"])\n",
    "             .merge(occurence.query('segment == 3').drop(['segment'], axis = 1), on=['strain'])\n",
    "             .merge(occurence.query('segment == 4').drop(['segment'], axis = 1), on=['strain'], suffixes = [\"_3\", \"_4\"])\n",
    "             .merge(occurence.query('segment == 5').drop(['segment'], axis = 1), on=['strain'])\n",
    "             .merge(occurence.query('segment == 6').drop(['segment'], axis = 1), on=['strain'], suffixes = [\"_5\", \"_6\"])\n",
    "             .merge(occurence.query('segment == 7').drop(['segment'], axis = 1), on=['strain'])\n",
    "             .merge(occurence.query('segment == 8').drop(['segment'], axis = 1), on=['strain'], suffixes = [\"_7\", \"_8\"])\n",
    "             .groupby(['cluster_1', 'cluster_2', 'cluster_3', 'cluster_4', 'cluster_5', 'cluster_6', 'cluster_7', 'cluster_8'])\n",
    "             .count().reset_index().set_index('strain'))\n",
    "\n",
    "    interaction.to_csv('interaction.csv', index=True, header=True, sep=',', mode='w')\n",
    "    merge.to_csv('merge.csv', index=False, header=True, sep=',', mode='w')\n",
    "\n",
    "    #interaction.to_latex('interaction.tex')\n",
    "    #merge.to_latex('merge.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_merge(merge):\n",
    "\n",
    "    #better split in 4 figures\n",
    "    #1 4 - 1 4\n",
    "    #5 8 - 1 4\n",
    "    #1 4 - 5 8\n",
    "    #5 8 - 5 8\n",
    "    \n",
    "    sns.set()\n",
    "    graph = sns.displot(data=merge, x=\"cluster_x\", y=\"cluster_y\", col ='segment_x', row = 'segment_y', binwidth=5, height=4)#, height=4\n",
    "    graph.set_axis_labels(\"Cluster\", \"Cluster\")\n",
    "    graph.set_titles(col_template=\"Segment {col_name}\", row_template=\"Segment {row_name}\")\n",
    "    #plt.subplots_adjust(top=0.9)\n",
    "    #graph.fig.suptitle('Strain interactions of Segments Clusters')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Interaction_Merge.svg')\n",
    "    plt.savefig('Interaction_Merge.pdf')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(interaction.sort_index(ascending = False))\n",
    "plot_merge(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-database",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "liable-memorabilia",
   "metadata": {},
   "source": [
    "### Unused Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def plot_interactions(dataframe, x, y):\n",
    "\n",
    "#    subset = dataframe.query('segment_x == @x & segment_y == @y')\n",
    "    \n",
    "#    plot = (ggplot(subset, aes(x=\"cluster_x\", y=\"cluster_y\"))\n",
    "#     + xlab(\"Cluster Segment \" + str(x))\n",
    "#     + ylab(\"Cluster Segment \" + str(y))\n",
    "#     + ggtitle(\"Segment Cluster Interactions\")\n",
    "#     + geom_bin2d(binwidth = 3)\n",
    "#     + geom_density_2d(colour = \"strain\")       \n",
    "#     + geom_tile(aes(fill = str(column), height = 3, width = 3))\n",
    "#     + scale_fill_gradient(low=\"lightgray\", high=\"blue\")\n",
    "#     + theme_538()\n",
    "#    )\n",
    "    \n",
    "#    return(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#occurence = cluster.groupby(['segment', 'cluster', 'strain']).size().to_frame(name='size').reset_index()\n",
    "#merge = occurence.merge(occurence, on=['strain']).query('segment_x != segment_y')\n",
    "#merge['intersection'] = merge.loc[:, ['size_x', 'size_y']].min(axis=1)\n",
    "#merge['sum'] = merge.loc[:, ['size_x', 'size_y']].sum(axis=1)\n",
    "#merge.query('segment_x == 1 & segment_y == 3', inplace=True)\n",
    "#interaction = merge.groupby(['segment_x', 'segment_y', 'cluster_x', 'cluster_y']).sum().reset_index()#.set_index('combinations')\n",
    "\n",
    "#occurence = cluster.reset_index(drop = True).drop(['H', 'N'], axis = 1).drop_duplicates()#[['segment', 'cluster', 'strain']]\n",
    "#merge = occurence.merge(occurence, on=['strain']).query('segment_x != segment_y')\n",
    "#interaction = merge.groupby(['segment_x', 'segment_y', 'cluster_x', 'cluster_y']).count().reset_index().set_index('strain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset = merge.query('segment_x == 4 & segment_y == 7')\n",
    " \n",
    "#(ggplot(subset, aes(x = \"cluster_x\", y = \"cluster_y\"))\n",
    "# + xlab(\"Cluster Segment \" + str(4))\n",
    "# + ylab(\"Cluster Segment \" + str(2))\n",
    "# + ggtitle(\"Segment Cluster Interactions\")\n",
    "# + theme_538()\n",
    "# + stat_density_2d(geom = \"geom_density_2d\")\n",
    "# + geom_bin2d(binwidth = 1)\n",
    "# + scale_x_continuous(limits = [-50, 300])\n",
    "# + scale_y_continuous(limits = [-50, 300])\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    plot = (ggplot(parameter, aes(x=\"epsilon\", y='DBCV', colour = \"factor(segment)\")) \n",
    "#     + labs(\n",
    "#        x=\"Epsilon\",\n",
    "#        y=\"DBCV\",\n",
    "#        colour=\"Segment\",\n",
    "#        title=\"DBCV by changing Epsilon\",\n",
    "#     )\n",
    "#     + geom_line()\n",
    "#    #+ geom_vline(aes(xintercept = epsilon_best))\n",
    "#     + scale_x_continuous()\n",
    "#     + scale_y_continuous()\n",
    "#     + facet_wrap('~segment', ncol = 4)\n",
    "#     + theme_538()\n",
    "#    )\n",
    "    \n",
    "#    return(plot)\n",
    "\n",
    "#    plot = (ggplot(density, aes(x=\"size\", colour = \"factor(segment)\", fill = \"factor(segment)\")) \n",
    "#     + labs(\n",
    "#        x=\"Cluster Size\",\n",
    "#        y=\"Density\",\n",
    "#        fill=\"Segment\",\n",
    "#        colour=\"Segment\",\n",
    "#        title=\"Cluster Size Distribution\",\n",
    "#     )\n",
    "#     + geom_density(alpha = 0.25)\n",
    "#     + scale_x_log10()\n",
    "#     + scale_y_continuous()\n",
    "#     + facet_wrap('~segment', ncol = 4)\n",
    "#     + theme_538()\n",
    "#    )\n",
    "    \n",
    "#    return(plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
