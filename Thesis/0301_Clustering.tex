\section{Method selection} \label{sec:Clustering}

Clustering with each of the four methods, resulted in a table containing the used settings and a summary of the clustering. For better visualization, the tables were combined based on the exploration method. \autoref{tab:Cluster_Knee} contains the results of the two methods using the Kneedle Algorithm (PK and UK) and \autoref{tab:Cluster_DBCV} the results that were based on using the \gls{DBCV} (PD and UD). Every segment of \gls{IAV} was clustered by each of the four methods. By result comparison of methods using the $\varepsilon$ exploration by the Kneedle Algorithm (PK and UK) in \autoref{tab:Cluster_Knee}, a major difference in the number of raw clusters stood out. Hybrid \texttt{HDBSCAN} clustering of the only \texttt{PCA} reduced 7-mer vectors (PK) resulted in around 60 to 70 raw clusters (after \texttt{DBSCAN} part) and subsequently 40 to 50 final clusters (after standard \texttt{HDBSCAN} part) per segment. The number of final clusters was relative close to the state of the art subtype classification with 18 \gls{HA} and 11 \gls{NA} antigen subtypes \autocite{noauthor_revision_1980}. The UK method, using \texttt{UMAP} for reduction, resulted in a higher number of clusters with no difference in raw and final cluster number. Therefore, the hybrid \texttt{HDBSCAN} clustering only used the \texttt{DBSCAN} part. This can be explained by the overall higher $\varepsilon$ threshold compared to the \texttt{PCA} version (PK). By a higher $\varepsilon$, more points were included in the \texttt{DBSCAN} part of the hybrid clustering. In addition to that, the embedding behavior of \texttt{UMAP} probably affected the position of the vectors, decreasing the distances inside of groups of vectors, thereby, leaving less vectors out by the \texttt{DBSCAN} part. With the \texttt{UMAP} reduction method (UK), all vectors of every segment could be clustered, thus, zero vectors were left out unclustered. The approach using \texttt{PCA} alone (PK) was unable to cluster the vectors of around 10 to 30 sequences per segment. However the number of unclustered with PK for, e.~ g.,~ segment 4 was 6 of 56617 used sequences and, therefore, neglectable $\approx$ 0.01\%. As a comparison, 1191 sequences segment 4 are declared unclassified by the current subtype convention making $\approx$ 2\%. 

\begin{table}[!hbt]
    \centering
    \caption[Clustering results with the Kneedle Algorithm]{\textbf{Clustering results with the Kneedle Algorithm.} The results of the clustering methods using the Kneedle Algorithm (PK and UK). Listed is every used segment with the number of raw clusters and the final cluster number by hybrid clustering with the given value of $\varepsilon$. The mixed cluster numbers of H and N denotes number of clusters that contained vectors related to more than one subtype. The variance was calculated as the sum of the explained variance by the \texttt{PCA}.}
    \label{tab:Cluster_Knee}
    \pgfplotstableset{
        create on use/method/.style={
            create col/set list={PK, , , , , , , , UK, , , , , , , }
        }
    }
    \pgfplotstablevertcat{\output}{PCA/information.csv}
    \pgfplotstablevertcat{\output}{UMAP/information.csv}
    \pgfplotstabletypeset[
        every nth row={8}{before row=\midrule},
        every head row/.style={
            before row={
                \toprule
                & & \multicolumn{3}{l}{\textbf{\#Cluster}} &  \multicolumn{2}{l}{\textbf{\#Mixed}} & &\\
                \cmidrule(lr){3-5}\cmidrule(lr){6-7}
            },
            after row={
                \midrule
            },
        },
        every last row/.style={
            after row={
                %... & ... & ... & ... & ... & ... & ... & ...\\
                \bottomrule
            },
        },
        begin table=\begin{tabular*}{\textwidth},
        end table=\end{tabular*},
        columns={method,0,1,2,3,5,6,4,7,8},
        columns/method/.style={string type, multicolumn names=l,column name=\textbf{Method}, column type=@{\extracolsep{\fill}\hspace{6pt}}l},
        columns/0/.style={int detect, multicolumn names=l,column name=\textbf{Segment}, column type=r},
        columns/1/.style={int detect, multicolumn names=l,column name=\textbf{Final}, column type=r},
        columns/2/.style={int detect, multicolumn names=l,column name=\textbf{Raw}, column type=r},
        columns/3/.style={multicolumn names=l,column name=\textbf{Normalized}, column type=r},
        columns/4/.style={int detect, multicolumn names=l,column name=\textbf{\#Unclustered}, column type=r},
        columns/5/.style={int detect, multicolumn names=l,column name=\textbf{H}, column type=r},
        columns/6/.style={int detect, multicolumn names=l,column name=\textbf{N}, column type=r},
        columns/7/.style={multicolumn names=l,column name=\textbf{$\varepsilon$}, column type=r},
        columns/8/.style={multicolumn names=l,column name=\textbf{$\text{Var}(X)$}, column type=r},
    ]
    {\output}
\end{table}

\vspace{1em}

Comparison of the methods PK and UK to the ones using the \gls{DBCV} for $\varepsilon$ exploration, instead of the Kneedle Algorithm (\autoref{tab:Cluster_DBCV}, did not result in much difference for the \texttt{UMAP} reduction with \gls{DBCV} exploration method (UD). The numbers of clusters found were a little higher compared to the results with the Kneedle Algorithm exploration (UK). Using the \gls{DBCV} to find the optimal $\varepsilon$ with the \texttt{PCA} workflow (PD) on the other hand changed the results drastically in comparison to PK. The numbers of final clusters were between 9000 and 12000 depending on the segment, with the exception of segment 4 and the raw cluster numbers were nearly equivalent to the total number of sequences for the other segments. Also the numbers of unclustered sequences were increased by a major amount to around 20\% for most segments, making the method in any case unusable for \gls{IAV} clustering. Only the clustering of segment 4 with \gls{DBCV} $\varepsilon$ exploration (PD) seemed to be as stable as with the Kneedle Algorithm (PK). The numbers of clusters for the other segments were higher, because the \gls{DBCV} method was searching for the $\varepsilon$ value setting that results in the highest \gls{DBCV} possible. In all the segments except 4 this resulted in a value of zero. Hybrid clustering with a $\varepsilon$ value of zero results in clustering with the standard \texttt{HDBSCAN} part only, without prior \texttt{DBSCAN} and is, considering the amount of unclustered sequences, not suited for \gls{IAV} clustering. Based on the results in \autoref{tab:Cluster_Knee} and \autoref{tab:Cluster_DBCV}, the best method for \gls{IAV} clustering seemed to be the combination of \texttt{PCA} and the Kneedle Algorithm (PK). The method resulted in an appropriate number of clusters and was the only combination using all the benefits of the hybrid \texttt{HDBSCAN} clustering. 

\begin{table}[!hbt]
    \centering
    \caption[Clustering results with the DBCV]{\textbf{Clustering results with the DBCV.} The results of the clustering methods using the \gls{DBCV} (PD and UD). Listed is every used segment with the number of raw clusters and the final cluster number by hybrid clustering with the given value of $\varepsilon$. The mixed cluster numbers of H and N denotes number of clusters that contained vectors related to more than one subtype. The variance is calculated as the sum of the explained variance by the \texttt{PCA}.}
    \label{tab:Cluster_DBCV}
    \pgfplotstableset{
        create on use/method/.style={
            create col/set list={PD, , , , , , , , UD, , , , , , , }
        }
    }
    \pgfplotstablevertcat{\output}{PCA/information_alt.csv}
    \pgfplotstablevertcat{\output}{UMAP/information_alt.csv}
    \pgfplotstabletypeset[
        every nth row={8}{before row=\midrule},
        every head row/.style={
            before row={
                \toprule
                & & \multicolumn{2}{l}{\textbf{\#Cluster}} & \multicolumn{2}{l}{\textbf{\#Mixed}} & & &\\
                \cmidrule(lr){3-4}\cmidrule(lr){5-6}
            },
            after row={
                \midrule
            },
        },
        every last row/.style={
            after row={
                %... & ... & ... & ... & ... & ... & ... & ...\\
                \bottomrule
            },
        },
        begin table=\begin{tabular*}{\textwidth},
        end table=\end{tabular*},
        columns={method,0,1,2,4,5,3,6,7,8},
        columns/method/.style={string type, multicolumn names=l,column name=\textbf{Method}, column type=@{\extracolsep{\fill}\hspace{6pt}}l},
        columns/0/.style={int detect, multicolumn names=l,column name=\textbf{Segment}, column type=r},
        columns/1/.style={int detect, multicolumn names=l,column name=\textbf{Final}, column type=r},
        columns/2/.style={int detect, multicolumn names=l,column name=\textbf{Raw}, column type=r},
        columns/3/.style={int detect, multicolumn names=l,column name=\textbf{\#Unclustered}, column type=r},
        columns/4/.style={int detect, multicolumn names=l,column name=\textbf{H}, column type=r},
        columns/5/.style={int detect, multicolumn names=l,column name=\textbf{N}, column type=r},
        columns/6/.style={multicolumn names=l,column name=\textbf{$\varepsilon$}, column type=r},
        columns/7/.style={multicolumn names=l,column name=\textbf{DBCV}, column type=r},
        columns/8/.style={multicolumn names=l,column name=\textbf{$\text{Var}(X)$}, column type=r},
    ]
    {\output}
\end{table}

\newpage

To backup this assumption, the results were visualized for better understanding and analysis. Since investigation of all segments would overfill this section, only the methods PK, UK and UD and their clustering behavior on segment 4 were discussed in detail. As described, the \texttt{PCA} method in combination with the \gls{DBCV} (PD) used only standard \texttt{HDBSCAN}, resulting in a very high number of clusters and 20\% unclustered sequences. Therefore, the method was already rejected and not included in the following discussions. Nevertheless, similar graphics for PD with segment 4, as well as, all used method for the other segments can be found in the \autoref{chap:Appendix}. 

\begin{figure}[!hbt]
    \centering
    %\begin{adjustbox}{minipage=\dimexpr\textwidth-2\fboxsep-2\fboxrule,fbox}
    \begin{subfigure}[b]{0.475\textwidth}
        \caption[Kneedle Algorithm]{\textbf{Kneedle Algorithm}}
        \label{subfig:PCA_Cluster_Knee_Kneedle_4}            \includegraphics[width=\textwidth]{PCA/Cluster_Knee_Segment_4.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \caption[Kneedle Knee]{\textbf{Kneedle Knee}}
        \label{subfig:PCA_Cluster_Knee_Elbow_4}            \includegraphics[width=\textwidth]{PCA/Cluster_Elbow_Knee_Segment_4.pdf}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \caption[Cluster Distribution]{\textbf{Cluster Distribution}}
        \label{subfig:PCA_Cluster_Knee_Distributione_4}            \includegraphics[width=\textwidth]{PCA/Cluster_Distribution_Segment_4.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \caption[Logarithmic Distribution]{\textbf{Logarithmic Distribution}}
        \label{subfig:PCA_Cluster_Knee_Distribution_log_4}            \includegraphics[width=\textwidth]{PCA/Cluster_Distribution_Log_Segment_4.pdf}
    \end{subfigure}
    %\end{adjustbox}
    \caption[Clustering of segment 4 with PK]{\textbf{Clustering of segment 4 with PK.} Segment 4 clustering using the combination of \texttt{PCA} and the Kneedle Algorithm (PK) results in the given figure. The green curve in the top left subfigure describes the change of the distance in the single linkage tree with increasing normalized cluster number and, therefore, the location of the knee, at the maximum, highlighted by the red line. The blue line represents the inverse polynomial representation of the blue line in top right subfigure. The top right subfigure shows the absolute relation of the distance in the single linkage tree to the total number of clusters as the blue line. The red line, indicates the number of raw clusters, by the \texttt{DBSCAN} part of the hybrid \texttt{HDBSCAN} clustering and the final cluster number in green. The yellow line describes the threshold, extracted from the knee and, therefore, the $\varepsilon$ value used to perform the hybrid clustering. The normalized cluster number in the red line in the top left subfigure is equivalent to the raw cluster number in the top right subfigure. The bottom subfigures give information about the distribution of the clusters sizes, by plotting the number of clusters containing a given counted number of sequences in continuous and logarithmic scale.}
    \label{fig:PCA_Cluster_Knee_4}
\end{figure}

\vspace{1em}

A big difference in the cluster size distribution stood out, when comparing the method with \texttt{PCA} (PK) to the ones with \texttt{UMAP} (UK and UD). Only the method using \texttt{PCA} and the Kneedle Algorithm created clusters with more than 10000 vectors (\autoref{subfig:PCA_Cluster_Knee_Distributione_4}). The different cluster sizes using \texttt{UMAP} were spreaded more equally with no cluster containing 3000 vectors or more (\autoref{subfig:UMAP_Cluster_DBCV_Distribution_4} and \autoref{subfig:UMAP_Cluster_Knee_Distributione_4}). Since the \texttt{UMAP} methods also used prior \texttt{PCA} reduction, the major difference was the additional use of \texttt{UMAP} (\autoref{sec:Dimension_Reduction}). Therefore, the difference in cluster size distribution was most likely caused by \texttt{UMAP} itself. As already mentioned \texttt{UMAP} not only reduces the dimension of the data but also changes the position of the vectors in the embedded dimension according to the used settings. The \texttt{n\_neighbors=100} setting seemed to be most likely the cause of a change of this magnitude. By this high number number of neighbors, the vectors were more crowded in large groups to support the bigger picture of the data and, therefore, seemed to build more crowded clusters of similar sizes. 

\vspace{1em}

With the distribution of segment 4 sequences in the data in \autoref{subfig:Frequency_4} in mind, it was expected that a distribution of cluster sizes in segment 4 clustering would in fact approximate the distribution of the sequences in the former one. Therefore, a clustering with \texttt{PCA} in combination with Kneedle Algorithm exploration seemed to give the expected results and appeared again as the best method for \gls{IAV} clustering. Taking also the relation of the cluster number and the distance into consideration, \autoref{subfig:PCA_Cluster_Knee_Elbow_4} indicated continuous merging of clusters with decreasing cluster number in the linkage tree. Thereby, exponential behavior, with a knee point that is easily distinguishable, even without computationally methods, developed. The behavior is not present in \autoref{subfig:UMAP_Cluster_DBCV_Elbow_4} and \autoref{subfig:UMAP_Cluster_Knee_Elbow_4} in a similar degree, as at least two knees occurred.  

% Also the distribution of cluster sizes seems to be more balanced e.~g.~, segment 4 (\autoref{subfig:UMAP_Cluster_Knee_Distributione_4} and \autoref{subfig:UMAP_Cluster_Knee_Distribution_log_4}) than in the \texttt{PCA} approach (\autoref{subfig:PCA_Cluster_Knee_Distributione_4} and \autoref{subfig:PCA_Cluster_Knee_Distribution_log_4}) which could also contribute to the hybrid clustering by \texttt{DBSCAN} alone. With a more distributed number of sequences in the clusters, less very small and very big clusters exist and overall more points are collected in small groups which decreases the chance for single points unaffected by the $\varepsilon$ threshold. 

\vspace{1em}

The results of the method using \texttt{PCA} in combination with the Kneedle Algorithm (PK) were visualized by the clustering tree in \autoref{fig:PCA_Clusteree_Knee_4}. Labeling of the tree was performed based on the \gls{HA} antibody subtype, as shown in \autoref{subfig:Frequency_4}. Therefore, clusters only containing sequences of a given subtype were labeled as such. If a cluster only contained sequences of one subtype, plus some not classified sequences, the not classified sequences were declared as the subtype too. That way, a clear presentation of the subtype distribution by labeling was possible, since the not classified sequences were very likely to actually belong to an existing subtype when clustered that way. If they actually did not belong to a existing subtype, a cluster only containing these not classified sequences would most likely have occurred, which did not happen. Furthermore, the chance to eventually break the classification of subtypes, by declaring not classified sequences to a existing subtype was not given. The clusters were not based on subtypes in any way and the visualization was only for guidance and not possible for any segments other than 4 and 6 anyway. Also, without this assignment of the not classified sequences, no presentation would be possible since they were present in a high number and distributed over mostly all clusters.

\vspace{1em}

If a cluster contained sequences of more than one subtype plus some not classified sequences, no labeling was performed, since the cluster was not homogeneous for one subtype and a declaration of the unclassified was not possible. In the following the term vector will be mostly replaced by the term sequence, to aid the discussion in terms of tree placement and subtype relation. Since the vectors represented the sequences, the terms were linked to each other and used as synonyms. %Exceptions are alignments that did not involve the vector representations, only the sequences. 

\begin{table}[!hbt]
    \centering
    \caption[Unclassified sequences in segment 4 cluster 29 with PK]{\textbf{Unclassified sequences in segment 4 cluster 29 with PK.} The \glspl{MSA} mean distance of the given sequences in comparison to a sample of H9 sequences of the same cluster and a sample of unclassified sequences from other clusters was calculated. Only the first 20 columns are presented, the full table can be found in the projects GitHub Repository\footnotemark.}
    \label{tab:PCA_Error_4_29}
    \pgfplotstabletypeset[
        every head row/.style={
            before row={
                \toprule
            },
            after row={
                \midrule
            },
        },
        every last row/.style={
            after row={
                ... & ... & ... \\
                \bottomrule
            },
        },
        begin table=\begin{tabular*}{0.5\textwidth},
        end table=\end{tabular*},
        columns={0,1,2},
        columns/0/.style={string type,multicolumn names=l,column name=\textbf{Accession}, column type=@{\extracolsep{\fill}\hspace{6pt}}l},
        columns/1/.style={multicolumn names=l,column name=\textbf{H9}, column type=l},
        columns/2/.style={multicolumn names=l,column name=\textbf{unclassified}, column type=l},
    ]
    {PCA/error_segment_4_cluster_29_difference_head.csv}
\end{table}

\vspace{1em}

The prime example of the not classified sequence annotation was the yellow labeled cluster 29 of subtype H9 (\autoref{fig:PCA_Clusteree_Knee_4} \textbf{\textsf{A}}). While the vectors of all sequences of subtype H9 were accumulated in this single cluster, the cluster size of 1569 was bigger than the number of H9 sequences in \autoref{subfig:Frequency_4} with 1454. This was justified by the presence of 115 not classified sequences in the cluster, which were declared as H9, since only H9 and unclassified sequences were present in the cluster. The declaration of the unclassified sequences to be the H9 sequences was supported by the very small evolutionary distance of the unclassified sequences to a sample of H9 sequences from the same cluster (\autoref{tab:PCA_Error_4_29}). The sequences were also compared to unclassified sequences from other clusters to prove the smaller evolutionary distance to H9 sequences. This comparison was performed to prove that, even when only used for visualization reasons, the annotation of not classified sequences in the described way was most likely appropriate.

\vspace{1em}

The number of clusters for a given subtype of \gls{HA} seemed to correspond roughly to the overview of sequences in \autoref{subfig:Frequency_4}. Clusters of very low represented subtypes, like H15, contained mostly all the subtypes sequences, while the high represented subtypes sequences, like H1 and H3, were spreaded over more clusters.

\begin{figure}[!hbt]
    \centering
    \footnotesize
    \begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{PCA/Clustertree_Segment_4_H_Knee.pdf}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
            %\draw[help lines,xstep=.1,ystep=.1] (0,0) grid (1,1);
            \node at (0.16,0.18) [arrowstyle=1.0cm, arrowfillR, anchor=east, rotate=45] {\textbf{\textsf{A}}};
            \node at (0.5,0.08) [arrowstyle=1.0cm, arrowfillR, anchor=east, rotate=45] {\textbf{\textsf{B}}};
            \node at (0.89,0.62) [arrowstyle=1.0cm, arrowfillR, anchor=east, rotate=180] {\rotatebox{180}{\textbf{\textsf{C}}}};
            \node at (0.7,0.9) [arrowstyle=1.0cm, arrowfillR, anchor=east, rotate=180] {\rotatebox{180}{\textbf{\textsf{D}}}};
        \end{scope}
    \end{tikzpicture}

    \caption[Clustering tree of segment 4 with PK]{\textbf{Clustering tree of segment 4 with PK.} The clustering tree of segment 4 clustering, using the combination of \texttt{PCA} and the Kneedle Algorithm (PK) (\autoref{fig:PCA_Cluster_Knee_4}). The labeling of the clusters in the tree is based on the subtype of the contained sequences. Unclassified sequences of a cluster were reclassified as a given subtype if sequences of only this subtype were present in the cluster, in addition to the unclassified ones. Unlabeled clusters contain sequences from at least two subtypes and zero or more unclassified sequences. Two clusters were mixed since they contained sequences of more than one subtype (\autoref{tab:Cluster_Knee}). These non homogeneous clusters are marked by \textbf{\textsf{B}} and \textbf{\textsf{C}}. The cluster 29 marked by \textbf{\textsf{A}} was an example for a cluster consisting of all sequences from a given subtype. The misplaced cluster from subtype H3 is marked by \textbf{\textsf{D}}. Dotted lines in the tree indicate the same host.}
    \label{fig:PCA_Clusteree_Knee_4}
\end{figure}

\vspace{1em}

\footnotetext{\url{https://github.com/ahenoch/Masterthesis.git}}
Striking anomalies divergent from the expected nearly uniform allocation of the subtypes in \autoref{fig:PCA_Cluster_Knee_4} are annotated by \textbf{\textsf{B}}, \textbf{\textsf{C}} and \textbf{\textsf{D}} and will be discussed in the following. For final acceptance of the method PK as the prime \gls{IAV} clustering method proposed in this project, the clustering tree was compared to a similar one created on the results from method UK (\autoref{fig:UMAP_Clusteree_Knee_4}). While the labeling of the clustering tree of method PK resemble the subtype classification of \gls{IAV} very closely, no recognizable subtype separation is present in the clustering tree of UK. 