\section{Differences in dimension reduction} \label{sec:Dimension_Reduction}

To investigate the dimension reduction behavior prior to the clustering and, thereby, find explanations for the mentioned errors, the small H13 and H16 subset of segment 4 $k$-mer frequencies, was reduced by \texttt{PCA} and \texttt{UMAP} to two components for visualization. Comparison to \texttt{UMAP} was done although the method was already declared as not appropriate, to validate this statement again and see the impact of different neighbor values mentioned in \autoref{chap:Materials_and_Methods}. 

\vspace{1em}

The target of the dimension reduction prior to the \texttt{HDBSCAN} clustering, was to find a representation of the data with lower complexity, that is suitable to be used for the clustering, while preserve as much information as possible. As explained in \autoref{sec:K_mer_Representation} and \autoref{sec:Comparison_Clustering}, the optimal representation of the vectors should make a clear difference between H13 and H16. This finding will also be used as the ground truth in the following. Since the vectors were visualized in two dimensions, the term point instead of vector will be used.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=\textwidth]{PCA/Difference_Segment_4_H_metric_cosine.pdf}
    \caption[Comparison of H13/H16 component reductions]{\textbf{Comparison of H13/H16 component reductions.} The subset of sequences from the H13 and H16 clusters in \autoref{fig:PCA_Clusteree_Knee_4} were reduced down to two dimensions enabling simple visualization. Cluster labeling was performed according to \autoref{fig:PCA_Clusteree_Knee_4}. Sole use of \texttt{PCA} (top left picture) as well as the combination with \texttt{UMAP} (other three pictures) was performed as described in \autoref{sec:Dimension_Reduction} with the reduction to two components for visualization. For the combination of \texttt{PCA} and \texttt{UMAP} different values for the neighbors setting were used, the \texttt{UMAP} standard value 15, a average value 50 and the standard value of this project 100. The subtypes of the sequences are labeled by different types of points.}
    \label{fig:Reduction_Comparison}
\end{figure}

\vspace{1em}

The visualization of the reduction by \texttt{PCA} is denoted as neighbors value -1 (\autoref{fig:Reduction_Comparison}). It shows five different accumulations of points. Labeling of these points is based on the original clustering example in \autoref{fig:PCA_Cluster_Knee_4}. This is becoming apparent when focusing on the cluster 48 points containing H13 and H16 sequences. That way a fundamental distribution on the points of H13 and H16 could be reviewed as well. 

%When using the right side as possible indication for clustering, all the points and accumulations of points are very close to each other. Nonetheless a separation with a imagined clustering can be made very easy by building two clusters of the blue points, one of the red and green and three of the orange points. Still, all the points related to H13 would be merged with the orange ones of H16 before the orange H16 points would be merged with the green ones of H16. Thereby the difference between H13 and H16, the higher-ranking goal would be not accomplished because the orange points are so close to each other. 
\vspace{1em}

The reduction with \texttt{PCA} on the subset resulted in easy separable accumulations of the cluster 46 and 48 points in \autoref{fig:Reduction_Comparison} (neighbors = -1). The distribution of these points is basically in line with the result shown in \autoref{fig:PCA_Cluster_Knee_4}, as their accumulations are well separated, building the two clusters with the same sequences in both figures. The major difference, however, is the distance between the accumulations of cluster 48 points to each other as well as to the ones of cluster 47. This would probably result in a imaginary clustering of unchanged cluster 46 and 45 and two or three clusters consisting of the cluster 48 points of which one also contains the points of cluster 47. It seems as if the distance of the cluster 47 points and the H13 cluster 48 points is largely affected by the reduction. The difference between cluster 46 and 45 in \autoref{fig:Reduction_Comparison} (neighbors = -1) is on the other hand preserved and would result in clustering similar to \autoref{fig:PCA_Cluster_Knee_4}. In \autoref{fig:PCA_Cluster_Knee_4} cluster 47 and 48 are also relatively close related as they would be linked on the next higher tree-node.

\vspace{1em}

In \autoref{fig:Reduction_Comparison} (neighbors = -1) it appears as if the points of cluster 47 and 48 are possibly quite similar, which is not the case as the \autoref{fig:Precalculated_Cosine} subtrees clearly show the wanted separation of H13 and H16 in cluster 48, as well as the wanted distance to 47. Keeping the lower complexity in mind, the consequence of lowering the dimension by \texttt{PCA} to two dimension seemed to preserve most of the information related to the difference of cluster 45 and 46. The difference of the subtype separation inside 48 as well as the overall difference to 47 on the other hand, seemed to be lost completely and caused the unwanted effects. Since the ground truth separation of \autoref{fig:Precalculated_Cosine} seems to be partially present in \autoref{fig:PCA_Clusteree_Knee_4}, by at least separating 47 completely from 48, the higher number of dimensions might be in direct connection to the correct separation of some part of H13 and H16. Therefore, even when raising the computationally effort, the number of components should be increased to the maximum of 50, that still preserves all functions of \texttt{HDBSCAN} for spanning tree building. 

\vspace{1em}

Comparing these results to the use of \texttt{UMAP} with different settings of the neighbors value, the impact of this parameter becomes clear. The higher the value, the more crowded the points. This also explains the crowded behavior in \autoref{subfig:Normalisation_UMAP}. Since a neighbors value of 100 was used as standard in this project, the values were overall crowded in groups of at least 100 points. The random subset for \autoref{subfig:Normalisation_UMAP} was reduced by the same setting with \texttt{UMAP}, despite the small sample size of 100 used there. The small random sample in addition to a high neighbors value resulted in a low number of overall distribution to clarify the behavior. Aside from the example in \autoref{subfig:Normalisation_UMAP}, the usage of a high neighbors value through the project was well reasoned and based on the huge size of the dataset used as described in \autoref{sec:Dimension_Reduction}. The same value of 100, as well as, 15 and 50 was used on the subset of H13 and H16 segment 4 sequences to visualize the difference in \autoref{fig:Reduction_Comparison}. 

\vspace{1em}

None of the settings resulted in a separation as good as with the sole use of \texttt{PCA}. With the \texttt{UMAP} standard neighbors value of 15, all the points are placed next to each other and there is no reasonable cluster building possible \autoref{fig:Reduction_Comparison} (neighbors = 15). Furthermore, H13 points would be merged with H16 points before merging with others from H13, thereby breaking the subtype division similar to the \texttt{PCA} use. Aside from the fact that the other points, when using \texttt{PCA}, are well separated. Setting the neighbors value to 50 results in a spreading of the cluster 46 points and mixing with little islands of cluster 48 points \autoref{fig:Reduction_Comparison} (neighbors = 50). With a neighbors value of 100, a separation into imaginary clusters is possible, when ignoring the cluster labeling and only taking the subtypes labeling into consideration. This is, therefore, the only setting with use of \texttt{UMAP} that would provide a more of less reasonable separation of the subtypes in imaginary clusters. However, clusters of different subtypes are closer than to similar subtypes, resulting also in no real subtype separation, even when ignoring the cluster 47 points that might be very sensible to the magnitude of preserved information.

%With normalization and without some information seem to be missing necessary to separate the orange points and the green ones. While on the left side the distance was underestimated to an extend making the orange H13 points and the green H16 points collide, the distance on the right side is overestimated, making the subtype distance of the H13 and H16 orange points to small. Since the separation between red and blue, as well as H13 orange and H16 orange is clearer, the method using normalization is still proved to be the better one, in the circumstances that the location of the green points is caused by the low dimension which is proved by \autoref{fig:PCA_Cluster_Knee_4} showing a separation between 47 and 48 and the right method not producing any better results related to the green points. 
\vspace{1em}

In conclusion, the use of PCA generated better results compared the ones with additional use of \texttt{UMAP}. Still, there were challenges to overcome as could be seen with the position of the cluster 47 points. Increasing the information preserved by the \texttt{PCA} would possibly give clearer results. This project aimed to find high-quality representations of \gls{IAV} genomes for the purpose or clustering in a extend that was never reached before. Therefore, the usability of hybrid \texttt{HDBSCAN} with parameters as good as possible was of higher importance than the use of \texttt{UMAP} at all costs. In the results of the project, \texttt{PCA} performed better than \texttt{UMAP} but only with all the tested parameters. Thus, it might be possible to find parameters for \texttt{UMAP} not explored in this project to represent the genomes even better in a equal low-dimension in the future. Also, change of \texttt{UMAP} in favor of \texttt{t-SNE} could be tested in terms of vector representation quality.